{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cours\n",
    "\n",
    "OC DS P5 - 4379436 [Explorez vos données avec des algorithmes non supervisés](https://openclassrooms.com/fr/courses/4379436-explorez-vos-donnees-avec-des-algorithmes-non-supervises)\n",
    "\n",
    "2 grands problèmes de l'apprentissage non supervisé :\n",
    "* réduction de dimension\n",
    "* partitionnement (clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Comprenez pourquoi réduire la dimension de vos données\n",
    "\n",
    "Vidéo : pourquoi réduire les dimensions :\n",
    "* intelligibilité (visualisation)\n",
    "* coûts d'acquisition : mémoire et traitements\n",
    "* amélioration des algorithmes\n",
    "* moins de paramètre, réduction du risque de surapprentissage\n",
    "* lutte contre le fléau de la dimensionnalité\n",
    "\n",
    "Nos données sont représentées sous la forme d'une matrice $X$ de dimension $n \\times p$, où $n$ est le nombre d'observations et $p$ le nombre de variables les représentant. $p$ est généralement un nombre assez grand, qui peut aller jusqu'à plusieurs dizaines de milliers dans certaines applications. C'est le cas par exemple lorsqu'on traite des images en haute résolution, et que chaque variable représente un pixel de cette image.\n",
    "\n",
    "Dans cette partie, nous allons étudier des techniques non supervisées permettant de réduire ce nombre de variables. Il s'agira de trouver $m$ variables, avec $m \\lt p$, que nous allons choisir d'utiliser pour construire une nouvelle matrice $\\tilde{X}$, de dimension $n \\times m$, pour représenter nos données.\n",
    "\n",
    "## Visualiser les données\n",
    "\n",
    "Nous pouvons assez facilement représenter des données en 2 ou 3 dimensions. Mais au-delà, on est obligé de regarder les variables paire par paire (ou triplet par triplet). Ça devient assez vite difficile, voire impossible quand le nombre de dimensions augmente. Si l'on pouvait représenter nos données avec 2 ou 3 dimensions seulement, ce serait beaucoup plus simple !\n",
    "\n",
    "## Réduire les coûts\n",
    "\n",
    "Si nous pouvons représenter nos données en quelques dimensions, cela fait aussi moins d'informations à stocker, ce qui réduit le **coût en espace mémoire**.\n",
    "\n",
    "Par ailleurs, avoir moins de variables réduit la complexité des algorithmes d'apprentissage que nous pouvons utiliser, et donc les **temps de calcul**.\n",
    "\n",
    "Enfin, si certaines variables sont inutiles, il n'est pas nécessaire de les obtenir pour de nouvelles observations : cela peut réduire le **coût d'acquisition** des données.\n",
    "\n",
    "## Améliorer la qualité des modèles d'apprentissage\n",
    "\n",
    "En utilisant moins de variables, on peut construire des modèles avec moins de paramètres, donc plus simples, ce qui est généralement préférable pour **éviter le surapprentissage**.\n",
    "\n",
    "De plus, **les variables non pertinentes peuvent induire l'algorithme d'apprentissage en erreur**. Prenons comme exemple l'algorithme des plus proches voisins, qui associe à une observation la même étiquette que celle de la majorité des $k$ points d'entraînement les plus proches. Si l'on utilise la distance euclidienne, toutes les variables comptent autant dans le calcul des plus proches voisins. Mais si l'une de ces variables n'est pas pertinente, elle peut changer la définition du plus proche voisin, et introduire du bruit dans le modèle.\n",
    "\n",
    "![](https://user.oc-static.com/upload/2017/05/11/14945111329723_P1C1-2.png)\n",
    "\n",
    "En utilisant les deux dimensions, les 3 plus proches voisins de l'étoile sont majoritairement bleus. En utilisant seulement la variable en abscisse, les 3 plus proches voisins sont majoritairement orange. Si la variable en ordonnée n'est pas pertinente, elle fausse l'algorithme.\n",
    "\n",
    "Enfin, le **fléau de la dimension** (*curse of dimensionality*, en anglais) est le terme que nous utilisons pour qualifier le fait qu'il est très difficile de faire de l'apprentissage en haute dimension. En effet, en haute dimension, on a besoin de beaucoup plus de points pour couvrir tout l'espace. Voir aussi le [chapitre correspondant dans le cours Initiez-vous au machine learning](https://openclassrooms.com/fr/courses/4011851-initiez-vous-au-machine-learning/4020651-gerez-le-fleau-de-la-dimension).\n",
    "\n",
    "Une autre façon de présenter le fléau de la dimension est de dire que toutes les observations sont loin les unes des autres, et il est très difficile de trouver ce qu'elles peuvent avoir de commun ou de différent. Les méthodes ou intuitions qui marchent en petite dimension ne marchent pas nécessairement en grande dimension.\n",
    "\n",
    "On en conclut que la réduction de dimension est nécessaire à la qualité de l'apprentissage.\n",
    "\n",
    "Pour comprendre pourquoi je dis que toutes les observations sont loin les unes des autres, on peut se placer en dimension $p$ et regarder la proportion du volume d'un hypercube comprise à l'extérieur de l'hypersphère incrite dans cet hypercube.\n",
    "\n",
    "En dimension 2, prenons un carré de côté $a = 2r$. Sa surface est $a^2 = 4r^2$. Le disque inscrit dans ce carré a pour surface $\\pi \\frac{a^2}{4} = \\pi r^2$. La surface entre ce disque et le carré vaut donc $a^2 - \\pi \\frac{a^2}{4} = (4 - \\pi)r^2$, et couvre donc une proportion $\\frac{a^2 - \\pi \\frac{a^2}{4}}{a^2} = \\frac{(4 - \\pi) r^2}{4r^2} =  1 - \\frac{\\pi}{4}$ de la surface du carré.\n",
    "\n",
    "En dimension $p$, prenons un hypercube de côté $a=2r$. Son volume est $(2r)^p$. Le volume d'une sphère de rayon $r$ est donné par $\\frac{(\\pi r^2)^q}{q \\Gamma(q)}$, avec $q = \\frac{p}{2}$. Par conséquent, la proportion qui nous intéresse vaut : $1 - \\frac{2 r^p \\pi^{p/2}}{p \\Gamma(p/2) 2^p r^p} = 1 - \\frac{\\pi^{p/2}}{p \\Gamma(p/2) 2^{p-1}} = 1 - \\left(\\frac{\\pi}{4}\\right)^q \\frac{1}{q \\Gamma(q)}$, qui tend vers 1 lorsque $p$ tend vers l'infini.\n",
    "\n",
    "**Note** - La fonction $\\Gamma$ est une généralisation de la factorielle : si $N$ est un entier, $\\Gamma(N) = (N-1)!$.\n",
    "\n",
    "![](https://user.oc-static.com/upload/2017/05/11/1494511177223_P1C1-3.png)\n",
    "Ici en dimension 3, on considère la proportion du volume du cube de côté a=2r  située à l'extérieur de la sphère de rayon r inscrite dans ce cube.\n",
    "\n",
    "## Résumé\n",
    "\n",
    "Réduire la dimensionalité des données, c'est-à-dire le nombre de variables utilisées pour les représenter, permet :\n",
    "* de **faciliter la visualisation** des données ;\n",
    "* de **réduire les coûts** de calcul, de stockage et d'acquisition des données ;\n",
    "* d'**améliorer l'apprentissage** en construisant des **modèles moins complexes**, en éliminant les variables non pertinentes qui pourraient **fausser les prédictions** et enfin en réduisant le problème du **fléau de la dimensionalité**.\n",
    "\n",
    "Dans la suite de cette partie, nous allons voir des techniques linéaires de réduction de dimension non supervisées.\n",
    "\n",
    "**Note** - Lorsque les données sont *étiquetées*, il existe de nombreuses autres techniques permettant de réduire leur dimension. Certaines, comme la LDA (analyse discriminante linéaire, sur laquelle vous pouvez en lire plus [**ici (en anglais : ressource de haute qualité)**](https://sebastianraschka.com/Articles/2014_python_lda.html), sont des extensions des techniques d'*extraction de variables* dont nous allons parler dans cette partie : il s'agit de créer un petit nombre de nouvelles variables par lesquelles représenter les données. D'autres sont des techniques de *sélection de variables* : il s'agit non pas de construire de nouvelles variables, mais d'éliminer les variables les moins informatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Calculez les composantes principales de vos données\n",
    "\n",
    "Vidéo : ACP\n",
    "* Simple et efficace\n",
    "* Trouver une nouvelle base orthonormée\n",
    "* Qui maximise la variance des données projetées\n",
    "* NB - c'est une révision du chapitre ??? du cours ???\n",
    "* Mais un bon complément un chouïa plus formal\n",
    "\n",
    "## Analysez les composantes principales de vos données\n",
    "\n",
    "Dans ce chapitre, nous allons parler d'une méthode très utilisée pour réduire la dimension d'un jeu de données : l'analyse en composantes principales, ou ACP. On parle aussi souvent de PCA, de son nom anglais Principal Components Analysis.\n",
    "\n",
    "### Objectif : maximiser la variance\n",
    "\n",
    "Le but d'une analyse en composantes principales est de trouver une nouvelle base orthonormée dans laquelle représenter nos données, telle que la variance des données selon ces nouveaux axes soit maximisée.\n",
    "\n",
    "![](https://user.oc-static.com/upload/2017/05/11/14945147458345_P1C2-1.png)\n",
    "*Variance des données*\n",
    "\n",
    "La variance des données selon l'axe orange est grande. Si on projette les points sur cet axe, ils auront tous des coordonnées différentes ; en utilisant cet axe comme unique dimension, on réduit la dimension de nos données (de 2 à 1) mais on continue à pouvoir distinguer les points les uns des autres.\n",
    "\n",
    "## Calculer les composantes principales\n",
    "\n",
    "Supposons que nous ayons $p$ variables : chaque observation est représentée par un vecteur dans $\\mathbb{R}^p$, et nous avons $n$ observations rassemblées dans une matrice $X \\in \\mathbb{R}^{p \\times n}$.\n",
    "\n",
    "Commençons par chercher *une* nouvelle direction, à savoir un vecteur $\\bf{w_1} \\in \\mathbb{R}^p$, de norme 1, tel que la variance de nos données projetées sur cette direction soit maximale. La projection des données $X$ sur $\\bf{w}$ est $\\bf{z_1} = \\bf{w_1}^\\top X$.\n",
    "\n",
    "**Attention** - Dans ce qui suit, nous allons supposer que $X$ est *centrée*, c'est-à-dire que la moyenne de ses colonnes est 0. Si ce n'est pas le cas, on peut utiliser la transformation $\\tilde X = X - \\mu$ où $\\mu$ est un vecteur contenant la moyenne de chaque colonne (variable).\n",
    "\n",
    "La variance de $\\bf{w_1}^\\top X$ est égale à $\\mathbb{E}[(\\bf{w_1}^\\top X - \\mathbb{E}[\\bf{w_1}^\\top X])^2]$, soit, comme $\\mathbb{E}[\\bf{X}] = 0$ (les données sont centrées), $\\bf{w_1}^\\top \\mathbb{E}[X X^\\top] \\bf{w_1}$.\n",
    "\n",
    "Appelons $\\Sigma$ la matrice de taille $p \\times p$ égale à $\\bf{X} \\bf{X}^\\top$. C'est la matrice de covariance des données et une estimation de $\\mathbb{E}[\\bf{X} \\bf{X}^\\top]$.\n",
    "\n",
    "Nous cherchons donc $\\bf{w_1}$ tel que :\n",
    "* $\\bf{w_1}^\\top \\Sigma \\bf{w_1}$ est maximale\n",
    "* $||\\bf{w_1}|| = 1$\n",
    "\n",
    "$\\Sigma$ est, par construction, une matrice symétrique définie positive. Elle est donc diagonalisable par un changement de base orthonormé : $\\Sigma = Q^\\top \\Lambda Q$, où $\\Lambda \\in \\mathbb{R}^{p \\times p}$ est une matrice diagonale dont les valeurs diagonales sont les valeurs propres de $\\Sigma$, qui sont toutes positives.\n",
    "\n",
    "Nous avons donc $\\bf{w_1}^\\top \\Sigma \\bf{w_1} = \\bf{w_1}^\\top Q^\\top \\Lambda Q \\bf{w_1} = (Q \\bf{w_1})^\\top \\Lambda (Q \\bf{w_1})$\n",
    "\n",
    "Posons $\\bf{v} = Q \\bf{w_1}$; nous cherchons à maximiser $\\sum_{j=1}^p v_j^2 \\lambda_j$. Comme $||\\bf{w_1}|| = 1$ et que $Q$ est orthonormée, $||\\bf{v}|| = 1$ et donc $\\sum_{j=1}^p v_j^2=1$. La somme $\\sum_{j=1}^p v_j^2 \\lambda_j$ est donc maximisée pour un vecteur $\\bf{v}$ qui a une seule entrée à 1 et les autres à 0, l'entrée à 1 étant pour la valeur maximale des   $\\lambda_j$, autrement dit **la plus grande valeur propre de $\\Sigma$**.\n",
    "\n",
    "Par conséquent, $\\bf{w_1}$ est **le vecteur propre correspondant à la plus grande valeur propre de $\\Sigma$.** C'est la **première composante principale de X**.\n",
    "\n",
    "La **deuxième composante principale de X** doit vérifier les mêmes critères que $\\bf{w_1}$ : être de norme 1 et maximiser $\\bf{w_2}^\\top \\Sigma \\bf{w_2}$, mais lui est orthogonale. Il s'agit donc du vecteur propre de $\\(\\Sigma\\)$ correspondant à sa **deuxième plus grande valeur propre**.\n",
    "\n",
    "Et ainsi de suite pour les autres composantes principales.\n",
    "\n",
    "Nous pouvons donc utiliser comme nouvelles dimensions **la base formée par les vecteurs propres de la matrice de covariance des données**.\n",
    "\n",
    "**Remarque** - Il est possible d'obtenir cette base directement en calculant la décompostion en valeurs singulières (ou SVD) de $X$, $X = U D V^\\top$ avec $U \\in \\mathbb{R^{p \\times p}}$ orthogonale, $V \\in \\mathbb{R^{n \\times n}}$ orthogonale, et $D \\in \\mathbb{R}^{p \\times n}$ diagonale.\n",
    "\n",
    "Les valeurs singulières de $X$, à savoir les entrées de $D$, sont les racines carrées des valeurs propres de $\\Sigma$, tandis que les vecteurs singuliers de $X$ sont les vecteurs propres de $\\Sigma$.\n",
    "\n",
    "En effet, $X X^\\top = U D V^\\top V D^\\top U\\top = U D^2 U^\\top$.\n",
    "\n",
    "L'intérêt d'utiliser une SVD de $X$ plutôt qu'une décomposition en valeurs propres de   $X X^\\top$ est la stabilité numérique des méthodes implémentant la première.\n",
    "\n",
    "## Comment choisir le nombre de composantes principales ?\n",
    "\n",
    "La méthode que nous venons de décrire nous permet de construire autant de composantes principales que $\\Sigma$ a de vecteurs propres, soit autant que le nombre de descripteurs $p$ de nos données. Nous n'avons pas encore réduit la dimension de nos données.\n",
    "\n",
    "Pour ce faire, nous allons regarder la proportion de variance expliquée par chacune des composantes principales.\n",
    "\n",
    "La variance totale du jeu de données est donnée par la somme des termes diagonaux de la matrice de covariance $\\Sigma$ (autrement dit la somme des variances de toutes les variables), soit sa trace. Or $Tr(\\Sigma) = \\lambda_1 + \\lambda_2 + \\dots + \\lambda_p$.\n",
    "\n",
    "La variance totale est donnée par $\\lambda_1 + \\lambda_2 + \\dots + \\lambda_p$, et celle expliquée par les $k$ premières composantes principales, par $\\lambda_1 + \\lambda_2 + \\dots + \\lambda_k$. La proportion de variance expliquée par les $k$ premières composantes principales est donc $\\frac{\\lambda_1 + \\lambda_2 + \\dots + \\lambda_k}{\\lambda_1 + \\lambda_2 + \\dots + \\lambda_p}$\n",
    "\n",
    "Nous pouvons maintenant regarder comment cette proportion évolue en fonction du nombre de composantes, et construire ce qu'on appelle en anglais un ***scree plot***. Ce graphique présente la proportion de variance expliquée par la $k$-ième composante principale, ou par les $k$ premières composantes principales, en fonction de $k$.\n",
    "\n",
    "On peut l'utiliser pour choisir :\n",
    "* le nombre de composantes principales qui explique un pourcentage de la variance que l'on s'est initialement fixé (par exemple, 90 %) ;\n",
    "* le nombre de composantes principales correspondant au « coude » du graphe, à partir duquel ajouter une nouvelle composante principale ne fait pas grande différence.\n",
    "\n",
    "<figure id=\"r-6814070\" data-claire-element-id=\"31539917\"><img id=\"r-6814068\" data-claire-element-id=\"30559936\" src=\"https://user.oc-static.com/upload/2020/01/20/15795446394714_scree_plot.png\" alt=\"le pourcentage de variance expliqué par chacune des composantes principales. À partir de 5 composantes principales, ajouter une composante n'est pas très informatif.\" />\n",
    "<figcaption>Le pourcentage de variance expliqué par chacune des composantes principales. À partir de 5 composantes principales, ajouter une composante n'est pas très informatif.</figcaption></figure>\n",
    "\n",
    "<figure id=\"r-6814081\" data-claire-element-id=\"31539919\"><img id=\"r-6814079\" data-claire-element-id=\"30559958\" src=\"https://user.oc-static.com/upload/2020/01/20/1579544692388_scree_plot_cum.png\" alt=\"le pourcentage cumulé de variance expliqué par chacune des composantes principales. Si on se fixe une proportion de variance expliquée de 95%, on peut se contenter de 9 composantes principales.\" /><figcaption>Le pourcentage cumulé de variance expliqué par chacune des composantes principales. Si on se fixe une proportion de variance expliquée de 90 %, on peut se contenter de 5 composantes principales.</figcaption></figure>\n",
    "\n",
    "Une autre possibilité consiste à utiliser seulement deux ou trois composantes pour visualiser les données.\n",
    "\n",
    "**Remarque** - Un exemple de ce que peut faire une ACP : en 2008, John Novembre et ses collègues ont publié une analyse en composantes principales des génomes de 1 387 Européens. La projection des données sur ces deux dimensions correspond assez bien à... la carte de l'Europe ! Ce résultat a été publié [**dans la revue Nature**](https://www.nature.com/nature/journal/v456/n7218/full/nature07331.html).\n",
    "\n",
    "<figure id=\"r-4448469\" data-claire-element-id=\"31539922\"><img id=\"r-4448467\" data-claire-element-id=\"7893322\" src=\"https://user.oc-static.com/upload/2017/05/11/14945141781365_P1C1-4.png\" alt=\"Les deux premières composantes principales des génomes de 1387 Européens correspondent à... une carte de l'Europe\" /><figcaption>Les deux premières composantes principales des génomes de 1 387 Européens correspondent à... une carte de l'Europe</figcaption></figure>\n",
    "\n",
    "**Remarque** - Vous trouverez plus de détails par exemple sur [**ce billet de *Science Étonnante***](https://sciencetonnante.wordpress.com/2011/05/16/comment-nos-genes-trahissent-nos-origines)\n",
    "\n",
    "\n",
    "## Résumé\n",
    "\n",
    "* Les composantes principales forment une base orthonormée et sont construites pour que les données aient une variance maximale selon ces nouveaux axes.\n",
    "* Les composantes principales sont en fait les vecteurs propres de la matrice de covariance des données, classés par ordre décroissant de valeur propre correspondante.\n",
    "* Pour choisir le nombre de composantes à utiliser, on regarde la proportion de la variance totale expliquée par k composantes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e03b612d84ba21ce95ed447e81b3062e1eb99b56c6d885cdab4aaa12f1b8e240"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

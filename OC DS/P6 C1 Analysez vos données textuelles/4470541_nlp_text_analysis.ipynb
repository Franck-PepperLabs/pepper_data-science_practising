{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [**Analysez vos données textuelles**](https://openclassrooms.com/fr/courses/4470541-analysez-vos-donnees-textuelles)\n",
    "\n",
    "* **ID** : 4470541\n",
    "* **Grade** : OC DS P6\n",
    "* **Durée** : 8 heures\n",
    "* **Difficulté** : Moyenne\n",
    "* **Séquence** : 3 + 4 + 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bienvenue dans ce cours de traitement du langage naturel. L’objectif de ce cours est de comprendre les méthodes qui permettent de **transformer le texte** en caractéristiques exploitables par des algorithmes de machine learning, et les architectures et modèles qui correspondent le mieux à ce type de données. En l’occurence un ensemble de documents texte **non-structurés**.\n",
    "\n",
    "Ce cours est divisé en 3 parties : une première qui traite de l'exploration, du **nettoyage et de la normalisation du texte**. Une seconde partie dédiée aux **différents types de transformations** qui vont vous permettre de mieux comprendre vos données textuelles et de **créer des caractéristiques** que vous pourrez utiliser dans vos algorithmes de machine learning. La dernière partie sera consacrée à la **classification du texte** à l'aide de l'apprentissage automatique sous forme de **réseau de neurones**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectifs pédagogiques\n",
    "\n",
    "* Effectuer un pré-traitement de corpus de texte\n",
    "* Maîtriser les techniques de bag-of-words et de plongements de mots (word embeddings)\n",
    "* Modéliser des sujets de manière non-supervisée (LDA, etc.)\n",
    "* Classer des corpus de texte avec des méthodes supervisées (réseaux de neurones)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prérequis\n",
    "\n",
    "Ce cours se situe au croisement des mathématiques et de l'informatique. Pour en profiter pleinement, n'hésitez pas à vous rafraîchir la mémoire, avant ou pendant le cours, sur :\n",
    "\n",
    "* [**Python pour le calcul numérique (numpy) et la création de graphiques (pyplot)**](https://openclassrooms.com/fr/courses/7771531-decouvrez-les-librairies-python-pour-la-data-science?archived-source=4452741), que nous utiliserons dans les parties TP du cours,\n",
    "* Quelques notions d'**algèbre linéaire** : manipulation de vecteurs, multiplications de matrices, normes, et valeurs/vecteurs propres,\n",
    "* Quelques notions de **probabilités** et de **statistiques**, telles que distribution de loi de probabilité et variance,\n",
    "* Les [**modèles non-supervisées**](https://openclassrooms.com/fr/courses/4379436-explorez-vos-donnees-avec-des-algorithmes-non-supervises) permettront de modéliser des caractéristiques automatiquement à partir du texte\n",
    "* Les [**modèles supervisées non-linéaires**](https://openclassrooms.com/fr/courses/4470406-utilisez-des-modeles-supervises-non-lineaires) sont indispensables au traitement du texte, notamment les réseaux de neurones séquentiels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan\n",
    "\n",
    "* Partie 1 - Prétraitez des données textuelles\n",
    "    * 1. Récupérez et explorez le corpus de textes\n",
    "    * 2. Nettoyez et normalisez les données\n",
    "    * 3. TP - Faites vos premiers pas dans l'analyse de données textuelles\n",
    "* Partie 2 - Transformez des données textuelles\n",
    "    * 1. Représentez votre corpus en \"bag of words\"\n",
    "    * 2. Effectuez des plongements de mots (word embeddings)\n",
    "    * 3. Modélisez des sujets avec des méthodes non supervisées\n",
    "    * Quiz : Partie 2\n",
    "* Partie 3 - Détectez automatiquement les sentiments de commentaires clients\n",
    "    * 1. Opérez une première classification naïve de sentiments\n",
    "    * 2. Allez plus loin dans la classification de mots\n",
    "    * 3. Traitez le corpus de textes à l'aide de réseaux de neurones\n",
    "    * 4. Entraînez-vous à classifier du texte\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  1.1. Récupérez et explorez le corpus de textes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La première étape du traitement des données est de récupérer le corpus de textes, de faire une analyse exploratoire afin de bien comprendre les spécificités du jeu de données, et de nettoyer les données afin de pouvoir les utiliser ultérieurement dans vos algorithmes.\n",
    "\n",
    "Mais avant de nous plonger dans le vif du sujet, introduisons quelques notions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est important de bien identifier 4 termes incontournables : \n",
    "\n",
    "* le **corpus** : un ensemble de documents (des textes dans notre cas), regroupés dans une optique ou dans une thématique précise. \n",
    "* un **document** : la notion de document fait référence à un texte appartenant au corpus, mais indépendant des autres textes. Il peut être constitué d'une ou plusieurs phrases, un ou plusieurs paragraphes.\n",
    "* un **jeton (token)** : le terme token désigne généralement un mot et/ou un élément de ponctuation. La phrase \"Hello World!\" comprend donc 3 tokens. \n",
    "* le **vocabulaire** : il s'agit de l'ensemble des tokens distincts présents dans l'ensemble du corpus."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si la notion de token ou de vocabulaire est relativement invariante en fonction des jeux de données, le corpus et les documents peuvent avoir des formes très variées :\n",
    "\n",
    "un fichier excel ou csv avec une liste de produits. Ici, la colonne 'description' est notre corpus, chaque cellule  de la colonne 'description' constitue un document:\n",
    "\n",
    "<figure id=\"r-7906592\" data-claire-element-id=\"33104169\"><img id=\"r-7906590\" data-claire-element-id=\"33104167\" src=\"https://user.oc-static.com/upload/2022/06/27/16563236580116_example_csv.jpg\" alt=\"chaque ligne de la colonne description est un document\" /><figcaption>un fichier csv</figcaption></figure>\n",
    "\n",
    "un dossier avec différents fichiers. Chaque fichier est un document et l'ensemble des fichiers constitue le corpus : \n",
    "\n",
    "<figure id=\"r-7906596\" data-claire-element-id=\"33104174\"><img id=\"r-7906594\" data-claire-element-id=\"33104172\" src=\"https://user.oc-static.com/upload/2022/06/27/16563235098743_contrats.jpg\" alt=\"chaque fichier est un document\" /><figcaption>une liste de fichiers dans un dossier</figcaption></figure>\n",
    "\n",
    "plusieurs pages web, au format html. Il faudra d'abord télécharger depuis le web les fichiers html, à la main ou de façon automatisée (on parle alors de 'scraping') :\n",
    "\n",
    "<figure id=\"r-7906601\" data-claire-element-id=\"33104179\"><img id=\"r-7906599\" data-claire-element-id=\"33104177\" src=\"https://user.oc-static.com/upload/2022/06/27/16563245092815_wiki.jpg\" alt=\"on utilisera les données du fichier html\" /><figcaption>exemple de page wikipedia</figcaption></figure>\n",
    "\n",
    "Dans le cadre d'un fichier html, il faudra transformer le document afin d'extraire l'information textuelle. Nous verrons cela un plus tard dans le cours."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠  Pour les documents .txt, .doc, .csv, .xls, cela semble facile, mais comment faire pour les documents **.pdf** ?\n",
    "\n",
    "Transformer les documents .pdf en .txt n'est pas une chose facile. Ce traitement spécifique à un nom, cela s'appelle **OCR** :  Optical Character Recognition. \n",
    "\n",
    "Heureusement, il existe de nombreux packages disponibles, comme par exemple [**Tesseract**](https://github.com/madmaze/pytesseract). Rassurez vous, nous ne couvrirons pas cette problématique en détail, mais gardez en tête qu'elle existe."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant aborder le pré-traitement du texte  en plusieurs étapes  :\n",
    "1. La récupération du **corpus**, ansi qu'un premier **traitement** de ce dernier pour avoir des données textuelles exploitables (au format string).\n",
    "2. La **tokenization**, qui désigne le découpage en mots des différents documents qui constituent votre corpus.\n",
    "3. La **normalisation** et la construction du dictionnaire qui permet de ne pas prendre en compte des détails importants au niveau local (ponctuation, majuscules, conjugaison, etc.)\n",
    "\n",
    "<figure id=\"r-4867873\" data-claire-element-id=\"33104195\"><img id=\"r-4867871\" data-claire-element-id=\"8982571\" src=\"https://lh5.googleusercontent.com/UmUtjHcacRCRs_Ba42Io7ZmU3xtcZ2xnuLn6ohZA5z7ZDU20Xzocm1t7sk5_UXS98vSSkcSKYlvQqIaiEOepKpOGqYkr4z1liJZbCCj-bZXmdTrcAWdUFqor_twdFJMywyKNuYDA\" alt=\"Le cycle de traitement d'un corpus de texte\" /><figcaption>Le cycle de traitement d'un corpus de texte</figcaption></figure>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'en conviens, la partie nettoyage n'est pas la plus intéressante, mais elle est essentielle. Pour rendre la chose plus attrayante, je vais réaliser une étude partiale et partielle des artistes français de rap et leur vocabulaire propre."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🛈 Tout au long de ce chapitre, nous allons utiliser la librairie de traitement de langage [**NLTK**](https://www.nltk.org/) ainsi que les librairies classiques pandas, numpy et scikit.\n",
    "\n",
    "Cette Librairie, bien qu'ancienne est connue et reconnue pour sa simplicité d'utilisation et sa grande polyvalence.\n",
    "\n",
    "Vous êtes bien entendu libres d'utiliser des librairies plus modernes mais parfois plus complexes. La librairie [**spacy**](https://spacy.io/) en est un bon exemple."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération du corpus de texte\n",
    "\n",
    "La première étape est la récupération du texte. Il existe plusieurs manières de récupérer du texte : soit depuis une base de donnée que vous possédez, soit depuis des fichiers XML ou autres que vous possédez, soit en scrapant des pages comme le font les moteurs de recherches, en utilisant une API.\n",
    "\n",
    "Nous ne traiterons pas cette partie, qui est relativement laborieuse et techniquement peu intéressante. Il existe diverses manières de scraper du texte comme à l'aide des librairies [**scrapy**](https://scrapy.org/) ou [**beautifulsoup**](https://beautiful-soup-4.readthedocs.io/en/latest/).\n",
    "\n",
    "Dans mon cas, j'ai scrapé la page wikipédia qui propose une liste des rappeurs français. J'ai ensuite récupéré les paroles des différentes chansons de ces rappeurs sur le site Genius, toujours en scrapant. Je ne suis malheureusement pas autorisé à vous fournir ce jeu de données, libre à vous d'effectuer la même démarche. Vous pouvez aussi utiliser un jeu de données présent par défaut dans la librairie NLTK."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠  Attention au format d'encodage de vos fichiers texte qui peuvent mener à des erreurs faciles à éviter. On favorisera sur ce cours l'**UTF-8** (encodage universel) très utilisé et qui permet l'usage des accents."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le texte que vous utilisez, que l'on appelle « corpus », peut être organisé de plusieurs manière différentes :\n",
    "\n",
    "<figure id=\"r-4867909\" data-claire-element-id=\"33104206\"><img id=\"r-4867907\" data-claire-element-id=\"8982643\" src=\"https://lh3.googleusercontent.com/AuaUziLNkLIyzqcnegyKz38ITKzmq0VNBgHHvddY2zUyo1hM5oVS5vs_AsYv9kNtz5cY1p2cZCVTQI34O_i64MhSEZgOnoIN0kgI4PMXnB47ZsLdX_yIFbPrKOYa4pEd0UyWx_zk\" alt=\"Les différents types de structuration du texte\" /><figcaption>Les différents types de structuration du texte</figcaption></figure>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargeons donc les données, dans un dictionnaire python, ce que je fais avec la fonction   `load_all_sentences` que j'ai créée pour mon exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_all_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\franc\\Projects\\pepper_data-science_practising\\OC DS\\P6 C1 Analysez vos données textuelles\\4470541_nlp_text_analysis.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/franc/Projects/pepper_data-science_practising/OC%20DS/P6%20C1%20Analysez%20vos%20donn%C3%A9es%20textuelles/4470541_nlp_text_analysis.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m db \u001b[39m=\u001b[39m load_all_sentences()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/franc/Projects/pepper_data-science_practising/OC%20DS/P6%20C1%20Analysez%20vos%20donn%C3%A9es%20textuelles/4470541_nlp_text_analysis.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mchargement de \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m vers dans la db\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mlen\u001b[39m(db\u001b[39m.\u001b[39mkeys())))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_all_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "db = load_all_sentences()\n",
    "print('chargement de {} vers dans la db'.format(len(db.keys())))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mon dictionnaire est simplement constitué d'objets de la forme { vers, artiste }\n",
    "\n",
    "Je crée aussi une table de recherche par artiste car on va s'intéresser à ce qui les différencie et les caractérise. De plus, je veux avoir assez de texte pour chaque artiste donc je vais éliminer ceux qui ont écrit moins de 200 vers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "base_artistes = defaultdict(set)\n",
    "for k,v in db.iteritems():\n",
    "    base_artistes[v['artistes']].add(k)\n",
    "artistes = { k:v for k,v in base_artistes.iteritems() if len(v) > 200 }\n",
    "print('{} artistes'.format(len(artistes)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration du texte : tokenisation et analyse des fréquences\n",
    "\n",
    "On veut dans un premier temps étudier le vocabulaire utilisé par chaque artiste. Pour une première intuition, il est judicieux d'observer le nombre de mots utilisés.\n",
    "\n",
    "On va utiliser la fonction [**`word_tokenize`**](https://www.nltk.org/api/nltk.tokenize.html) (« tokenize » signifie « séparer par mot ») qui va décomposer les vers en tableaux de mots afin de pouvoir effectuer des opérations dessus. Observons déjà son comportement sur un bout de texte simple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bonjour',\n",
       " ',',\n",
       " 'je',\n",
       " 'suis',\n",
       " 'un',\n",
       " 'texte',\n",
       " \"d'exemple\",\n",
       " 'pour',\n",
       " 'le',\n",
       " 'cours',\n",
       " \"d'Openclassrooms\",\n",
       " '.',\n",
       " 'Soyez',\n",
       " 'attentifs',\n",
       " 'à',\n",
       " 'ce',\n",
       " 'cours',\n",
       " '!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install nltk\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "test = \"Bonjour, je suis un texte d'exemple pour le cours d'Openclassrooms. Soyez attentifs à ce cours !\"\n",
    "\n",
    "nltk.word_tokenize(test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a bien une séparation par mot. Petit problème en revanche, la ponctuation est conservée comme étant un \"token\" ! Il faut donc trouver un moyen d'éliminer cette ponctuation, car ce sont les mots qui nous intéressent comme caractéristiques . On remarque aussi qu'il y a un problème sur les apostrophes considérés comme faisant partie du mot. Ainsi \"d'exemple\" devrait être séparé en \"de\" et \"exemple\". Un autre problème, c'est que certains mots on des majuscules car ils apparaissent en début de phrases ou de vers, alors que ce sont les mêmes mots.\n",
    "\n",
    "Ca parait tout d'un coup un peu compliqué à mettre en place, n'est-ce pas ? 😏"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🛈 Les accents sont affichés encodés mais ce n'est pas très grave, on peut revenir à un affichage normal si on le souhaite."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est vraiment important de regarder les options sur ce genre de fonctions qui englobent plusieurs actions sur votre corpus afin d'être bien sûr qu'elles effectuent ce que vous voulez.\n",
    "\n",
    "Le fait d'essayer d'harmoniser les tokens est un processus nommé « **normalisation** ». Bon, on va déjà utiliser les bonne vieilles expressions régulières pour ne récupérer que les caractères alphanumériques de chaque phrase. Vous trouverez à [cette adresse](https://www.debuggex.com/cheatsheet/regex/python) un rappel utile sur les expressions régulières.\n",
    "\n",
    "Ensuite, on va utiliser un tokenizer specifique au français ce qui permet de traiter la ponctuation de la bonne manière. On élimine aussi les majuscules peu informatives, avec la fonction « lower »."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bonjour',\n",
       " 'je',\n",
       " 'suis',\n",
       " 'un',\n",
       " 'texte',\n",
       " 'd',\n",
       " 'exemple',\n",
       " 'pour',\n",
       " 'le',\n",
       " 'cours',\n",
       " 'd',\n",
       " 'Openclassrooms',\n",
       " 'Soyez',\n",
       " 'attentifs',\n",
       " 'à',\n",
       " 'ce',\n",
       " 'cours']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "tokenizer.tokenize(\"Bonjour, je suis un texte d'exemple pour le cours d'Openclassrooms. Soyez attentifs à ce cours !\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, ça commence à être mieux ! Maintenant qu'on a bien séparé notre texte en unité de mots (tokens) on peut l'appliquer au jeu de données qui nous intéresse, et compter la fréquence d'apparition des différents mots pour avoir une idée du champ lexical. On effectue ce comptage par artiste pour comparer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'defaultdict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\franc\\Projects\\pepper_data-science_practising\\OC DS\\P6 C1 Analysez vos données textuelles\\4470541_nlp_text_analysis.ipynb Cell 29\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/franc/Projects/pepper_data-science_practising/OC%20DS/P6%20C1%20Analysez%20vos%20donn%C3%A9es%20textuelles/4470541_nlp_text_analysis.ipynb#X42sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m (freq, stats, corpora)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/franc/Projects/pepper_data-science_practising/OC%20DS/P6%20C1%20Analysez%20vos%20donn%C3%A9es%20textuelles/4470541_nlp_text_analysis.ipynb#X42sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Récupération des comptages\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/franc/Projects/pepper_data-science_practising/OC%20DS/P6%20C1%20Analysez%20vos%20donn%C3%A9es%20textuelles/4470541_nlp_text_analysis.ipynb#X42sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m freq, stats, corpora \u001b[39m=\u001b[39m freq_stats_corpora()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/franc/Projects/pepper_data-science_practising/OC%20DS/P6%20C1%20Analysez%20vos%20donn%C3%A9es%20textuelles/4470541_nlp_text_analysis.ipynb#X42sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame\u001b[39m.\u001b[39mfrom_dict(stats, orient\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/franc/Projects/pepper_data-science_practising/OC%20DS/P6%20C1%20Analysez%20vos%20donn%C3%A9es%20textuelles/4470541_nlp_text_analysis.ipynb#X42sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Affichage des fréquences\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\franc\\Projects\\pepper_data-science_practising\\OC DS\\P6 C1 Analysez vos données textuelles\\4470541_nlp_text_analysis.ipynb Cell 29\u001b[0m in \u001b[0;36mfreq_stats_corpora\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/franc/Projects/pepper_data-science_practising/OC%20DS/P6%20C1%20Analysez%20vos%20donn%C3%A9es%20textuelles/4470541_nlp_text_analysis.ipynb#X42sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfreq_stats_corpora\u001b[39m():\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/franc/Projects/pepper_data-science_practising/OC%20DS/P6%20C1%20Analysez%20vos%20donn%C3%A9es%20textuelles/4470541_nlp_text_analysis.ipynb#X42sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     corpora \u001b[39m=\u001b[39m defaultdict(\u001b[39mlist\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/franc/Projects/pepper_data-science_practising/OC%20DS/P6%20C1%20Analysez%20vos%20donn%C3%A9es%20textuelles/4470541_nlp_text_analysis.ipynb#X42sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# Création d'un corpus de tokens par artiste\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/franc/Projects/pepper_data-science_practising/OC%20DS/P6%20C1%20Analysez%20vos%20donn%C3%A9es%20textuelles/4470541_nlp_text_analysis.ipynb#X42sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mfor\u001b[39;00m artiste,sentence_id \u001b[39min\u001b[39;00m artistes\u001b[39m.\u001b[39miteritems():\n",
      "\u001b[1;31mNameError\u001b[0m: name 'defaultdict' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def freq_stats_corpora():\n",
    "    corpora = defaultdict(list)\n",
    "\n",
    "    # Création d'un corpus de tokens par artiste\n",
    "    for artiste, sentence_ids in artistes.iteritems():\n",
    "        for sentence_id in sentence_ids:\n",
    "            corpora[artiste] += tokenizer.tokenize(\n",
    "                                    db[sentence_id]['text'].decode('utf-8').lower()\n",
    "                                )\n",
    "\n",
    "    stats, freq = dict(), dict()\n",
    "\n",
    "    for k, v in corpora.iteritems():\n",
    "        freq[k] = fq = nltk.FreqDist(v)\n",
    "        stats[k] = {'total': len(v)} \n",
    "        \n",
    "    return (freq, stats, corpora)\n",
    "\n",
    "# Récupération des comptages\n",
    "freq, stats, corpora = freq_stats_corpora()\n",
    "df = pd.DataFrame.from_dict(stats, orient='index')\n",
    "\n",
    "# Affichage des fréquences\n",
    "df.sort(columns='total', ascending=False)\n",
    "df.plot(kind='bar', color=\"#f56900\", title='Top 50 Rappeurs par nombre de mots')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure id=\"r-4867916\" data-claire-element-id=\"33104209\"><img id=\"r-4867914\" data-claire-element-id=\"8982657\" src=\"https://lh6.googleusercontent.com/PLxZshHniJeQHJirRxYYDFoIcdIFLtVs2dJx8JQannUb4rYy7EK5mKe_so51Jmg90h-BwUSO8TrRNK9jyPdmqg97wqY-5uNpCipVS4d7MWdxhScu1-4lJ_OLMbUGZzDwtakqKx4I\" alt=\"Première tokenisation du corpus\" /><figcaption>Première tokenisation du corpus </figcaption></figure>\n",
    "\n",
    "\n",
    "**Nous voyons ici quel artiste a écrit le plus de texte et de chansons.** C'est intéressant, mais qu'en est-il de la variété du champ lexical utilisé, c'est à dire le nombre de mots uniques utilisés par chaque artistes dans leurs chansons ? Nous souhaitons en effet savoir qui a le vocabulaire le plus riche !😀\n",
    "\n",
    "Pour le savoir, nous devons représenter un document (ou ici, un artiste) par ce qu'on appelle un **bag-of-words**.\n",
    "\n",
    "Modifions donc notre fonction `freq_stats_corpora` pour faire le comptage du vocabulaire unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_stats_corpora():\n",
    "    corpora = defaultdict(list)\n",
    "    for artiste,sentence_ids in artistes.iteritems():\n",
    "        for sentence_id in sentence_ids:\n",
    "            corpora[artiste] += tokenizer.tokenize(\n",
    "                                    db[sentence_id]['text'].decode('utf-8').lower()\n",
    "                                )\n",
    "        \n",
    "    stats, freq = dict(), dict()\n",
    "\n",
    "    for k, v in corpora.iteritems():\n",
    "        freq[k] = fq = nltk.FreqDist(v)\n",
    "        stats[k] = {'total': len(v), 'unique': len(fq.keys())}\n",
    "\n",
    "    return (freq, stats, corpora)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichons à nouveau nos comptages :\n",
    "\n",
    "<img id=\"r-4867949\" data-claire-element-id=\"8982808\" src=\"https://lh5.googleusercontent.com/z1_GN_SjfpGmYi2ApJgW_-Som9c6gB_s3ICts86mQm-S9QWfyCFlt5P_H7Q-IDs0-otR_ULVz5-yU0C30krRuMPAstmiqRJvQ9CfoviYWLkg5188r5_txrOrP7ErkhvxW46XNl4J\" alt=\"\" />\n",
    "\n",
    "Mais ça ne se terminera donc jamais ?! 😢\n",
    "\n",
    "Pour faciliter des choses, nous décomposons les différentes étapes. À force d'utiliser des corpus de textes, vous saurez les traiter de manière un peu plus automatique en fonction de votre problématique.\n",
    "\n",
    "Ceci-dit, le prétraitement du texte est une première étape importante et il faut vraiment observer le contenu de votre corpus après transformation pour être sûr que les données correspondent à ce que vous désirez, en vue des traitements ultérieurs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Vous possédez à présent une première idée des étapes qui constituent le prétraitement du texte : récupération du corpus, tokenisation et première visualisation des différentes fréquences."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mouais.."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code publié n'a pas été testé.\n",
    "\n",
    "Tutos, bouquins sérieux pour apprendre à utiliser NLTK:\n",
    "* https://realpython.com/nltk-nlp-python/\n",
    "* https://www.nltk.org/book/\n",
    "* https://riptutorial.com/nltk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2. Nettoyez et normalisez les données\n",
    "\n",
    "Après la tokenization, voyons comment nettoyer et normaliser notre corpus afin d'obtenir une matrice de vocabulaire et un dictionnaire représentatifs de nos documents."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Première passe de nettoyage : supprimer les stopwords (en français, les mots vides)\n",
    "\n",
    "La première manipulation souvent effectuée dans le traitement du texte est la suppression de ce qu'on appelle en anglais les *stopwords*. Ce sont les mots très courants dans la langue étudiée (\"et\", \"à\", \"le\"... en français) qui **n'apportent pas de valeur informative** pour la compréhension du \"sens\" d'un document et corpus. Il sont très fréquents et ralentissent notre travail : nous souhaitons donc les supprimer.\n",
    "\n",
    "Il existe dans la librairie NLTK une liste par défaut des stopwords dans plusieurs langues, notamment le français. Mais nous allons faire ceci d'une autre manière : on va supprimer les mots les plus fréquents du corpus et considérer qu'il font partie du vocabulaire commun et n'apportent aucune information. Ensuite on supprimera aussi les stopwords fournis par NLTK.\n",
    "\n",
    "Allez, on s'en débarasse !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Premièrement, on récupère la fréquence totale de chaque mot\n",
    "# sur tout le corpus d'artistes\n",
    "freq_totale = nltk.Counter()\n",
    "for k, v in corpora.iteritems():\n",
    "    freq_totale += freq[k]\n",
    "\n",
    "# Deuxièmement on décide manière un peu arbitraire du nombre de mots\n",
    "# les plus fréquents à supprimer.\n",
    "# On pourrait afficher un graphe d'évolution du nombre de mots pour\n",
    "# se rendre compte et avoir une meilleure heuristique. \n",
    "most_freq = zip(*freq2.most_common(100))[0]\n",
    "\n",
    "# On créé notre set de stopwords final qui cumule ainsi les 100 mots\n",
    "# les plus fréquents du corpus ainsi que l'ensemble de stopwords\n",
    "# par défaut présent dans la librairie NLTK\n",
    "sw = set()\n",
    "sw.update(stopwords)\n",
    "sw.update(tuple(nltk.corpus.stopwords.words('french')))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons maintenant le nombre de mots uniques non *stopwords* utilisés par les artistes. Pour rappel, on souhaite comprendre la variété lexicale des rappeurs choisis. Il est donc logique de supprimer les mots les plus utilisés, ce qui signifie par extension qu'ils ne sont pas porteurs de sens.\n",
    "\n",
    "On réeffectue notre tokenisation en ignorant les *stopwords* et on affiche ainsi notre nouveau histogramme des fréquences duquel on a supprimé les stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_stats_corpora2(lookup_table=[]):\n",
    "    corpora = defaultdict(list)\n",
    "    for artist, block_ids in lt_artists.iteritems():\n",
    "        for block_id in block_ids:\n",
    "            tokens = tokenizer.tokenize(db_flat[block_id]['text'].decode('utf-8'))\n",
    "            corpora[artist] += [w for w in tokens if not w in list(sw)]\n",
    "\n",
    "    stats, freq = dict(), dict()\n",
    "\n",
    "    for k, v in corpora.iteritems():\n",
    "        freq[k] = fq = nltk.FreqDist(v)\n",
    "        stats[k] = {'total': len(v), 'unique': len(fq.keys())}\n",
    "    return (freq, stats, corpora)\n",
    "\n",
    "freq2, stats2, corpora2 = freq_stats_corpora2()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img id=\"r-4867956\" data-claire-element-id=\"8982841\" src=\"https://lh6.googleusercontent.com/HaChBwQxXRmQ-s3X_DDUBkK-meICT_crs7Q970w6ooy-YmiIgZ_gMA04gwUHCZ1UZsDBEwDUp69vnN5phwd0s63UvY-uWh0DI9Iww0nhf_bLQ0o0K6xXQt-u2UqAXUyqpQETqPlY\" alt=\"\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a bien un changement dans le classement, maintenant qu'on a enlevé les mots les plus communs, si vous comparez au classement du chapitre précédent."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🛈 J'effectue ce classement à titre d'exercice. Il existe de nombreux autres de critères (taille du répertoire, durée de la carrière, etc.) qui ne sont pas pris en compte."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deuxième passe : lemmatisation ou racinisation (stemming)\n",
    "\n",
    "Plus qu'une dernière étape et vous en aurez terminé avec le prétraitement !\n",
    "\n",
    "Le processus de « lemmatisation » consiste à représenter les mots (ou « lemmes » 😉) sous leur forme canonique. Par exemple pour un verbe, ce sera son infinitif. Pour un nom, son masculin singulier. L'idée étant encore une fois de ne **conserver que le sens des mots** utilisés dans le corpus.\n",
    "\n",
    "Si l'on reprend notre exemple précédent, \"Bonjour, je suis un texte d'exemple pour le cours d'Openclassrooms. Soyez attentifs à ce cours !\"\n",
    "\n",
    "L'idéal serait d'extraire les lemmes suivants : « bonjour, être, texte, exemple, cours, openclassrooms, être, attentif, cours ». Dans le processus de lemmatisation, on transforme donc « suis » en « être»  et « attentifs » en « attentif ».\n",
    "\n",
    "Dans notre cas, je voulais étudier la richesse du vocabulaire des artistes. C'est donc mieux de compter le nombre d'occurrences du verbe être plutôt que de compter séparément chaque usage de conjugaison de ce même verbe. De même pour les pluriels etc. On estime que c'est plus représentatif, j'espère que vous êtes d'accord ! 😏\n",
    "\n",
    "Il existe un autre processus qui exerce une fonction similaire qui s'appelle la **racinisation** (ou *stemming* en anglais). Cela consiste à ne conserver que la racine des mots étudiés. L'idée étant de supprimer les suffixes, préfixes et autres des mots afin de ne conserver que leur origine. C'est un procédé plus simple que la lemmatisation et plus rapide à effectuer puisqu'on tronque les mots essentiellement contrairement à la lemmatisation qui nécessite d'utiliser un dictionnaire.\n",
    "\n",
    "Dans notre cas, on va effectuer une racinisation parce qu'il n'existe pas de fonction de lemmatisation de corpus français dans NLTK 😶 Je suis d'accord que ce serait encore mieux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import FrenchStemmer\n",
    "\n",
    "stemmer = FrenchStemmer()\n",
    "\n",
    "def freq_stats_corpora3(lookup_table=[]):\n",
    "    corpora = defaultdict(list)\n",
    "    for artist, block_ids in lt_artists.iteritems():\n",
    "        for block_id in block_ids:\n",
    "            tokens = tokenizer.tokenize(db_flat[block_id]['text'].decode('utf-8').lower())\n",
    "            corpora[artist] += [stemmer.stem(w) for w in tokens if not w in list(sw)]\n",
    "\n",
    "    stats, freq = dict(), dict()\n",
    "\n",
    "    for k, v in corpora.iteritems():\n",
    "        freq[k] = fq = nltk.FreqDist(v)\n",
    "        stats[k] = {'total': len(v), 'unique': len(fq.keys())}\n",
    "    return (freq, stats, corpora)\n",
    "\n",
    "freq3, stats3, corpora3 = freq_stats_corpora3()\n",
    "df3 = pd.DataFrame.from_dict(stats3, orient='index').sort(columns='unique', ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img id=\"r-4867961\" data-claire-element-id=\"8982852\" src=\"https://lh3.googleusercontent.com/an1lAMmPNmo9QcJk5AHs5bztDOGW6d4j2cUbfKu9u-k1LQoe7tmlefdiuUax_rxyDtSaUtGLsNcEgzFfWQRA-ZVPr0i-wjHQtZTfDxEbqlX0QX6JS5zM8PTHebQ8fyv6FICD4ng0\" alt=\"\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a ici utilisé des étapes de nettoyage classique de texte mais en réalité, il est parfois utile de conserver le texte brut quand on est pas dans une recherche du sens d'un document mais de phrases dans leur ensemble puisqu'on cherche le lien entre les différents mots. On aura un aperçu de cet aspect dans les prochains chapitres."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Vous avez effectué quelques étapes essentielles du prétraitement du texte : tokenisation, suppression des stop-words, lemmatisation et stemming. Nous pouvons maintenant passer à la création de notre ensemble de features représentatives de notre corpus de texte. C'est le sujet de la prochaine partie ! Suivez-moi."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3. TP - Faites vos premiers pas dans l'analyse de données textuelles\n",
    "\n",
    "Vidéo d'introduction (2:49) : https://vimeo.com/746902007"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contexte\n",
    "\n",
    "Pour ce premier TP, nous allons travailler sur un jeu de données simple mais très intéressant. Il s’agit d’un corpus de tweets, pour lesquels il s'agit de prédire s’ils font référence à une “catastrophe” ou non.\n",
    "\n",
    "Il s’agit d’une compétition Kaggle. Pour ceux qui ne connaissent pas encore Kaggle, c’est LA plateforme web orientée Data Science. Elle héberge des datasets, des notebooks et des compétitions. C’est aussi un réseau social, sur lequel vous pouvez notamment publier vos notebooks.\n",
    "\n",
    "Le jeu de données peut être trouvé [à cette adresse](https://www.kaggle.com/competitions/nlp-getting-started/data)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🛈 Nous travaillerons uniquement sur le jeu de données de **train**. \n",
    "\n",
    "La colonne “**target**” fait référence à la nature du tweet : “catastrophe” = 1, “pas catastrophe” = 0."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consignes\n",
    "\n",
    "Le TP se décompose en 2 parties :\n",
    "\n",
    "1 - **Exploratory Data Analysis** : il vous est demandé de faire un premier notebook afin de **comprendre, d’explorer et d’effectuer un premier nettoyage** des données. Vous devez notamment être capable de répondre aux questions suivantes : \n",
    "* Quelle est la forme du Dataframe ? \n",
    "* Y a t-il des valeurs manquantes ou des valeurs dupliquées ? \n",
    "* Quelles sont les colonnes qui vont nous intéresser ? \n",
    "* Y a-t-il des données aberrantes ou des incohérences majeures dans les données ? \n",
    "* Y a t-il des tweets anormalement longs / courts ? Peut-on les considérer comme des outliers ? \n",
    "* Quel est le ratio tweet qui parlent de “catastrophes” / tweet normaux ?\n",
    "* En regardant quelques tweets au hasard, peut-on deviner facilement la “target” ? \n",
    "* Peut-on déjà détecter des “patterns” ou des mots clés dans les tweets?\n",
    "* A votre avis quel serait l’accuracy score qu’un humain pourrait obtenir s’il prédisait  les données “à la main” ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🛈 Le temps indicatif proposé pour ce travail est de 30 min à 1 heure. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - **Text Processing** : Il vous est demandé d’effectuer un premier traitement des données textuelles (colonne ‘text’). Il s’agira de transformer les données textuelles en **tokens** et de **réduire la dimensionnalité du corpus** en réduisant le vocabulaire (le nombre de tokens différents). L’enjeu est complexe, il en faut ni trop, ni trop peu… Pour vous aider dans ce travail, essayez de répondre aux questions suivantes : \n",
    "\n",
    "* Pouvez-vous écrire une fonction qui : tokenize un document, supprime les stopwords, supprime les tokens de moins de 3 lettres ?\n",
    "* Comment peut-on reconstituer le corpus (c'est-à dire un texte avec l’ensemble des documents) ? \n",
    "* Une fois ce corpus constitué, combien de tokens uniques le constitue? Ce nombre vous apparaît-il faible, important, gigantesque ?\n",
    "* Comment réduire ce nombre de tokens uniques, ou autrement dit “comment réduire la taille du vocabulaire” de ce corpus ? \n",
    "* Combien de tokens sont présents une seule fois ? Ces tokens nous seront-ils utiles ? \n",
    "* Appliquer une méthode de stemmatisation ou de lemmatisation peut-elle nous aider à réduire la dimensionnalité du corpus ? \n",
    "* Comment visualiser graphiquement, par un WordCloud par exemple, les tokens les plus présents ? \n",
    "* Pouvez vous appliquer tous les traitements évoqués afin de créer une nouvelle colonne “text” qui serait plus pertinente ? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🛈 Le temps indicatif proposé pour ce travail est très variable. Il dépend notamment de votre connaissance du sujet mais aussi et surtout de votre degré d'exigence. Certains pourront y passer 2 ou 4h, d'autres une journée entière. Si vous vous sentez bloqués, perdus, ou vous ne savez pas conclure le travail, pas de soucis! Les vidéos ci dessous sont là pour ça."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠ Avant de visionner les vidéos ci dessous, assurez vous d'avoir essayé de faire le travail par vous même ! En effet, c'est durant cette phase que vous apprendrez le plus."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour l'EDA : https://vimeo.com/746901784 (32:02)\n",
    "\n",
    "penser à utiliser davantage seaborn + à faire des sauvegardes de mes résultats intermédiaires.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le Text Processing : https://vimeo.com/746902006 (43:06)\n",
    "\n",
    "libs à connaître :\n",
    "* wordcloud, pillow : graphismes\n",
    "* pandarallel : exécution multi-cpu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aller plus loin :\n",
    "\n",
    "Nous n’avons volontairement pas évoqué dans le cadre de ce TP plusieurs sujets. Nous vous laissons le soin de poursuivre librement les pistes évoquées ci dessous :\n",
    "\n",
    "* Nous avons choisi de ne garder que les “mots” au sens grammatical du terme. Mais est-ce bien judicieux ? En effet, l’utilisation d’un **emoji** ou de certains **caractères de ponctuation** peut être très impactant. Par exemple:  “ terrorist attack downtown !!! 😱😱😱”\n",
    "* Nous avons choisi de transformer les documents avec la méthode .lower(). Mais est-ce bien judicieux ? En effet, les **lettres capitales** sont peut-être plus utilisées dans des textes ayant un impact fort. Par exemple : OMG, THE BUILDING IS BURNING !!! \n",
    "* Nous n’avons pas évoqué la notion de **bi-gramme** ou de **tri-gramme** (groupe de 2 ou 3 mots se faisant suite). Existe-t-il des bi ou tri-grammes qui seraient intéressants ?\n",
    "* Le **stemmer et le lemmentizer*** de NLTK ne sont pas très “puissants”. Est-ce que la librairie spacy nous propose des outils plus intéressants ? \n",
    "* Qu'est-ce que le **POS** (part of speech) ? Spacy peut-il nous aider à ne garder que les tokens faisant référence aux adjectifs ou aux verbes? Cela peut-il avoir un impact sur la taille de notre vocabulaire ? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🛈 N'hésitez pas à rendre votre travail public ! Ce travail est le votre, vous pouvez mettre votre notebook sur Kaggle ou sur github. Cela vous permettra de le rendre disponible pour un futur recruteur, de pouvoir le retrouver facilement ou encore d'inspirer de futurs apprenants en Machine Learning !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ressources complémentaires :\n",
    "\n",
    "Voici une liste non exhaustive de ressources complémentaires pour poursuivre votre travail :\n",
    "\n",
    "* Une vidéo de [freecodecamp sur NLTK](https://www.youtube.com/watch?v=X2vAabgKiuM) (38:09 en anglais)\n",
    "* Une vidéo de [freecodecamp sur Spacy](https://www.youtube.com/watch?v=dIUTsFT2MeQ) : (3:02:32 en anglais)\n",
    "* ✔ Une vidéo de [David Louapre (Science étonnante) sur le SOA (State Of Art) du NLP](https://www.youtube.com/watch?v=CsQNF9s78Nc) (26:15). ATTENTION : la vidéo couvre des notions beaucoup plus complexes que celles présentées dans la première partie de cours. Ces notions seront couvertes dans les chapitres suivants.\n",
    "    * https://nlp.stanford.edu/projects/glove/\n",
    "* Quelques notebooks Kaggle à lire :  [un premier notebook](https://www.kaggle.com/code/longtng/nlp-preprocessing-feature-extraction-methods-a-z), [un deuxième notebook](https://www.kaggle.com/code/dikshabhati2002/nlp-for-beginners), [**un troisième notebook**](https://www.kaggle.com/code/ashagutlapalli/nlp-101-with-nltk-and-spacy-text-analysis) (en anglais. ATTENTION : certaines notions présentes dans ces notebooks sont beaucoup plus complexes que celles présentées dans la première partie de cours. Ces notions seront couvertes dans les chapitres suivants. \n",
    "\n",
    "## Commentaires, suggestions ou questions :\n",
    "\n",
    "N’hésitez pas à nous faire un retour : nous contacter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  2.1. Représentez votre corpus en \"bag of words\" "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vidéo introductive : https://vimeo.com/284320353\n",
    "\n",
    "Après avoir vu les différents types de nettoyage du texte possible dans les chapitres précédent, nous allons maintenant étudier comment extraire l'information du texte pour le traitement ultérieur par des modèles de machine learning. En d'autres termes, nous cherchons une représentation du langage pour un modèle statistique qui vise à exploiter des données textes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qu'est-ce qu'un « bag of words »\n",
    "\n",
    "La manière la plus simple de représenter un document, c'est ce qu'on a effectué dans le chapitre précédent où l'on a considéré tous les mots utilisés pour chaque artiste, sans distinction ni dépendance par vers, chanson, etc. L'analogie est donc qu'on a considéré chaque artiste par la représentation brute d'un \"sac\" de tous les mots qu'il a utilisé, sans soucis de contexte (ordre, utilisation, etc).\n",
    "\n",
    "On peut faire la même chose à l'échelle d'un document qu'on représente par un ensemble des mots qu'il contient. En pratique, ça peut être par exemple un vecteur de fréquence d'apparition des différents mots utilisés (ou stem 😉).\n",
    "\n",
    "Une représentation bag-of-words classique sera donc celle dans laquelle on représente chaque document par un vecteur de la taille du vocabulaire $|V|$ et on utilisera la matrice composée de l’ensemble de ces $n$ documents qui forment le corpus comme entrée de nos algorithmes.\n",
    "\n",
    "Vidéo : https://vimeo.com/284320371\n",
    "\n",
    "* Analyse des co-occurrences : bigrammes, etc (n-grammes).\n",
    "* probabilité conditionnelle : elle n'est pas expliquée clairement\n",
    "* TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "* TN-SNE\n",
    "* Named Entity Recognition\n",
    "* Extraction des relations, des événements\n",
    "* POS tagging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prendre en compte les co-occurences\n",
    "\n",
    "La première chose à considérer, au delà d'une tokenisation, c'est qu'il est possible de séparer le texte en groupes de plusieurs mots. On appelle les groupes de mots les n-grammes (n-gram) : bigrammes pour les couples de mots, trigrammes pour les groupes de 3, etc. Séparer en mot unique est en fait un cas particulier appelé unigrammes.\n",
    "\n",
    "Par exemple dans la phrase : « Je mange une pomme », on peut extraire les bigrammes {(je, mange), (mange, une) et (une, pomme)}\n",
    "\n",
    "**Pourquoi utiliser des n-grammes avec n>1 ?**\n",
    "\n",
    "Lorsqu'on fait face à une problématique de modélisation du langage, on voit bien que pour étudier idéalement le sens d'un mot il faudrait l'observer dans son contexte. Il existe donc dans un texte (et par extension dans le langage) une forme de dépendance plus ou moins grande entre les mots.\n",
    "\n",
    "A titre d'exemple, le pronom \"je\" aura grandement plus de chance d'être suivi d'un verbe. On peut donc traiter chaque mot comme ayant une probabilité d'apparition en fonction du texte qui le précède, c'est à dire comme une séquence. Dans l'idéal, on veut traiter tout le texte de cette façon, mais ce n'est pas possible en terme de capacité de calculs.\n",
    "\n",
    "En pratique, on peut prendre les quelques mots précédents qui représentent assez d'information pour avoir un modèle séquentiel (markovien) intéressant, d'où l'apparition des n-grammes.\n",
    "\n",
    "Par exemple on peut assigner une probabilité au bigramme (\"je\", \"mange\") :\n",
    "\n",
    "$$p(mange \\vert\\ je) = \\frac{p(mange, je)}{p(mange) p(je)}$$\n",
    "\n",
    "En pratique, on peut aussi utiliser la fonction `bigrams` de NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"Bonjour, je suis un texte d'exemple pour le cours d'Openclassrooms. Soyez attentifs à ce cours !\"\n",
    "tokens = tokenizer.tokenize(test.lower())\n",
    "list(nltk.bigrams(tokens))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🛈 Le modèle de bag-of-words est en fait un cas particulier du modèle n-gram avec n=1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Une autre manière de pondérer : le tf-idf\n",
    "\n",
    "Depuis le départ, on a seulement utilisé les fréquences d'apparition des différents mots/n-grammes présents dans notre corpus. Le problème est que si l'on veut vraiment représenter un document par les n-grammes qu'il contient, il faudrait le faire relativement à leur apparition dans les autres documents.\n",
    "\n",
    "En effet, si un mot apparait dans d'autres documents, il est donc moins représentatif du document qu'un mot qui n'apparait que uniquement dans ce document.\n",
    "\n",
    "Nous avons d'abord supprimé les mots les plus fréquents de manière générale dans le langage (les fameux stopwords). À présent, il ne faut pas considérer le poids d'un mot dans un document comme sa fréquence d'apparition uniquement, mais pondérer cette fréquence par un indicateur **si ce mot est commun ou rare** dans tous les documents.\n",
    "\n",
    "Pour résumer, le poids du n-gramme est le suivant :\n",
    "\n",
    "$\\text{poids}=\\text{fréquence du terme}×\\text{indicateur similarité}$\n",
    "\n",
    "En l’occurence, la métrique tf-idf (Term-Frequency - Inverse Document Frequency) utilise comme indicateur de similarité l'inverse document frequency qui est l'inverse de la proportion de document qui contient le terme, à l'échelle logarithmique. Il est appelé logiquement « inverse document frequency » (idf). \n",
    "\n",
    "Nous calculons donc le poids tf-idf final attribué au n-gramme :\n",
    "\n",
    "$\\text{poids}=\\text{fréquence du n-gram}×\\text{idf(n-gramme)}$\n",
    "\n",
    "Dans notre exemple, un document égale un artiste. Pour connaître les termes qui représentent le plus un artiste, nous allons utiliser la fonction tf-idf de scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        file_path = subdir + os.path.sep + file\n",
    "        shakes = open(file_path, 'r')\n",
    "        text = shakes.read()\n",
    "        lowers = text.lower()\n",
    "        no_punctuation = lowers.translate(None, string.punctuation)\n",
    "        token_dict[file] = no_punctuation\n",
    " \n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words=sw)\n",
    "values = tfidf.fit_transform(token_dict.values())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant qu'on a cet indicateur, on peut comparer les différents champs lexicaux qui représentent le plus un artiste. Qui dit comparaison dit similarité. On peut donc utiliser ... t-SNE !\n",
    "\n",
    "<figure id=\"r-5174005\" data-claire-element-id=\"30532046\"><img id=\"r-4868002\" data-claire-element-id=\"8982984\" src=\"https://user.oc-static.com/upload/2017/12/11/15129745591631_t-sne.png\" alt=\"Première visualisation t-sne de notre corpus\" /><figcaption>Première visualisation t-sne de notre corpus</figcaption></figure>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraire les informations\n",
    "\n",
    "La récupération de caractéristiques va assez loin puisqu'on essaie de dégager de nos documents texte non structurés des informations structurées informatives très restreintes :\n",
    "\n",
    "* **NER (Named Entity Recognition)** : reconnaître des personnes, endroits, entreprises, etc.\n",
    "* **Extraction de relations** : essayer d'extraire des relations sémantiques entre différents termes du texte. Par exemple, des relations familiales (\"Marie est l'enfant de Patrick\") spatiales (\"Le piano est devant la fenêtre\"), etc. Ces informations peuvent ensuite être stockées dans une base de données relationnelles ou un graphe.\n",
    "* **Extraction d'événements** : extraire des actions qui arrivent à nos entités. Par exemple \"le cours de l'action X a augmenté de 5%\" ou bien \"le président à déclaré X dans son discours\"\n",
    "* **POS Tagging (Part-of-Speech Tagging)** : représente les méthodes qui récupèrent la nature grammatical des mots d’une phrase - nom, verbe, adjectif, etc. Ce sont des propriété qui peuvent servir de caractéristiques utile lors de la création de certains modèles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🛈 Ce ne sont pas des méthodes que l’on va traiter dans ce cours. Nous vous donnerons cependant les bases pour pouvoir vous documenter et créer votre propre tagger. Notez qu'en français, il existe peu de ressources toute faites, contrairement à l'anglais où il existe des NER généralistes.\n",
    "\n",
    "<img id=\"r-4868038\" data-claire-element-id=\"8983048\" src=\"https://lh6.googleusercontent.com/fOK5o_kpnRxaW_F0YiEeK85GNHrzSWDXdVn0hvwgJf7Wqau-5l-vh0JnM_ibe34ErAd3efpYCfaOdPVV-Qk2HR55vpitHM4B8jGhsq7j94e2lH2Xv2MglkTHh-HRwV-oGs93he0U\" alt=\"\" />\n",
    "\n",
    "*Vous pouvez essayer l'API Google Natural Langage pour avoir une idée des capacité d'extraction d'information possible par les algorithmes industriels.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention aux matrices creuses\n",
    "\n",
    "Avec les méthodes de comptage évoquées, nous créons en réalité des « matrices creuses ». En effet, les mots ne sont pas présents dans chaque document (le ratio vocabulaire / taille de document est trop élevé). De plus, on utilise plus souvent certains mots (“et”, “le”, etc.) et d’autres plus rarement (dans des contextes précis).\n",
    "\n",
    "Cette grosse différence créé des matrices larges (de la taille « nombre de documents * taille du vocabulaire ») qui sont essentiellement vides. On verra qu’on peut utiliser ces matrices avec un certain nombre d’algorithmes, mais c’est tout de même un gaspillage non négligeable de ressources que de travailler avec des matrices de cette taille alors que la plupart des entrées ne sont pas informatives.\n",
    "\n",
    "De plus, les matrices creuses peuvent biaiser les algorithmes qui considèrent ainsi que les observations à zéro (qui sont présentes en majorité) représente une information à prendre en considération. Si on pense en terme de moyenne par exemple, elle sera écrasée par la présence de toute ces entrées vides sans pour autant apporter plus de sens à notre calcul. Nous allons voir dans les prochaines parties des alternatives de représentation de texte afin de contrecarrer ce problème quand c'est nécessaire."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Ces extractions de caractéristiques seront assez intuitives en fonction du problème rencontré. L'idée principale étant de transformer cette masse de texte non structurée en données digestes pour vos algorithmes et vos capacités de calculs. Cependant, ce type de représentation créé des matrices creuses qu’il est parfois difficile à gérer, par exemple dans le cadre de l’utilisation de réseaux de neurones."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Effectuez des plongements de mots (word embeddings)\n",
    "\n",
    "Vidéo introductive : https://vimeo.com/284320383\n",
    "\n",
    "Les algorithmes et modèles de machine learning utilisent des entrées numériques, puisque nous travaillons avec des espaces topologiques voire métriques la plupart du temps.\n",
    "\n",
    "? Alors comment faire pour représenter du texte.. ?\n",
    "\n",
    "Lorsqu’on traite des données brutes à grande dimensions comme des images ou du spectre audio, on utilise directement des vecteurs de coefficients associées aux intensité de pixel dans le premier cas ou les coefficients de densité spectrales dans le second cas.\n",
    "\n",
    "Le problème avec un texte, c’est que l’on va traiter les mots et groupes de mots comme des entités symboliques non porteuses de sens, c’est à dire indépendantes les unes des autres.\n",
    "\n",
    "<img id=\"r-4868041\" data-claire-element-id=\"8983061\" src=\"https://lh5.googleusercontent.com/ucKgKMHzkUwGqm9wWn-1nTpLIEyhywOQF1q42TwwL49QiKCYp4wrcQ67qOJqmZu-5oxNxDHf6txQ6OZDsKvT_FHuCEIK4kgzYxhvTTv6N7XPp8y5LaJhKTmSEvQ3G04XlVAC9JEu\" alt=\"\" />\n",
    "\n",
    "🛈 On appelle la technique de représentation d’un mot, ou un ensemble de mots en vecteurs de dimension inférieure, word embeddings, soit littéralement « plongement de mot ».\n",
    "\n",
    "Dans les chapitre précédents,  nous avons vu une première manière de représenter nos documents comme des bag-of-words ou bag-of-ngrams, essentiellement des méthodes de comptage direct (fréquence ou tf-idf). On a donc techniquement représenté chaque document par un vecteur de fréquences du dictionnaire de mots que l’on avait à disposition. Nous sommes amené à traiter des données lacunaires ou creuses (en anglais sparse). Même avec TF-IDF, on considère des comptages déterministes comme représentatifs de nos données, en l’occurrence la fréquence du mot et la fréquence inverse du document.\n",
    "\n",
    "Une récente famille de techniques (circa 2013) a permi de repenser ce modèle avec une représentation des mots dans un espace avec une forme de similarité entre eux (c'est-à-dire probabiliste), dans lesquels le sens des mots les rapproche dans cet espace, en terme de distances statistiques. C’est un plongement dans un espace de dimension inférieur autour de 20-100 dimensions généralement. Son petit nom : word2vec.\n",
    "\n",
    "<img id=\"r-4868046\" data-claire-element-id=\"8983072\" src=\"https://lh4.googleusercontent.com/u3CG-wlVB5QyyRrg0_26HgCELgQLTAS6SYzygiPH7SeRQy-2B7dpZyLjX4l8Izy_XIoTPH6gvAPBNDqmx9vw8x5hYlYT9KNjFSSq5C1jfgpHyIUBEPr_mS7A9sDBHgZYGufB-PaU\" alt=\"\" />\n",
    "\n",
    "Mais comment trouver le sens des mots ? C’est un peu vague comme concept, non ?\n",
    "\n",
    "Effectivement. L’hypothèse principale de ces méthodes étant de prendre en compte le “contexte” dans lequel le mot a été trouvé, c’est à dire les mots avec lesquels il est souvent utilisé. On appelle cette hypothèse distributional hypothesis.\n",
    "\n",
    "Et ce qui est intéressant, c’est que ce contexte permet de créer un espace qui rapproche des mots qui ne se sont pas forcément trouvés à côté les uns des autres dans un corpus ! Ces méthodes de représentation vectorielles ont aussi permis d’entraîner des modèles de représentation des mots sur des corpus beaucoup plus grands (des centaines de milliards de mots par exemple...)\n",
    "\n",
    "Ces représentations possèdent des capacités surprenantes. Par exemple, on peut retrouver beaucoup de régularités linguistiques simplement en effectuant des translation linéaires dans cet espace de représentation. Par exemple le résultat de vec(“Madrid”) - vec(“Spain”) + vec(“France”) donne une position dont le vecteur le plus proche est vec(“Paris”) !\n",
    "\n",
    "<img id=\"r-4868051\" data-claire-element-id=\"8983091\" src=\"https://lh6.googleusercontent.com/xvk-_fxvJp2atVExddjkGBLgUNKJKjbJexMTn7vQRdDNJBZo-eDM9dNN4ZtYzcphYdRRSGhm7NFp-kOD4UvxdzJTeSLye6a3_J_U4p5Rkyis82DwQvg40qtG-IwwSZwFM0RdDPYb\" alt=\"\" />\n",
    "\n",
    "Afin de calculer les vecteurs qui représentent les mots, les méthodes word2vec utilisent des perceptrons linéaires simples avec une seule couche cachée. L’idée est de compresser notre corpus vers un dictionnaire de vecteurs denses de dimension bien inférieure choisie.\n",
    "\n",
    "Je n’ai pas l’impression que le contexte a été pris en compte, si ?\n",
    "\n",
    "On ne va pas détailler les méthodes d’entraînement en détails, mais il faut savoir qu’il en existe deux principales. La première appelée « Continuous Bag of Words » (CBOW), qui entraîne le réseau de neurones pour prédire un mot en fonction de son contexte, c’est à dire les mots avant/après dans une phrase. Dans la seconde méthode, on essaie de prédire le contexte en fonction du mot. C’est la technique du « skip-gram ».\n",
    "\n",
    "En d’autres termes, l’entrée du réseau de neurones dans le cadre du CBOW prend une fenêtre autour du mot et essaie de prédire le mot en sortie. Dans le cadre du skip-gram on essaie de faire l’inverse, c’est-à-dire prédire les mots autour sur une fenêtre déterminée à l’avance à l’aide du mot étudié en entrée.\n",
    "\n",
    "<img id=\"r-4880386\" data-claire-element-id=\"9012501\" src=\"https://lh4.googleusercontent.com/kNbiyzjUV37jfNv65bkh4uF9zFIvso1I3DI5ccx56JP_bgjp1p92ev9bfotRSkNI7ljk-OPQGGX6Ci8oT3fVNBgaW2ZVRe9qvAUUuK7oiZ35spqtjTv1raw-F6MZf0T9Pxnuag4G\" alt=\"Représentation du modèle CBOW\" />\n",
    "\n",
    "Et c’est cette hypothèse - le fait que les mots soient caractérisés par les mots les entourants - qui permet de créer cette compression. Des mots davantage associés aux mêmes mots seront proches dans l’espace d’arrivée."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utiliser un modèle de langage\n",
    "\n",
    "Avoir cette représentation vectorielle des mots permet d’utiliser ces vecteurs comme des features dans un grand nombre de tâches de base de traitement du langage. On peut ainsi alimenter des algorithmes classiques tels qu’un SVM ou un réseau de neurones avec nos vecteurs caractéristiques des mots.\n",
    "\n",
    "Pour récapituler, on peut transformer un texte en ses features soit :\n",
    "* En utilisant une représentation de comptage creuse - fréquence d’apparition du mot dans un document, ou vecteur tf-idf d’un document, etc.\n",
    "* En utilisant une représentation type word2vec dense - dans laquelle le mot possède une représentation dans un espace qui le positionne en fonction des mots adjacents\n",
    "\n",
    "Nous verrons dans les prochains chapitres comment utiliser ces représentations de documents dans nos algorithmes afin de pouvoir effectuer différentes tâches de classification, modélisations non supervisées et autres manipulations propres au traitement du langage."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatives à word2vec\n",
    "\n",
    "En pratique, on peut utiliser d’autres types de représentations denses des mots, au-delà du choix de l’algorithme (CBOW, skipgram) et de la dimension. Il existe notamment d’autres méthodes de plongement (word embeddings) tels que [gloVe](https://nlp.stanford.edu/projects/glove/) et [FastText](https://fasttext.cc/).\n",
    "\n",
    "[Certains favorisent](https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/) même l’utilisation d’une simple décomposition SVD sur une matrice PMI (pointwise mutual information) qui donneraient des performances largement suffisante pour la plupart des applications industrielles.\n",
    "\n",
    "L’idée fondamentale reste la même : compresser de manière non supervisée la représentation d’un mot à partir d’un gros corpus de texte représentatif de votre langage, afin d’obtenir un vecteur dense qui permet de visualiser et de fournir à nos algorithmes des features plus intéressantes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraîner son propre embedding\n",
    "\n",
    "Il existe plusieurs plongements disponibles directement en ligne (surtout en anglais) qui ont été entrainés avec les méthodes évoquées plus haut. C’est utile pour avoir un modèle de représentation très général de votre vocabulaire sur une langue en particulier. Cependant vous êtes limité au registre du corpus utilisé. Par exemple si l’embedding a été réalisé sur Wikipédia, vous allez avoir un registre relativement soutenu. Cela biaise un peu la modélisation et surtout rend moins précis votre plongement vis à vis du problème rencontré. Cela a aussi pour effet de dissiper des différences entre vecteurs qui pourraient être utiles pour votre problème en faveur des grandes disparités (comme l’exemple masculin / féminin ou capitale / pays).\n",
    "\n",
    "L’avantage d’entraîner son propre embedding est donc d’avoir un plongement spécifique à notre corpus et donc plus performant concernant la problématique que l’on veut traiter. Vous pouvez ensuite comparer cet embedding à une baseline utilisant un embedding général.\n",
    "\n",
    "Vous pouvez par exemple entraîner votre propre embedding word2vec en suivant ce tutorial : https://radimrehurek.com/gensim/auto_examples/\n",
    "\n",
    "Le problème qui en découle est bien sûr un manque de généralisation si le jeu de données utilisé n’est pas représentatif de la population totale."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Les plongements de mots sont les représentations les plus utilisées actuellement dans les dernières méthodes de traitement du langage. C'est devenu un outil incontournable à tester lors de vos manipulations de texte. Si vous avez le temps et les ressources nécessaires, n'hésitez pas à entraîner votre propre embedding spécialisé sur votre problématique."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Modélisez des sujets avec des méthodes non supervisées\n",
    "\n",
    "Vidéo d'introduction : https://vimeo.com/284320403\n",
    "\n",
    "Dans le monde actuel, où la quantité de texte non structuré, augmente drastiquement (commentaires, articles de blog, etc.), il serait vraiment utile d’avoir des outils qui permettent de structurer automatiquement l’information, de manière à pouvoir rapidement accéder à ce qui nous intéresse, filtrer le bruit mais aussi détecter l’apparition de nouveau sujet d’intérêts.\n",
    "\n",
    "On peut retrouver cette nécessité à plusieurs niveau dans les applications :\n",
    "* Détecter les fameux “trending topics” de Twitter\n",
    "* Trouver les nouveaux sujets d’information abordés par les médias\n",
    "* Détecter un nouvel investissement intéressant d’après un groupement de textes d’experts\n",
    "* Organiser un corpus de textes scientifiques autour des thématiques abordées\n",
    "* Trouver les différentes aspects d’un produit abordés par des commentaires afin de pouvoir plus facilement l’améliorer à partir de feedback utilisateur\n",
    "\n",
    "<img id=\"r-4883082\" data-claire-element-id=\"9020716\" src=\"https://user.oc-static.com/upload/2017/12/13/15131779850317_Screen%20Shot%202017-11-28%20at%2015.44.16.png\" alt=\"Les « trending topics » de Twitter\" />\n",
    "Les « trending topics » de Twitter\n",
    "\n",
    "C’est dans ce cadre qu’intervient la modélisation de sujets (*topic modeling* en anglais) qui représente le spectre des différentes approches permettant cette détection.\n",
    "\n",
    "Dans ce chapitre, on va étudier les plus populaires, afin d’avoir une intuition de cette famille d’algorithmes. Il faut savoir, en revanche, qu’il reste difficile d’appliquer directement ces algorithmes à toute les situations et qu’il existe un grand nombre de variantes spécifiques à des problématiques plus précises qui correspondront à ce que vous recherchez.\n",
    "\n",
    "Il faut donc se documenter et comprendre les différents critères différenciants (e.g. modélisation dynamique des sujets dans le temps, longueur du document, nombre de sujets abordés, etc.) qui vous permettront d’effectuer un choix informé.\n",
    "\n",
    "C’est aussi une famille de méthode utilisée essentiellement en exploration voire semi-supervisée, c’est à dire qui permet de détecter si effectivement il y a de grandes catégories abordées, et ensuite les affiner lors du passage en production, et supervision des nouveaux documents entrants.\n",
    "\n",
    "Pour résumer, la modélisation automatique de sujets permet de détecter les sujets latents abordés dans un corpus de documents, assigner les sujets détectés à ces différents documents. On peut ensuite utiliser ces sujets pour effectuer des recherches plus rapide, organiser les documents ou les résumer automatiquement."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Première intuition : Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "La première méthode vraiment efficace est nommé **LDA** (*Latent Dirichlet Allocation*). C’est une méthode non-supervisée générative qui se base sur les hypothèses suivantes :\n",
    "* Chaque document du corpus est un ensemble de mots sans ordre (*bag-of-words*) ;\n",
    "* Chaque document $m$ aborde un certain nombre de thèmes dans différentes proportions qui lui sont propres $p(\\theta_m)$ ;\n",
    "* Chaque mot possède une distribution associée à chaque thème $(p(\\phi_k)$. On peut ainsi représenter chaque thème par une probabilité sur chaque mot.\n",
    "* $z_n$ représente le thème du mot $w_n$\n",
    "\n",
    "Puisque l'on a accès uniquement aux documents, on doit déterminer quels sont les thèmes, les distributions de chaque mot sur les thèmes, la fréquence d’apparition de chaque thème sur le corpus.\n",
    "\n",
    "Une représentation formelle sous forme de modèle probabiliste graphique est la suivante :\n",
    "\n",
    "<img id=\"r-4883117\" data-claire-element-id=\"9020814\" src=\"https://lh3.googleusercontent.com/BGQYdx1LDOr4fwpBzeI5IJbspqkb2xLRhGewRKtChQkK15KGnW9I9GLpEHpCYKp13jL2ZHZTMe9PUqNI7MZYPBKa-oOIQUlEQnZuGF3u7hH3kOnOlm-c8b-L9T2k3CXhOUt3GJBp\" alt=\"Modèle probabiliste\" />\n",
    "\n",
    "Pour rappel, un modèle génératif définit une probabilité de distribution jointe sur les différentes variables identifiées, à la fois observées et latentes. Une fois ces variables identifiées, ainsi que les différentes probabilités de distribution associées, l’objectif est de retrouver par exemple les distributions latentes par rapport aux variables observées.\n",
    "\n",
    "Dans notre cas, nous souhaitons retrouver les distributions de mots sur les différents thèmes, les différentes proportions de thèmes pour chaque document, les proportions d’apparition d’un thème sur le corpus. Tout cela à partir des différents documents. Ce qui nous permet par la suite de déterminer le thème d’un document, les mots les plus associés à certains thèmes, etc.\n",
    "\n",
    "<img id=\"r-4868062\" data-claire-element-id=\"8983123\" src=\"https://lh5.googleusercontent.com/oLha3ucca8lMSS1mvpOPpJuYsRYgJqYncdcOPz5HGn-iZxt1qENFUBlVDCGPjaSQ5qf-rPAM1AUk4zlUa9is6Rk2QTkc6jN4ZG9iSqJQjJl9fDsfXbZCzaHjv9Agif2MMBzvenxW\" alt=\"\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🛈 La **loi Dirichlet** est la conjuguée de la loi multinomiale, ce qui signifie qu’elle s’accorde bien avec cette loi en tant que distribution a posteriori en termes de factorisation. On utilise ainsi la distribution de Dirichlet sur la proportion globale des thèmes ainsi que sur chaque distribution de thèmes sur les mots."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous l'avons dit en introduction, ce modèle représente une version de base assez générale de la modélisation d’un corpus de documents. Il est possible d’ajouter des hypothèses supplémentaires sur la structure des données afin de capturer une plus grande partie de l’organisation de l’information. A titre d’exemple, voici une liste des hypothèses supplémentaires qui mènent à une modélisation plus riche :\n",
    "* **La distribution des mots sur les thèmes évoluent avec le temps**. Ce qui signifie qu’il faut créer une séquence de distribution pour chaque thème qui permet de modéliser l’évolution du thème dans le temps.\n",
    "* **Certains thèmes sont plus proches que d’autres**. L’hypothèse d’utiliser la distribution de Dirichlet considère que les différents thèmes sont complètement indépendants alors qu’en réalité certains thèmes ont en général plus de chances d’apparaître ensemble."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laissez la partie inférence aux librairies !\n",
    "\n",
    "Effectuer l’inférence de ce modèle est relativement complexe techniquement. Il faut notamment passer par des approximations et des algorithmes qui simplifient le modèle afin de pouvoir le calculer (par exemple mean field ou gibbs sampling). Dans tous les cas on va utiliser des packages tout fait afin de travailler sur nos données.\n",
    "\n",
    "Les plus connus sont déjà intégrés directement dans les librairies (scikit implémente une version de LDA) mais il faudra par la suite effectuer vos propres recherche afin de trouver des implémentations que vous pourrez utiliser dans des cas plus précis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utiliser LDA sur un cas pratique : Le newsgroup dataset\n",
    "\n",
    "Afin d’illustrer le type de retours que l’on peut avoir avec ce genre de méthodes, on va appliquer l’algorithme du LDA sur un dataset classique déjà présent dans la librairie scikit : le newsgroup dataset qui regroupe un ensemble de 20,000 document articles d'actualité.\n",
    "\n",
    "Vidéo : https://vimeo.com/284320427"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prétraitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Créer le modèle LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(learning_method=&#x27;online&#x27;, learning_offset=50.0,\n",
       "                          max_iter=5, n_components=20, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(learning_method=&#x27;online&#x27;, learning_offset=50.0,\n",
       "                          max_iter=5, n_components=20, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LatentDirichletAllocation(learning_method='online', learning_offset=50.0,\n",
       "                          max_iter=5, n_components=20, random_state=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "n_topics = 20\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tf_vectorizer = CountVectorizer(\n",
    "    max_df=0.95,\n",
    "    min_df=2,\n",
    "    max_features=1_000,\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Créer le modèle LDA\n",
    "lda = LatentDirichletAllocation(\n",
    "        n_components=n_topics, \n",
    "        max_iter=5, \n",
    "        learning_method='online', \n",
    "        learning_offset=50.,\n",
    "        random_state=0)\n",
    "\n",
    "# Fitter sur les données\n",
    "lda.fit(tf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "On affiche les mots les plus représentatifs des sujets modélisés, afin de nous donner une idée de leur signification et voir si effectivement on trouve des catégories claires pour les humains et représentatifs de notre corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "people gun state control right guns crime states law police\n",
      "Topic 1:\n",
      "time question book years did like don space answer just\n",
      "Topic 2:\n",
      "mr line rules science stephanopoulos title current define int yes\n",
      "Topic 3:\n",
      "key chip keys clipper encryption number des algorithm use bit\n",
      "Topic 4:\n",
      "edu com cs vs w7 cx mail uk 17 send\n",
      "Topic 5:\n",
      "use does window problem way used point different case value\n",
      "Topic 6:\n",
      "windows thanks know help db does dos problem like using\n",
      "Topic 7:\n",
      "bike water effect road design media dod paper like turn\n",
      "Topic 8:\n",
      "don just like think know people good ve going say\n",
      "Topic 9:\n",
      "car new price good power used air sale offer ground\n",
      "Topic 10:\n",
      "file available program edu ftp information files use image version\n",
      "Topic 11:\n",
      "ax max b8f g9v a86 145 pl 1d9 0t 34u\n",
      "Topic 12:\n",
      "government law privacy security legal encryption court fbi technology information\n",
      "Topic 13:\n",
      "card bit memory output video color data mode monitor 16\n",
      "Topic 14:\n",
      "drive scsi disk mac hard apple drives controller software port\n",
      "Topic 15:\n",
      "god jesus people believe christian bible say does life church\n",
      "Topic 16:\n",
      "year game team games season play hockey players league player\n",
      "Topic 17:\n",
      "10 00 15 25 20 11 12 14 16 13\n",
      "Topic 18:\n",
      "armenian israel armenians war people jews turkish israeli said women\n",
      "Topic 19:\n",
      "president people new said health year university school day work\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic {}:\".format(topic_idx))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(lda, tf_vectorizer.get_feature_names(), 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on peut le voir, quelques sujets qui ont été modélisés sont effectivement interprétables : le sujet 4 représente simplement les nombres. Le sujet 5 représente globalement l'informatique. Le sujet 8 semble représenter la religion, etc etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Une alternative, NMF\n",
    "\n",
    "Une autre type de modélisation de sujet automatique non supervisée est NMF (Negative Matrix Factorisation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "c:\\Users\\franc\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1477: FutureWarning: `alpha` was deprecated in version 1.0 and will be removed in 1.2. Use `alpha_W` and `alpha_H` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "people time right did good said say make way government\n",
      "Topic 1:\n",
      "window problem using server application screen display motif manager running\n",
      "Topic 2:\n",
      "god jesus bible christ faith believe christian christians sin church\n",
      "Topic 3:\n",
      "game team year games season players play hockey win league\n",
      "Topic 4:\n",
      "new 00 sale 10 price offer shipping condition 20 15\n",
      "Topic 5:\n",
      "thanks mail advance hi looking info help information address appreciated\n",
      "Topic 6:\n",
      "windows file files dos program version ftp ms directory running\n",
      "Topic 7:\n",
      "edu soon cs university ftp internet article email pub david\n",
      "Topic 8:\n",
      "key chip clipper encryption keys escrow government public algorithm nsa\n",
      "Topic 9:\n",
      "drive scsi drives hard disk ide floppy controller cd mac\n",
      "Topic 10:\n",
      "just ll thought tell oh little fine work wanted mean\n",
      "Topic 11:\n",
      "does know anybody mean work say doesn help exist program\n",
      "Topic 12:\n",
      "card video monitor cards drivers bus vga driver color memory\n",
      "Topic 13:\n",
      "like sounds looks look bike sound lot things really thing\n",
      "Topic 14:\n",
      "don know want let need doesn little sure sorry things\n",
      "Topic 15:\n",
      "car cars engine speed good bike driver road insurance fast\n",
      "Topic 16:\n",
      "ve got seen heard tried good recently times try couple\n",
      "Topic 17:\n",
      "use used using work available want software need image data\n",
      "Topic 18:\n",
      "think don lot try makes really pretty wasn bit david\n",
      "Topic 19:\n",
      "com list dave internet article sun hp email ibm phone\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, \n",
    "    min_df=2, \n",
    "    max_features=1_000, \n",
    "    stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 20\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd')\n",
    "nmf.fit(tfidf)\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "L'objectif de ce type de modélisation de sujets est de récupérer de potentielles catégories pour des traitements ultérieurs. Cette modélisation offre surtout une meilleure compréhension de la structuration du texte en vue de création de features manuelles (mettre l'accent sur certains mots, comprendre ce qui définit une catégorie, etc.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Opérez une première classification naïve de sentiments\n",
    "\n",
    "Vidéo d'introduction : https://vimeo.com/284320448\n",
    "\n",
    "Dans cette partie, nous allons traiter le problème de la classification supervisée, et voir comment utiliser les features que nous avons créées pour alimenter nos algorithmes d’apprentissage, essentiellement dans le cadre de la catégorisation de texte.\n",
    "\n",
    "! Cette partie ne traitera pas le problème de classification plus général qui s’applique à plusieurs niveaux dans un texte : prédire quel sera le prochain mot si on considère chaque mot comme une catégorie. Ou encore le POS tagging qui consiste à considérer la catégorie grammaticale d’un mot (verbe nom etc).\n",
    "\n",
    "? Mais attends, on a vraiment besoin d’algorithme d’apprentissage supervisé pour catégoriser du texte ? On ne peut pas juste assigner quelques mots-clés à des catégories, et hop ?\n",
    "\n",
    "Vous faites référence aux « systèmes experts » (des règles de fonctionnement codées à la main) qui ne sont malheureusement pas assez flexibles dans la plupart des cas qui nous intéressent et demandent beaucoup de maintenance, qui va en grandissant avec l’échelle de nos données et le nombre de classes à traiter. Cela reste cependant une alternative valable et robuste dans certains cas, à ne pas négliger automatiquement."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Petit rappel de la méthodologie\n",
    "\n",
    "La première question à se poser est : qu’est-ce que l'on veut accomplir ?\n",
    "\n",
    "Dans cette partie, nous allons nous concentrer sur le problème de classification : **pouvoir assigner une catégorie à un document texte fourni en entrée**. Par exemple, un type d’actualité associé à un article, les catégories d’une page Wikipédia, le tag associé à un tweet, etc.\n",
    "\n",
    "🛈 Une application spécifique au langage est la détection de sentiment/émotion d’un document. Cela peut être binaire (positif ou négatif), ou avec plus de nuances (triste, joyeux, en colère, etc). C’est une forme de classification qui est souvent demandée.\n",
    "\n",
    "Pour cela on va avoir besoin d’un jeu de données d’entraînement. L’idée, comme d’habitude est qu’il soit équilibré par catégorie et relativement consistant. Cette première étape est essentielle afin de s’assurer du bon fonctionnement du reste des traitements.\n",
    "\n",
    "La seconde étape sera de faire passer nos données non-structurées de texte à la moulinette que l’on a vue dans la première partie pour en sortir des features utilisables et structurées, par exemple sous forme vectorielle.\n",
    "\n",
    "Une fois notre jeu de donnée d’entraînement et de test formaté, on va pouvoir appliquer les méthodes classiques de classification (SVM, réseau de neurones, régression logistique) sur ces données afin de résoudre la problématique énoncée.\n",
    "\n",
    "S’en suit un réglage des hyperparamètres par recherche de grille (ou autre méthodologie) pour améliorer le modèle en fonction de la mesure de performance choisie."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rappel sur les algorithmes de classification\n",
    "\n",
    "Dans ce chapitre, on va se concentrer sur la famille d’algorithme de classification Naïve Bayes et sur la  régression logistique. Ces algorithmes permettent de catégoriser le sentiment d’un texte (positif ou négatif), ou encore si un e-mail est un spam ou non.\n",
    "\n",
    "Pour rappel, l’algorithme Naive Bayes (multinomial ou non) permet d’effectuer des classifications probabilistes, qui assignent la probabilité d’appartenance à une classe.\n",
    "\n",
    "C’est un type d’algorithme génératif, où l’on va modéliser chaque classe et essayer de déterminer la probabilité l’appartenance d’une observation à cette classe. A contrario, les algorithmes discriminatifs essaient de comprendre les caractéristiques différenciantes entre les différentes classes possibles."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation du sentiment d’un corpus de commentaires Amazon\n",
    "\n",
    "Comme terrain d’études, nous allons utiliser le jeu de données de commentaire de produits Amazon. L’objectif sera de déterminer si un commentaire est positif ou pas.\n",
    "\n",
    "### Petit rappel de classification Naive Bayes\n",
    "\n",
    "Un peu plus formellement, le problème de classification se traduit ainsi : \"trouver la classe 'c' qui a la probabilité la plus grande étant donné le document 'd' fourni\" :\n",
    "\n",
    "$$\\hat{c} = argmax_c p(c \\vert d)$$\n",
    "\n",
    "On ne sait pas calculer p(c|d) directement, on a donc besoin de simplifier un peu le problème. Ainsi, d’après le théorème de Bayes on a :\n",
    "\n",
    "$$p(c|d) = \\frac{p(d \\vert c)p(c)}{p(d)}$$\n",
    "\n",
    "L’objectif étant une maximisation sur la classe c, p(d) n’influence pas le résultat. Le problème peut être simplifié :\n",
    "\n",
    "$$\\hat{c} = argmax_c p(d \\vert c) p(c)$$\n",
    "\n",
    "Comme on l’a vu dans les chapitres précédents, le document 'd' est représenté par un certain nombres de features (les mots qu’il contient), sans conserver l’ordre de ces mots (bag-of-words) et en considérant qu’ils sont indépendant. On a ainsi pour un document de N mots $w_i$\n",
    "\n",
    "$$p(d \\vert c) = \\prod_{i=1}^N p(w_i \\vert c)$$\n",
    "\n",
    "Dans le cadre d’étude de texte, nous travaillons sur des probabilités faibles. Nous allons donc plutôt travailler à l’échelle logarithmique, ce qui ne change rien au problème de maximisation (la fonction log est monotone strictement croissante). Cela nous permet, en bonus, de travailler avec des sommes.\n",
    "\n",
    "$$\\hat{c} = argmax_c log ( p( d \\vert c) p(c) ) = argmax_c log p(c) + \\sum log( p(w_i \\vert c ) )$$\n",
    "\n",
    "Dans le cadre d’une classification binaire de texte avec unigramme.\n",
    "\n",
    "La question restante est donc : comment estimer p(c) et $p(w_i \\vert c)$ à partir de notre jeu de données d’entraînement. Vous l’aurez deviné, on va utiliser des fréquences :)\n",
    "\n",
    "La probabilité d’une classe est simplement la fréquence d’apparition de la classe dans le jeu de données d’entraînement :\n",
    "\n",
    "$$\n",
    "p(c) = \\frac{N_c}{N_{total doc}}\n",
    "$$\n",
    "\n",
    "Et la probabilité d’un mot dans une classe est simplement : la fréquence d’apparition de ce mot dans un type de document par rapport au nombre de mot total dans c.\n",
    "\n",
    "$$\n",
    "p(w_i \\vert c) = \\frac{N_{w_i \\text{dans c}}}{ \\sum_V N_{w_t \\text{dans c}}}\n",
    "$$\n",
    "\n",
    "On lisse cette probabilité pour les mots qui n'apparaitraientt pas dans une classe, ce qui évite de rendre nulle notre fonction de vraisemblance si un mot est à zéro prob (lissage Laplacien) :\n",
    "\n",
    "$$\n",
    "p(w_i \\vert c) = \\frac{N_{w_i \\in c} + 1}{ \\sum_V N_{w_t \\in c} + \\vert V \\vert }\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testons le modèle\n",
    "\n",
    "Nous allons utiliser [ce jeu de données](https://ai.stanford.edu/~amaas/data/sentiment/) uniquement pour le test de la détection de sentiment. Avec NLTK, il suffit de charger les données dans un tableau labellisé pour créer notre classifieur, avec chaque mot associé à un booléen confirmant son existence dans le document (en l’occurence ici dans le commentaire).\n",
    "\n",
    "C'est parti ! Chargeons les données et formatons-les en bag-of-words associé à des booléens :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m ap\n\u001b[0;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mformat_sentence\u001b[39m(sent):\n\u001b[0;32m      6\u001b[0m     \u001b[39mreturn\u001b[39;00m ({ word: \u001b[39mTrue\u001b[39;00m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m nltk\u001b[39m.\u001b[39mword_tokenize(sent\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)) })\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tools'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "from tools import ap  # THIS IS DEPRECATED MODULE. USE http://pypi.python.org/pypi/weblib MODULE INSTEAD.\n",
    "\n",
    "def format_sentence(sent):\n",
    "    return ({ word: True for word in nltk.word_tokenize(sent.decode('utf-8')) })\n",
    "\n",
    "def load_training_set():\n",
    "    training = []\n",
    "\n",
    "    for fp in os.listdir(ap('aclImdb/train/pos')):\n",
    "        example = '{}/{}'.format(ap('aclImdb/train/pos'), fp)\n",
    "        with open(example) as fp:\n",
    "            for i in fp:\n",
    "                training.append([format_sentence(i), 'pos'])\n",
    "\n",
    "    for fp in os.listdir(ap('aclImdb/train/neg')):\n",
    "        example = '{}/{}'.format(ap('aclImdb/train/neg'), fp)\n",
    "        with open(example) as fp:\n",
    "            for i in fp:\n",
    "                training.append([format_sentence(i), 'neg'])\n",
    "\n",
    "    return training\n",
    "\n",
    "training = load_training_set()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant entraîner notre classifieur, qui va utiliser les comptages expliqués plus haut pour créer le modèle probabiliste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je vous invite très fortement à regarder le code d'implémentation du classifieur, voire d’implémenter le vôtre pour comparer les performances et comprendre comment il fonctionne : http://www.nltk.org/_modules/nltk/classify/naivebayes.html \n",
    "\n",
    "Maintenant qu’on a entraîné notre modèle, on peut par exemple observer les features les plus représentatives des classes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.show_most_informative_features(n=25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ca parait relativement réaliste sur les ratio - l'utilisation de *avoid* est très significative d'une review negative pour un ratio d'utilisation de 1 / 93.4 etc\n",
    "\n",
    "Et les performances de classification sur les données test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy(classifier, test))\n",
    "# 0.88754"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit qu'on arrive à avoir une précision de prédiction de sentiments relativement intéressante (88.75% de précision) pour un premier essai sur le jeu de données test. Pour aller plus loin dans l'amélioration des performances, il peut être judicieux d’effectuer une validation croisée 😉\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "L'application d'algorithmes de classification classiques fonctionne dès lors que l'on sait quelles features utiliser à partir de notre corpus de départ.\n",
    "\n",
    "D'autres étudiants confirment mon sentiment général : ce cours est une catastrophe qui en dit long sur le contrôle qualité chez OC : https://openclassrooms.com/forum/sujet/cours-analysez-vos-donnees-textuelles\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2  Allez plus loin dans la classification de mots\n",
    "\n",
    "L'utilisation de manière brute d'algorithmes de classification n'est souvent pas suffisante pour obtenir des performances optimales en vue des problèmes auxquels vous serez confrontés. A travers l'exemple de la classification de sentiments, on va voir qu'il est important d'effectuer des modifications préalables de son corpus et de ses features spécifiques à nos problèmes.\n",
    "\n",
    "## Particularités de la classification de sentiments\n",
    "\n",
    "Comme la classification des sentiments d’un texte est un problème relativement classique de traitement du langage, il existe un certain nombre de techniques spécifiques qui permettent d’obtenir des modèles plus performants sur cette tâche. C’est une bonne illustration du type de manipulation qui permet d’orienter son travail de modélisation en fonction de la problématique abordée.\n",
    "\n",
    "Par exemple, on ne va pas utiliser un *bag-of-words* avec comptage, mais simplement la **présence ou pas d’un mot** dans un document. En effet, que le mot “désastre” apparaisse une ou deux fois influence peu la probabilité que le document soit négatif. C'est un critère flag binaire.\n",
    "\n",
    "Voici une autre technique utilisée pour régler **le problème de la négation**. Si je dis « je n’ai pas aimé ce film », la négation “n’” **inverse** le sentiment de ma phrase comparé à “aimer” qui se retrouve davantage dans des documents positifs habituellement. Pour contrer ce problème de manière simple, il suffit d’ajouter un indicateur au mot suivant la négation comme signifiant que son sens a été modifié.\n",
    "\n",
    "Ainsi, une phrase telle que “je n’aime pas cette personne” sera transformée en bag-of-words : { “je”, “ne”, “NON_aime”, “pas”, “cette”, “personne” }. en ajoutant des préfixes du type \"NON_\", une bonne partie des problèmes sur ce genre d’inversions de sens est ainsi réglée.\n",
    "\n",
    "Enfin, une technique de base est d’utiliser un lexique de mots représentatifs de vos classes, en l’occurrence ici positif et négatif. Si on utilise simplement le nombre d’apparitions de mots du lexique par classe bien construit pour entraîner un modèle de classification, on a déjà une bonne baseline.\n",
    "\n",
    "Ainsi, toute ces petites modifications et ajouts permettent de rendre notre modèle plus spécialisé pour notre problématique afin de donner de meilleures performances. Je vous encourage à développer ce genre de tweaks lors de la création d’un modèle pour un problème donné.\n",
    "\n",
    "## Utiliser d’autres types d'algorithmes de classification supervisée\n",
    "\n",
    "❓ On a utilisé un modèle simple et robuste, le classifieur bayésien naïf. Pourquoi vouloir utiliser un autre type de classifieur ?\n",
    "\n",
    "Une hypothèse très forte qui est faite lors de l’utilisation du classifieur Bayes est **l’indépendance des features**. Ce qui signifie que si deux features sont en réalité corrélées, elles auront un effet plus fort que ce qu’elles apportent en réalité comme information pour la classification. D’autres types de modèles ne sont pas aussi sensibles à cette corrélation entre les features, voire permettent de modéliser cette interaction entre les features pour rendre le modèle plus performant. C'est une des différences possibles qui influenceront votre choix de classifieur. En général, l'idée est de pré-tester un certain nombre de classifieurs qui intuitivement correspondent à votre problème pour savoir sur lequel vous concentrer.\n",
    "\n",
    "### Régression Logistique\n",
    "\n",
    "La régression logistique, à l’opposé de la classification bayes, est un modèle de classification discriminant. Il est expliqué [plus en détail ici](https://openclassrooms.com/fr/courses/4444646-entrainez-un-modele-predictif-lineaire/4507831-predisez-lineairement-la-probabilite-de-l-appartenance-d-un-point-a-une-classe).\n",
    "\n",
    "Son objectif est de maximiser la probabilité d’avoir une classe $y = c$ étant donné certaines features $f(x, c)$ calculées à partir des observations $x$. Cette maximisation s’effectue en général avec les méthodes classiques de descente de gradient, avec un terme de régularisation supplémentaire.\n",
    "\n",
    "On peut directement appliquer la régression logistique sur des matrices creuses de la taille du vocabulaire ou sur les vecteurs plus denses que l’on a créé à l’aide des techniques utilisées dans les chapitres précédents.\n",
    "\n",
    "### SVM, forêts aléatoires\n",
    "\n",
    "Le choix d’un classifieur est en fait un retour à l’éternel dilemme biais-variance, indépendamment du fait qu’on est amené à traiter du texte. Le classifieur Naive Bayes possède une variance faible et va pouvoir mieux généraliser plus rapidement, ce qui peut être utile lorsqu’on a un petit jeu de données ou des documents avec peu de texte. La contrepartie, bien sûr, c’est que ce genre de classifieur va avoir une plus faible précision (un biais plus grand) comparé à des classifieurs type SVM + RBF Kernel ou une régression logistique.\n",
    "\n",
    "📌 Pour en savoir plus sur les avantages et inconvénients de différents classifieurs, notamment appliquées à la classification de sentiments, vous pouvez consulter [cet article simple](https://aclanthology.org/P12-2018/) et efficace. Il s'agit d'un exemple de comparaison qu’il vous sera utile de consulter lors de votre travail préliminaire d’exploration.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Dans un  premier temps, nous favorisions un travail expert sur les features avant de se concentrer sur l'utilisation d'un classifieur approprié à la problématique en cours et aux hypothèses de départ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Traitez le corpus de textes à l'aide de réseaux de neurones\n",
    "\n",
    "Dans ce chapitre, on va s’intéresser à des architectures de réseaux de neurones assez utilisées dans le cadre de tâches de traitement de langage. Ils vont permettre notamment de traiter nos problèmes sous forme de série temporelle (de séquences), ce qui semble adapté à nos applications puisque le texte est une séquence de mots qui sont eux mêmes une séquence de lettres. 🚀\n",
    "\n",
    "📌 On traitait déjà dans certains modèles l’aspect séquentiel (par exemple en utilisant des n-grammes > 1) mais l’hypothèse markovienne est très contraignante pour la réprésentativité du modèle puisqu’on “oublie” de prendre en compte les éléments précédents les éléments conditionnés.\n",
    "\n",
    "⚡ Attention à ne pas plonger dans ces architectures sans tester au préalable des solutions qui sont en apparence plus simple mais beaucoup plus efficace (en terme de temps de calcul notamment) pour la majorité des problématiques que vous rencontrerez. En effet, les débutants ont tendance à tout de suite vouloir se diriger vers ces méthodes qui sont certes extrêmement performantes mais sont difficiles à structurer, entraîner et relativement lente par rapport à des méthodes plus simple qui offrent des performances similaires. L'idée est au moins d'avoir une baseline sur des classifieurs classiques avant de tester des architectures plus complexes !\n",
    "\n",
    "❗Ce chapitre a vocation à être une introduction aux (relativement) nouvelles architectures de réseaux de neurones et les méthodes associées. Cependant, je vous invite à vous documenter de manière spécifique sur votre problématique afin de pouvoir créer votre modèle. Je présente ici les bases afin que vous puissiez vous débrouiller dans le champ des possibles et pouvoir aborder les ressources avec le bagage nécessaire.\n",
    "\n",
    "## Représentation des features\n",
    "\n",
    "On retrouve notamment les plongements (*embeddings*) en dimension inférieure word2vec (ou autre) comme entrée pour les réseaux de neurones. Ces manipulations préalables ne sont pas inutiles ! Il faut notamment prendre en compte les capacités de calcul et le niveau de représentativité nécessaire dans votre application afin de déterminer la dimension de l’espace d’embedding, par exemple (qui peut varier grosso modo entre 20 et 300 dimensions).\n",
    "\n",
    "## Différentes architectures possibles\n",
    "\n",
    "### L’architecture de base et ses problèmes : le RNN\n",
    "\n",
    "Les RNN représentent la famille de réseau de neurones qui traite les données de manière séquentielles. L’idée principale étant que chaque nouveau mot qui est prédit à partir de notre modèle, prend en compte l’état précédent afin de s’actualiser. L’état représente fondamentalement l’historique - la mémoire utilisée dans le réseau de neurones pour prendre en compte le passé (TOUT le passé) afin de l’utiliser dans la prédiction à l’instant $t$.\n",
    "\n",
    "<figure id=\"r-4881228\" data-claire-element-id=\"30015822\"><img id=\"r-4881226\" data-claire-element-id=\"9015461\" src=\"https://user.oc-static.com/upload/2017/12/13/15131655946094_RNN-rolled.png\" alt=\"L'architecture du RNN\"   style=\"background-color:White;\"/><figcaption>L'architecture du RNN</figcaption></figure>\n",
    "\n",
    "Dans le cas de tâches associées au texte, on essaie par exemple à chaque étape de prédire le mot en fonction des mots précédents ainsi qu’une observation. Par exemple, le mot dans une langue étrangère dans le cadre d’une traduction.\n",
    "\n",
    "On a donc une série temporelle qui à chaque instant t, va prendre en entrée une observation $o_t$ et l’état précédent $x_{t−1}$ pour être calculé :\n",
    "\n",
    "$$\n",
    "x_t = W_{rec} \\sigma(x_{t-1}) + W_{in} o_t + b\n",
    "$$\n",
    "\n",
    "<img id=\"r-4881247\" data-claire-element-id=\"9015305\" src=\"https://lh5.googleusercontent.com/7U_SQzt7w9KJ3HbWEarKSZybYjOPjhnQkRKT2KPX8Umeb_pkYzkDhoWsNVDghBDCZuHImE4HNDjoqKiZggPfGpOayoyo-L1MzJada678lYdH2eGGrrBktTsjOm5e1LMbTcF1-OR9\" alt=\"\"   style=\"background-color:White;\"/>\n",
    "\n",
    "Deux problèmes principaux font suite à cette architecture fondamentale : la **disparition** du gradient, et son **explosion**. En effet lors de la backpropagation, on doit calculer un gradient (en particulier $x_t/x$) qui est sujet à disparition/explosion. Cette disparition notamment a pour effet de faire “oublier” au RNN des informations qui pourraient pourtant être utiles au traitement actuel. Si j’ai un texte dans lequel une information primordiale est présente au début, il est très difficile de créer un RNN qui prenne en compte cette information sur la fin du texte. \n",
    "\n",
    "En pratique, on voit que le RNN ne peut prendre en compte que très peu de contexte passé à cause de cette disparition de gradient.\n",
    "\n",
    "Il existe une grande variété de techniques qui sont utilisés pour mitiger ce problème. Notamment en premier lieu, une architecture plus robuste qui a été imaginée quelques années après l’apparition des RNN appelée **LSTM (Long Short Term Memory)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Networks (Long Short Term Memory Networks)\n",
    "\n",
    "L’idée principale des LSTM est de permettre au réseau “d’oublier” ou de ne pas prendre en compte certaines observations passées afin de pouvoir donner du poids aux informations importantes dans la prédiction actuelle.\n",
    "\n",
    "Cette idée se traduit par des “gates” qui sont chargées de déterminer l’importance d’une entrée, afin de savoir si on enregistre l’information qui en sort ou pas. Pour une explication détaillée intuitive, rendez-vous sur [cet article de blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) qui explique les différentes étapes. En pratique la seule partie qui change vraiment est lors de la mise à jour des réseaux de neurones qui est un peu plus technique.\n",
    "\n",
    "**Note** L'article de blog est la source des images du cours.\n",
    "\n",
    "<figure id=\"r-4881324\" data-claire-element-id=\"30015825\"><img id=\"r-4881322\" data-claire-element-id=\"9015494\" src=\"https://lh6.googleusercontent.com/iyYgSSiBUHfIPkjSmEzw70VRzy7ru77mXhOuXRhwqAe6xKLLu45P5F4M_PaEVuKaXiCX739uEvz_qTozgYZB73GlTsdRiDns-6NxqioTRrIY3DziFyMMA3rCutWMZpHwi6JQdBV6\" alt=\"Une cellule LSTM\"  style=\"background-color:White;\"/><figcaption>Une cellule LSTM </figcaption></figure>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etat de l’art des réseaux de neurones en traitement du langage\n",
    "\n",
    "L’utilisation des réseaux de neurones dans les applications de traitement de langage représente encore un problème ouvert dans la recherche, qui demande à être traité plus en profondeur, notamment sur les problématiques spécifique à des tâches (quelle architecture correspondent à quelles tâches), des optimisations plus globales et les technique de transfert d’apprentissage afin de pouvoir utiliser des réseaux déjà entraînés sur de nouveaux problèmes plus facilement.\n",
    "\n",
    "A titre illustratif, je voudrais citer deux mécanismes qui attirent l’attention depuis quelques temps.\n",
    "\n",
    "### Les réseaux de neurones résiduels et autre augmentation de densité\n",
    "\n",
    "Les réseaux de neurones résiduels sont très utilisés, à la base pour le traitement d’images mais aussi maintenant dans le traitement du texte. Ils permettent aussi de mitiger l’effet de diminution trop brutal du gradient. Pour cela simplement, on ajoute directement l’identité de l’état précédent dans la fonction de calcul. Cela a pour effet de diminuer ce problème de disparition du gradient.\n",
    "\n",
    "<img id=\"r-4881343\" data-claire-element-id=\"9015568\" src=\"https://lh5.googleusercontent.com/1QSJNWWNxC8acis3ynDXs3cLJpe-81MMOlVR3dvtM6_sjo_OK1BwoFJXCejx63ThjcjEQyM34yQxTWhjCubNf5y7Jkxe79Zh_IFLLNhkgDw3JZfVHHBdOVZxbXer0bvyM5poko6h\" alt=\"\"   style=\"background-color:White;\"/>\n",
    "\n",
    "D’autres procédés approfondissent cette idée comme par exemple les [highway networks](https://arxiv.org/pdf/1505.00387.pdf) qui augmentent les fonctions de manière différente mais servent ce même objectif. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les techniques d’attention\n",
    "\n",
    "L’idée principale des techniques d’attention vise à se concentrer sur une partie de l’information en particulier qui est fournie au réseau de neurones. Ca peut être un mot à traduire au milieu d’une phrase, une partie d’une image lors de la description de celle-ci, etc. Il s'agit donc d’entraîner un réseau de neurones aussi sur un score d’importance à donner à l’ensemble de l’information qui lui est présentée. Ces techniques sont très prometteuses en terme de performance de prédiction.\n",
    "\n",
    "<img id=\"r-4881363\" data-claire-element-id=\"9015660\" src=\"https://lh6.googleusercontent.com/HYNAC-98gAJs0FysDiUtmAOG-LGsphynk5Lc3vmGGAjStf_hOp9vPcwfLKnYz3Dw4nuZ1nySf9PUOkNzDy25cTkvoCMipteo7vEHRRELG10DXLoYl5JVd6BnsBXxvK-0Ex1fPL_k\" alt=\"\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications\n",
    "\n",
    "Avec ces architectures, on peut avoir une meilleure représentation d’un corpus de texte puisqu’on prend en compte son caractère séquentiel. Voici quelques domaines d’application dans lesquels ce genre d’architecture a été particulièrement performant :\n",
    "* Classification de texte\n",
    "* POS Tagging et NER\n",
    "* Speech to text\n",
    "* Traduction automatique\n",
    "* Génération automatique de texte et notamment de légendes d’images\n",
    "\n",
    "Ce qu’il faut garder en tête, c’est que les réseaux de neurones récurrent bien entraînés permettent d’intégrer la manière dont est structuré un langage (ou un sous-ensemble du langage) ou toute séquence de texte. Le fait d’avoir ce modèle permet une infinité d’applications liée aux texte, qu’il faut paramétrer et personnaliser pour votre problème spécifique.\n",
    "\n",
    "❗Encore une fois, attention à ne pas  utiliser un canon pour tuer une mouche, et être bien sûr que toutes les circonstances (taille des données d’entraînement etc) sont présentes pour l’utilisation d’un réseau de neurones efficace."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Une petite application illustrative : retour aux chanteurs de rap\n",
    "\n",
    "Afin de démontrer l’efficacité de ces modèles récurrents, essayons de l'appliquer à l'échelle des caractères d’une phrase afin de générer des paroles de rap, dans le style d’un rappeur. Pour cela, on va récupérer [l’implémentation du LSTM de Andrej Karpathy](https://github.com/karpathy/char-rnn).\n",
    "\n",
    "Si on entraîne sur un corpus de texte particulier (ici le rappeur Nekfeu), voici le texte généré au bout de quelques milliers d'itérations :\n",
    "\n",
    "    Paclois ce b'est un boutes qu'on heêtembarais j'on sa c'est, sout je julam\n",
    "    Dévans\n",
    "    J'etfu l'é-travy pei fais se à jà aura\n",
    "    Qu'le garl\n",
    "    La quincer gé veull'\n",
    "    Atrait\n",
    "    tu vow mon avet peu san enfiiment aure maisas\n",
    "    Et quand j'ta rag, s'lerr\n",
    "    D’est pomps\n",
    "    Fourde bes des quand tout samiectieu ryais\n",
    "    L'heux\n",
    "    Lans l'ai à le gropas, caquis\n",
    "    Je Dacie, à nan poustier juk qu’retape ys qurises suis des conpes émoy ne da tiendir\n",
    "    Plassine grissoble\n",
    "    J'es it-di\n",
    "    J'suis d'acinon puant fet-pes c'tui t'baq\n",
    "\n",
    "On peut observer notamment la propension à comprendre la longueur d’un vers ou même la taille d'un mot, ce qui montre que le RNN retient que le vers a commencé quelque part avec une majuscule et doit finir par un retour à la ligne. De même, il a compris environ la taille d'un mot et ajoute des espaces dans les bonnes proportions. Il y a donc une forme de mémorisation du contexte passé. On voit aussi des virgules, apostrophes ou des tirets en bonne proportions. L'alternance consoles-voyelles aussi est respectée ainsi que certaines combinaisons caractéristiques ('tt', 'eu', 'es', etc). Quelques mots commencent à être réalistes, mais il n'y a pas assez de données sur le corpus (<250k caractère)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "On a vu ici le type de modélisation possible à l'aide des réseaux de neurones. L'idéal est pour vous d'entraîner votre propre réseau de neurones en vu d'une des tâches applicatives citée plus haut - classification, POS tagging, traduction automatique etc afin de vous rendre compte de la méthodologie d'entraînement sur ce type de problématique. L'entraînement de réseaux de neurones reste pour le moment un processus relativement intuitif avec beaucoup d'heuristiques et de tests nécessaires avant d'obtenir des résultats probants et stables, notamment pour déterminer la valeur des hyperparamètres (nombre & taille des calques, learning rate, algorithme de gradient utilisé, etc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Entraînez-vous à classifier du texte\n",
    "\n",
    "## À vous de jouer\n",
    "\n",
    "❗Pour vous entraîner, réalisez cet exercice étape par étape. Une fois terminé, vous pouvez comparer votre travail avec les pistes que je vous propose.\n",
    "\n",
    "Vous utiliserez les données « Reuters Corpus Volume » accessible directement dans scikit learn à l’aide de la fonction `fetch_rcv1` qui contient 800,000 annonces de presses Reuters étiquetées manuellement. Votre objectif est de réaliser un benchmark de différents types de classifieurs afin de comparer les différentes performances sur ce type de problème.\n",
    "\n",
    "### Consigne\n",
    "\n",
    "Vous devez réaliser les tâches suivantes :\n",
    "* Charger les données\n",
    "* Créer différents classifieurs (au moins 3)\n",
    "* Effectuer une validation croisée sur les différents classifieurs\n",
    "* Afficher les différentes performances\n",
    "\n",
    "Le jeu de données est relativement lourd pour un travail en local, avec 650MB compressé de données. Il est conseillé de travailler sur un échantillon dans un premier temps pour s’assurer que tout fonctionne comme prévu pour ensuite traiter tout le jeu de données et obtenir les résultats finaux.\n",
    "\n",
    "Vérifiez-bien que vous avez les éléments suivants :\n",
    "* Au moins 3 classifieurs différents ont été appliqués par validations croisées sur les données correctement, puis les performances ont été évaluées sur chacun."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9aff9e50adfaa9e30c910fb3872ffdc72747acb5f50803ca0504f00e980f7c25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

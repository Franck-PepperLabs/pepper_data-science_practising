{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [**Analysez vos donnÃ©es textuelles**](https://openclassrooms.com/fr/courses/4470541-analysez-vos-donnees-textuelles)\n",
    "\n",
    "* **ID** : 4470541\n",
    "* **Grade** : OC DS P6\n",
    "* **DurÃ©e** : 8 heures\n",
    "* **DifficultÃ©** : Moyenne\n",
    "* **SÃ©quence** : 3 + 4 + 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bienvenue dans ce cours de traitement du langage naturel. Lâ€™objectif de ce cours est de comprendre les mÃ©thodes qui permettent de **transformer le texte** en caractÃ©ristiques exploitables par des algorithmes de machine learning, et les architectures et modÃ¨les qui correspondent le mieux Ã  ce typeÂ de donnÃ©es. En lâ€™occurence un ensemble de documents texte **non-structurÃ©s**.\n",
    "\n",
    "Ce cours est divisÃ© en 3 parties : une premiÃ¨re qui traite de l'exploration, du **nettoyage et de la normalisation du texte**. Une seconde partie dÃ©diÃ©e aux **diffÃ©rents types de transformations** qui vontÂ vous permettre de mieux comprendreÂ vos donnÃ©es textuelles et deÂ **crÃ©er des caractÃ©ristiques** queÂ vousÂ pourrez utiliser dansÂ vos algorithmes de machine learning. La derniÃ¨re partie sera consacrÃ©e Ã  la **classification du texte** Ã  l'aide de l'apprentissage automatique sous forme de **rÃ©seau de neurones**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectifs pÃ©dagogiques\n",
    "\n",
    "* Effectuer un prÃ©-traitement de corpus de texte\n",
    "* MaÃ®triser les techniques de bag-of-words etÂ de plongements de mots (word embeddings)\n",
    "* ModÃ©liser des sujets de maniÃ¨re non-supervisÃ©e (LDA, etc.)\n",
    "* Classer des corpus de texteÂ avec des mÃ©thodes supervisÃ©es (rÃ©seaux de neurones)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PrÃ©requis\n",
    "\n",
    "Ce cours se situe au croisement des mathÃ©matiques et de l'informatique. Pour en profiter pleinement, n'hÃ©sitez pas Ã  vous rafraÃ®chir la mÃ©moire, avant ou pendant le cours, sur :\n",
    "\n",
    "* [**Python pour le calcul numÃ©rique (numpy) et la crÃ©ation de graphiques (pyplot)**](https://openclassrooms.com/fr/courses/7771531-decouvrez-les-librairies-python-pour-la-data-science?archived-source=4452741), que nous utiliserons dans les parties TP du cours,\n",
    "* Quelques notions d'**algÃ¨bre linÃ©aire**Â : manipulation de vecteurs, multiplications de matrices, normes, et valeurs/vecteurs propres,\n",
    "* Quelques notions deÂ **probabilitÃ©s** et de **statistiques**,Â telles que distribution de loi de probabilitÃ© et variance,\n",
    "* Les [**modÃ¨les non-supervisÃ©es**](https://openclassrooms.com/fr/courses/4379436-explorez-vos-donnees-avec-des-algorithmes-non-supervises) permettront de modÃ©liser des caractÃ©ristiques automatiquement Ã  partir du texte\n",
    "* Les [**modÃ¨les supervisÃ©es non-linÃ©aires**](https://openclassrooms.com/fr/courses/4470406-utilisez-des-modeles-supervises-non-lineaires) sont indispensables au traitement du texte, notamment les rÃ©seaux de neurones sÃ©quentiels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan\n",
    "\n",
    "* Partie 1 - PrÃ©traitez des donnÃ©es textuelles\n",
    "    * 1.Â RÃ©cupÃ©rez et explorez le corpus de textes\n",
    "    * 2.Â Nettoyez et normalisez les donnÃ©es\n",
    "    * 3.Â TP - Faites vos premiers pas dans l'analyse de donnÃ©es textuelles\n",
    "* Partie 2 - Transformez des donnÃ©es textuelles\n",
    "    * 1.Â ReprÃ©sentez votre corpus en \"bag of words\"\n",
    "    * 2.Â Effectuez des plongements de mots (word embeddings)\n",
    "    * 3.Â ModÃ©lisez des sujets avec des mÃ©thodes non supervisÃ©es\n",
    "    * Quiz : Partie 2\n",
    "* Partie 3 - DÃ©tectez automatiquement les sentiments de commentaires clients\n",
    "    * 1.Â OpÃ©rez une premiÃ¨re classification naÃ¯ve de sentiments\n",
    "    * 2.Â Allez plus loin dans la classification de mots\n",
    "    * 3.Â Traitez le corpus de textes Ã  l'aide de rÃ©seaux de neurones\n",
    "    * 4.Â EntraÃ®nez-vous Ã  classifier du texte\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  1.1. RÃ©cupÃ©rez et explorez le corpus de textes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La premiÃ¨re Ã©tapeÂ du traitement des donnÃ©es estÂ de rÃ©cupÃ©rer le corpus de textes, de faire une analyse exploratoire afin de bien comprendre les spÃ©cificitÃ©s du jeu de donnÃ©es, et de nettoyer les donnÃ©es afin de pouvoir les utiliser ultÃ©rieurement dansÂ vos algorithmes.\n",
    "\n",
    "Mais avant de nous plonger dans le vif du sujet, introduisons quelques notions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est important de bienÂ identifierÂ 4 termes incontournables :Â \n",
    "\n",
    "* le **corpus**Â :Â un ensemble de documents (des textes dans notre cas), regroupÃ©s dans une optique ou dans une thÃ©matique prÃ©cise.Â \n",
    "* un **document**Â : la notion de document fait rÃ©fÃ©rence Ã  un texte appartenant au corpus, mais indÃ©pendant des autres textes. Il peut Ãªtre constituÃ© d'une ou plusieurs phrases, un ou plusieurs paragraphes.\n",
    "* un **jeton (token)** : le terme token dÃ©signe gÃ©nÃ©ralement un mot et/ou un Ã©lÃ©ment de ponctuation. La phrase \"Hello World!\" comprend donc 3 tokens.Â \n",
    "* le **vocabulaire** : il s'agit de l'ensemble des tokens distincts prÃ©sents dans l'ensemble du corpus."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si la notion de token ou de vocabulaire est relativement invariante en fonction des jeux de donnÃ©es, le corpus et les documents peuvent avoir des formes trÃ¨s variÃ©es :\n",
    "\n",
    "un fichier excel ou csv avec une liste de produits. Ici, la colonne 'description' est notre corpus, chaque cellule  de la colonne 'description' constitue un document:\n",
    "\n",
    "<figure id=\"r-7906592\" data-claire-element-id=\"33104169\"><img id=\"r-7906590\" data-claire-element-id=\"33104167\" src=\"https://user.oc-static.com/upload/2022/06/27/16563236580116_example_csv.jpg\" alt=\"chaque ligne de la colonne description est un document\" /><figcaption>un fichier csv</figcaption></figure>\n",
    "\n",
    "un dossier avec diffÃ©rents fichiers. Chaque fichier est un document et l'ensemble des fichiersÂ constitue le corpus :Â \n",
    "\n",
    "<figure id=\"r-7906596\" data-claire-element-id=\"33104174\"><img id=\"r-7906594\" data-claire-element-id=\"33104172\" src=\"https://user.oc-static.com/upload/2022/06/27/16563235098743_contrats.jpg\" alt=\"chaque fichier est un document\" /><figcaption>une liste de fichiers dans un dossier</figcaption></figure>\n",
    "\n",
    "plusieurs pages web, au format html. Il faudra d'abord tÃ©lÃ©charger depuis le web les fichiers html, Ã  la main ou de faÃ§on automatisÃ©e (on parle alors de 'scraping') :\n",
    "\n",
    "<figure id=\"r-7906601\" data-claire-element-id=\"33104179\"><img id=\"r-7906599\" data-claire-element-id=\"33104177\" src=\"https://user.oc-static.com/upload/2022/06/27/16563245092815_wiki.jpg\" alt=\"on utilisera les donnÃ©es du fichier html\" /><figcaption>exemple de page wikipedia</figcaption></figure>\n",
    "\n",
    "Dans le cadre d'un fichier html, il faudra transformer le document afin d'extraire l'information textuelle. Nous verrons cela un plus tard dans le cours."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âš   Pour les documents .txt, .doc, .csv, .xls, cela semble facile, mais comment faire pour les documents **.pdf** ?\n",
    "\n",
    "Transformer les documents .pdf en .txt n'est pas une chose facile. Ce traitement spÃ©cifique Ã  un nom, cela s'appelle **OCR** :  Optical Character Recognition.Â \n",
    "\n",
    "Heureusement, il existe de nombreux packages disponibles, comme par exemple [**Tesseract**](https://github.com/madmaze/pytesseract). Rassurez vous, nous ne couvrirons pas cette problÃ©matique en dÃ©tail, mais gardez en tÃªte qu'elle existe."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant aborder le prÃ©-traitement du texte  en plusieurs Ã©tapes  :\n",
    "1. La rÃ©cupÃ©ration du **corpus**, ansi qu'un premierÂ **traitement** de ce dernier pour avoir des donnÃ©es textuelles exploitables (au format string).\n",
    "2. La **tokenization**,Â qui dÃ©signe le dÃ©coupage en mots des diffÃ©rents documents qui constituent votre corpus.\n",
    "3. LaÂ **normalisation** et la construction duÂ dictionnaire qui permet de ne pas prendre en compte des dÃ©tails importants au niveau local (ponctuation, majuscules, conjugaison, etc.)\n",
    "\n",
    "<figure id=\"r-4867873\" data-claire-element-id=\"33104195\"><img id=\"r-4867871\" data-claire-element-id=\"8982571\" src=\"https://lh5.googleusercontent.com/UmUtjHcacRCRs_Ba42Io7ZmU3xtcZ2xnuLn6ohZA5z7ZDU20Xzocm1t7sk5_UXS98vSSkcSKYlvQqIaiEOepKpOGqYkr4z1liJZbCCj-bZXmdTrcAWdUFqor_twdFJMywyKNuYDA\" alt=\"Le cycle de traitement d'un corpus de texte\" /><figcaption>Le cycle de traitement d'un corpus de texte</figcaption></figure>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'en conviens, la partie nettoyage n'est pas la plus intÃ©ressante, mais elle est essentielle. Pour rendre la chose plus attrayante, je vais rÃ©aliser une Ã©tude partiale et partielle desÂ artistes franÃ§ais de rap et leur vocabulaire propre."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ›ˆ Tout au long de ce chapitre, nous allons utiliser la librairie de traitement de langage [**NLTK**](https://www.nltk.org/) ainsi que les librairies classiques pandas, numpy et scikit.\n",
    "\n",
    "Cette Librairie, bien qu'ancienne est connueÂ et reconnue pour sa simplicitÃ© d'utilisation et sa grande polyvalence.\n",
    "\n",
    "Vous Ãªtes bien entendu libres d'utiliser des librairies plus modernes mais parfois plus complexes. La librairieÂ [**spacy**](https://spacy.io/)Â en est un bon exemple."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RÃ©cupÃ©ration du corpus de texte\n",
    "\n",
    "La premiÃ¨re Ã©tape est la rÃ©cupÃ©ration du texte. Il existe plusieurs maniÃ¨res de rÃ©cupÃ©rer du texte : soit depuis une base de donnÃ©e que vous possÃ©dez, soit depuis des fichiers XML ou autres que vous possÃ©dez, soit en scrapant des pages comme le font les moteurs de recherches, en utilisant une API.\n",
    "\n",
    "Nous ne traiterons pas cette partie,Â qui est relativement laborieuseÂ et techniquement peuÂ intÃ©ressante. Il existe diverses maniÃ¨res de scraper du texte comme Ã  l'aide des librairies [**scrapy**](https://scrapy.org/) ou [**beautifulsoup**](https://beautiful-soup-4.readthedocs.io/en/latest/).\n",
    "\n",
    "Dans mon cas, j'ai scrapÃ© la page wikipÃ©dia quiÂ proposeÂ une liste des rappeurs franÃ§ais. J'ai ensuite rÃ©cupÃ©rÃ© les paroles des diffÃ©rentes chansons de ces rappeurs sur le site Genius, toujours en scrapant. Je ne suis malheureusement pas autorisÃ© Ã  vous fournir ce jeu de donnÃ©es, libre Ã  vous d'effectuer la mÃªme dÃ©marche. Vous pouvez aussi utiliser un jeu de donnÃ©es prÃ©sent par dÃ©faut dans la librairie NLTK."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âš   Attention au format d'encodage de vos fichiers texte qui peuvent mener Ã  des erreurs faciles Ã  Ã©viter. On favorisera sur ce cours l'**UTF-8** (encodage universel) trÃ¨s utilisÃ© et qui permet l'usage des accents."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le texte que vous utilisez, que l'on appelle Â« corpus Â», peut Ãªtre organisÃ© de plusieurs maniÃ¨re diffÃ©rentes :\n",
    "\n",
    "<figure id=\"r-4867909\" data-claire-element-id=\"33104206\"><img id=\"r-4867907\" data-claire-element-id=\"8982643\" src=\"https://lh3.googleusercontent.com/AuaUziLNkLIyzqcnegyKz38ITKzmq0VNBgHHvddY2zUyo1hM5oVS5vs_AsYv9kNtz5cY1p2cZCVTQI34O_i64MhSEZgOnoIN0kgI4PMXnB47ZsLdX_yIFbPrKOYa4pEd0UyWx_zk\" alt=\"Les diffÃ©rents types de structuration du texte\" /><figcaption>Les diffÃ©rents types de structuration du texte</figcaption></figure>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargeons donc les donnÃ©es, dans un dictionnaire python, ce que je fais avecÂ la fonction   `load_all_sentences` que j'ai crÃ©Ã©e pour mon exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_all_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\franc\\Projects\\pepper_data-science_practising\\OC DS\\P6 C1 Analysez vos donnÃ©es textuelles\\4470541_nlp_text_analysis.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/franc/Projects/pepper_data-science_practising/OC%20DS/P6%20C1%20Analysez%20vos%20donn%C3%A9es%20textuelles/4470541_nlp_text_analysis.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m db \u001b[39m=\u001b[39m load_all_sentences()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/franc/Projects/pepper_data-science_practising/OC%20DS/P6%20C1%20Analysez%20vos%20donn%C3%A9es%20textuelles/4470541_nlp_text_analysis.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mchargement de \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m vers dans la db\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mlen\u001b[39m(db\u001b[39m.\u001b[39mkeys())))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_all_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "db = load_all_sentences()\n",
    "print('chargement de {} vers dans la db'.format(len(db.keys())))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mon dictionnaire est simplement constituÃ© d'objets de la forme { vers, artiste }\n",
    "\n",
    "Je crÃ©e aussi une table de recherche par artiste car on va s'intÃ©resser Ã  ce qui les diffÃ©rencie et les caractÃ©rise. De plus, je veux avoir assez de texte pour chaque artiste donc je vaisÂ Ã©liminer ceux qui ont Ã©crit moins de 200 vers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "base_artistes = defaultdict(set)\n",
    "for k,v in db.iteritems():\n",
    "    base_artistes[v['artistes']].add(k)\n",
    "artistes = { k:v for k,v in base_artistes.iteritems() if len(v) > 200 }\n",
    "print('{} artistes'.format(len(artistes)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration du texte : tokenisation et analyse des frÃ©quences\n",
    "\n",
    "On veut dans un premier temps Ã©tudier le vocabulaire utilisÃ© par chaque artiste.Â Pour une premiÃ¨re intuition, il est judicieux d'observer le nombre de mots utilisÃ©s.\n",
    "\n",
    "On va utiliser la fonction [**`word_tokenize`**](https://www.nltk.org/api/nltk.tokenize.html)Â (Â« tokenize Â» signifie Â« sÃ©parer par mot Â») qui va dÃ©composerÂ les vers en tableaux de mots afin de pouvoir effectuer des opÃ©rations dessus. Observons dÃ©jÃ  son comportement sur un bout de texte simple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bonjour',\n",
       " ',',\n",
       " 'je',\n",
       " 'suis',\n",
       " 'un',\n",
       " 'texte',\n",
       " \"d'exemple\",\n",
       " 'pour',\n",
       " 'le',\n",
       " 'cours',\n",
       " \"d'Openclassrooms\",\n",
       " '.',\n",
       " 'Soyez',\n",
       " 'attentifs',\n",
       " 'Ã ',\n",
       " 'ce',\n",
       " 'cours',\n",
       " '!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install nltk\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "test = \"Bonjour, je suis un texte d'exemple pour le cours d'Openclassrooms. Soyez attentifs Ã  ce cours !\"\n",
    "\n",
    "nltk.word_tokenize(test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a bien une sÃ©paration par mot. Petit problÃ¨me en revanche, la ponctuation est conservÃ©e comme Ã©tant un \"token\" ! Il faut donc trouver un moyen d'Ã©liminer cette ponctuation, car ce sont les mots qui nous intÃ©ressent comme caractÃ©ristiques . On remarque aussi qu'il y a un problÃ¨me sur les apostrophes considÃ©rÃ©s comme faisant partie du mot. AinsiÂ \"d'exemple\" devrait Ãªtre sÃ©parÃ© en \"de\" et \"exemple\". Un autre problÃ¨me, c'est que certains mots on des majuscules car ils apparaissent en dÃ©but de phrases ou de vers, alors que ce sont les mÃªmes mots.\n",
    "\n",
    "Ca parait tout d'un coupÂ un peu compliquÃ© Ã  mettre en place, n'est-ce pas ? ğŸ˜"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ›ˆ Les accents sont affichÃ©s encodÃ©s mais ce n'est pas trÃ¨s grave, on peut revenir Ã  un affichage normal si on le souhaite."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il estÂ vraiment important de regarder les options sur ce genre de fonctions qui englobent plusieurs actions sur votre corpus afin d'Ãªtre bien sÃ»r qu'elles effectuent ce que vous voulez.\n",
    "\n",
    "Le fait d'essayer d'harmoniser les tokens est un processus nommÃ© Â«Â **normalisation** Â». Bon, on va dÃ©jÃ  utiliser les bonne vieilles expressions rÃ©guliÃ¨res pour ne rÃ©cupÃ©rer que les caractÃ¨res alphanumÃ©riques de chaque phrase. Vous trouverez Ã  [cette adresse](https://www.debuggex.com/cheatsheet/regex/python) un rappel utile sur les expressions rÃ©guliÃ¨res.\n",
    "\n",
    "Ensuite, on va utiliser un tokenizer specifique au franÃ§ais ce qui permet de traiter la ponctuation de la bonne maniÃ¨re. On Ã©limine aussi les majuscules peu informatives, avec la fonction Â« lower Â»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bonjour',\n",
       " 'je',\n",
       " 'suis',\n",
       " 'un',\n",
       " 'texte',\n",
       " 'd',\n",
       " 'exemple',\n",
       " 'pour',\n",
       " 'le',\n",
       " 'cours',\n",
       " 'd',\n",
       " 'Openclassrooms',\n",
       " 'Soyez',\n",
       " 'attentifs',\n",
       " 'Ã ',\n",
       " 'ce',\n",
       " 'cours']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "tokenizer.tokenize(\"Bonjour, je suis un texte d'exemple pour le cours d'Openclassrooms. Soyez attentifs Ã  ce cours !\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, Ã§a commence Ã  Ãªtre mieux ! Maintenant qu'on a bien sÃ©parÃ© notre texte en unitÃ© de mots (tokens) on peut l'appliquer au jeu de donnÃ©es qui nous intÃ©resse, et compter la frÃ©quence d'apparition des diffÃ©rents mots pour avoir une idÃ©e du champ lexical. On effectue ce comptage par artiste pour comparer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'defaultdict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\franc\\Projects\\pepper_data-science_practising\\OC DS\\P6 C1 Analysez vos donnÃ©es textuelles\\4470541_nlp_text_analysis.ipynb Cell 29\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/franc/Projects/pepper_data-science_practising/OC%20DS/P6%20C1%20Analysez%20vos%20donn%C3%A9es%20textuelles/4470541_nlp_text_analysis.ipynb#X42sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m (freq, stats, corpora)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/franc/Projects/pepper_data-science_practising/OC%20DS/P6%20C1%20Analysez%20vos%20donn%C3%A9es%20textuelles/4470541_nlp_text_analysis.ipynb#X42sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# RÃ©cupÃ©ration des comptages\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/franc/Projects/pepper_data-science_practising/OC%20DS/P6%20C1%20Analysez%20vos%20donn%C3%A9es%20textuelles/4470541_nlp_text_analysis.ipynb#X42sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m freq, stats, corpora \u001b[39m=\u001b[39m freq_stats_corpora()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/franc/Projects/pepper_data-science_practising/OC%20DS/P6%20C1%20Analysez%20vos%20donn%C3%A9es%20textuelles/4470541_nlp_text_analysis.ipynb#X42sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame\u001b[39m.\u001b[39mfrom_dict(stats, orient\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/franc/Projects/pepper_data-science_practising/OC%20DS/P6%20C1%20Analysez%20vos%20donn%C3%A9es%20textuelles/4470541_nlp_text_analysis.ipynb#X42sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Affichage des frÃ©quences\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\franc\\Projects\\pepper_data-science_practising\\OC DS\\P6 C1 Analysez vos donnÃ©es textuelles\\4470541_nlp_text_analysis.ipynb Cell 29\u001b[0m in \u001b[0;36mfreq_stats_corpora\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/franc/Projects/pepper_data-science_practising/OC%20DS/P6%20C1%20Analysez%20vos%20donn%C3%A9es%20textuelles/4470541_nlp_text_analysis.ipynb#X42sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfreq_stats_corpora\u001b[39m():\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/franc/Projects/pepper_data-science_practising/OC%20DS/P6%20C1%20Analysez%20vos%20donn%C3%A9es%20textuelles/4470541_nlp_text_analysis.ipynb#X42sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     corpora \u001b[39m=\u001b[39m defaultdict(\u001b[39mlist\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/franc/Projects/pepper_data-science_practising/OC%20DS/P6%20C1%20Analysez%20vos%20donn%C3%A9es%20textuelles/4470541_nlp_text_analysis.ipynb#X42sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# CrÃ©ation d'un corpus de tokens par artiste\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/franc/Projects/pepper_data-science_practising/OC%20DS/P6%20C1%20Analysez%20vos%20donn%C3%A9es%20textuelles/4470541_nlp_text_analysis.ipynb#X42sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mfor\u001b[39;00m artiste,sentence_id \u001b[39min\u001b[39;00m artistes\u001b[39m.\u001b[39miteritems():\n",
      "\u001b[1;31mNameError\u001b[0m: name 'defaultdict' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def freq_stats_corpora():\n",
    "    corpora = defaultdict(list)\n",
    "\n",
    "    # CrÃ©ation d'un corpus de tokens par artiste\n",
    "    for artiste, sentence_ids in artistes.iteritems():\n",
    "        for sentence_id in sentence_ids:\n",
    "            corpora[artiste] += tokenizer.tokenize(\n",
    "                                    db[sentence_id]['text'].decode('utf-8').lower()\n",
    "                                )\n",
    "\n",
    "    stats, freq = dict(), dict()\n",
    "\n",
    "    for k, v in corpora.iteritems():\n",
    "        freq[k] = fq = nltk.FreqDist(v)\n",
    "        stats[k] = {'total': len(v)} \n",
    "        \n",
    "    return (freq, stats, corpora)\n",
    "\n",
    "# RÃ©cupÃ©ration des comptages\n",
    "freq, stats, corpora = freq_stats_corpora()\n",
    "df = pd.DataFrame.from_dict(stats, orient='index')\n",
    "\n",
    "# Affichage des frÃ©quences\n",
    "df.sort(columns='total', ascending=False)\n",
    "df.plot(kind='bar', color=\"#f56900\", title='Top 50 Rappeurs par nombre de mots')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure id=\"r-4867916\" data-claire-element-id=\"33104209\"><img id=\"r-4867914\" data-claire-element-id=\"8982657\" src=\"https://lh6.googleusercontent.com/PLxZshHniJeQHJirRxYYDFoIcdIFLtVs2dJx8JQannUb4rYy7EK5mKe_so51Jmg90h-BwUSO8TrRNK9jyPdmqg97wqY-5uNpCipVS4d7MWdxhScu1-4lJ_OLMbUGZzDwtakqKx4I\" alt=\"PremiÃ¨re tokenisation du corpus\" /><figcaption>PremiÃ¨re tokenisation du corpusÂ </figcaption></figure>\n",
    "\n",
    "\n",
    "**Nous voyons ici quel artiste a Ã©crit le plus de texte et de chansons.** C'est intÃ©ressant, mais qu'en est-il de la variÃ©tÃ© du champ lexical utilisÃ©, c'est Ã  dire le nombre de mots uniques utilisÃ©s par chaque artistes dans leurs chansons ?Â Nous souhaitons en effet savoir qui aÂ le vocabulaire le plus riche !ğŸ˜€\n",
    "\n",
    "Pour le savoir, nous devons reprÃ©senter un document (ou ici, un artiste)Â par ce qu'on appelle un **bag-of-words**.\n",
    "\n",
    "ModifionsÂ donc notre fonction `freq_stats_corpora` pour faire le comptage du vocabulaire unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_stats_corpora():\n",
    "    corpora = defaultdict(list)\n",
    "    for artiste,sentence_ids in artistes.iteritems():\n",
    "        for sentence_id in sentence_ids:\n",
    "            corpora[artiste] += tokenizer.tokenize(\n",
    "                                    db[sentence_id]['text'].decode('utf-8').lower()\n",
    "                                )\n",
    "        \n",
    "    stats, freq = dict(), dict()\n",
    "\n",
    "    for k, v in corpora.iteritems():\n",
    "        freq[k] = fq = nltk.FreqDist(v)\n",
    "        stats[k] = {'total': len(v), 'unique': len(fq.keys())}\n",
    "\n",
    "    return (freq, stats, corpora)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichons Ã  nouveau nos comptages :\n",
    "\n",
    "<img id=\"r-4867949\" data-claire-element-id=\"8982808\" src=\"https://lh5.googleusercontent.com/z1_GN_SjfpGmYi2ApJgW_-Som9c6gB_s3ICts86mQm-S9QWfyCFlt5P_H7Q-IDs0-otR_ULVz5-yU0C30krRuMPAstmiqRJvQ9CfoviYWLkg5188r5_txrOrP7ErkhvxW46XNl4J\" alt=\"\" />\n",
    "\n",
    "Mais Ã§a ne se terminera donc jamais ?! ğŸ˜¢\n",
    "\n",
    "Pour faciliter des choses, nous dÃ©composons les diffÃ©rentes Ã©tapes.Â Ã€ force d'utiliser des corpus de textes, vous saurez les traiter de maniÃ¨re un peu plus automatique en fonction de votre problÃ©matique.\n",
    "\n",
    "Ceci-dit, le prÃ©traitement du texte est une premiÃ¨re Ã©tape importante et il faut vraiment observer le contenu de votre corpus aprÃ¨s transformation pour Ãªtre sÃ»r que les donnÃ©es correspondent Ã  ce que vous dÃ©sirez, en vue des traitements ultÃ©rieurs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Vous possÃ©dez Ã  prÃ©sentÂ une premiÃ¨re idÃ©e des Ã©tapes qui constituent le prÃ©traitement du texte : rÃ©cupÃ©ration du corpus,Â tokenisation et premiÃ¨re visualisation des diffÃ©rentes frÃ©quences."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mouais.."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code publiÃ© n'a pas Ã©tÃ© testÃ©.\n",
    "\n",
    "Tutos, bouquins sÃ©rieux pour apprendre Ã  utiliser NLTK:\n",
    "* https://realpython.com/nltk-nlp-python/\n",
    "* https://www.nltk.org/book/\n",
    "* https://riptutorial.com/nltk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2. Nettoyez et normalisez les donnÃ©es\n",
    "\n",
    "AprÃ¨s la tokenization,Â voyons comment nettoyer et normaliser notre corpus afin d'obtenir une matrice de vocabulaire et un dictionnaire reprÃ©sentatifs de nos documents."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PremiÃ¨re passe de nettoyage : supprimer les stopwords (en franÃ§ais, les mots vides)\n",
    "\n",
    "LaÂ premiÃ¨re manipulation souvent effectuÃ©e dans le traitement du texte est la suppression de ce qu'on appelle en anglais les *stopwords*. Ce sont les mots trÃ¨s courants dans la langue Ã©tudiÃ©e (\"et\", \"Ã \", \"le\"... en franÃ§ais) qui **n'apportent pas de valeur informative** pour la comprÃ©hension du \"sens\" d'un document et corpus. Il sont trÃ¨s frÃ©quents et ralentissent notre travail : nous souhaitons donc les supprimer.\n",
    "\n",
    "Il existe dans la librairieÂ NLTK une liste par dÃ©faut des stopwords dans plusieurs langues, notamment le franÃ§ais. Mais nous allons faire ceci d'une autre maniÃ¨re : on va supprimer les mots les plus frÃ©quents du corpus et considÃ©rer qu'il font partie du vocabulaire commun et n'apportent aucune information. Ensuite on supprimera aussi les stopwords fournis par NLTK.\n",
    "\n",
    "Allez, on s'en dÃ©barasse !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PremiÃ¨rement, on rÃ©cupÃ¨re la frÃ©quence totale de chaque mot\n",
    "# sur tout le corpus d'artistes\n",
    "freq_totale = nltk.Counter()\n",
    "for k, v in corpora.iteritems():\n",
    "    freq_totale += freq[k]\n",
    "\n",
    "# DeuxiÃ¨mement on dÃ©cide maniÃ¨re un peu arbitraire du nombre de mots\n",
    "# les plus frÃ©quents Ã  supprimer.\n",
    "# On pourrait afficher un graphe d'Ã©volution du nombre de mots pour\n",
    "# se rendre compte et avoir une meilleure heuristique. \n",
    "most_freq = zip(*freq2.most_common(100))[0]\n",
    "\n",
    "# On crÃ©Ã© notre set de stopwords final qui cumule ainsi les 100 mots\n",
    "# les plus frÃ©quents du corpus ainsi que l'ensemble de stopwords\n",
    "# par dÃ©faut prÃ©sent dans la librairie NLTK\n",
    "sw = set()\n",
    "sw.update(stopwords)\n",
    "sw.update(tuple(nltk.corpus.stopwords.words('french')))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avonsÂ maintenant le nombre de mots uniques non *stopwords* utilisÃ©s par les artistes. Pour rappel, onÂ souhaiteÂ comprendre la variÃ©tÃ© lexicale des rappeurs choisis. Il est donc logique de supprimer les mots les plus utilisÃ©s, ce qui signifie par extension qu'ils ne sont pas porteurs de sens.\n",
    "\n",
    "On rÃ©effectue notre tokenisation en ignorant les *stopwords* et on affiche ainsi notre nouveau histogramme des frÃ©quencesÂ duquel on a supprimÃ© les stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_stats_corpora2(lookup_table=[]):\n",
    "    corpora = defaultdict(list)\n",
    "    for artist, block_ids in lt_artists.iteritems():\n",
    "        for block_id in block_ids:\n",
    "            tokens = tokenizer.tokenize(db_flat[block_id]['text'].decode('utf-8'))\n",
    "            corpora[artist] += [w for w in tokens if not w in list(sw)]\n",
    "\n",
    "    stats, freq = dict(), dict()\n",
    "\n",
    "    for k, v in corpora.iteritems():\n",
    "        freq[k] = fq = nltk.FreqDist(v)\n",
    "        stats[k] = {'total': len(v), 'unique': len(fq.keys())}\n",
    "    return (freq, stats, corpora)\n",
    "\n",
    "freq2, stats2, corpora2 = freq_stats_corpora2()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img id=\"r-4867956\" data-claire-element-id=\"8982841\" src=\"https://lh6.googleusercontent.com/HaChBwQxXRmQ-s3X_DDUBkK-meICT_crs7Q970w6ooy-YmiIgZ_gMA04gwUHCZ1UZsDBEwDUp69vnN5phwd0s63UvY-uWh0DI9Iww0nhf_bLQ0o0K6xXQt-u2UqAXUyqpQETqPlY\" alt=\"\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a bien un changement dans le classement, maintenant qu'on a enlevÃ© les mots les plus communs, si vous comparez au classement du chapitre prÃ©cÃ©dent."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ›ˆ J'effectue ce classement Ã  titre d'exercice. Il existeÂ de nombreux autres de critÃ¨res (taille du rÃ©pertoire, durÃ©e de la carriÃ¨re, etc.) qui ne sont pas pris en compte."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeuxiÃ¨me passe : lemmatisation ou racinisation (stemming)\n",
    "\n",
    "Plus qu'une derniÃ¨re Ã©tape etÂ vous en aurez terminÃ© avec le prÃ©traitement !\n",
    "\n",
    "Le processus de Â« lemmatisation Â» consiste Ã  reprÃ©senter les mots (ou Â« lemmes Â» ğŸ˜‰) sous leur forme canonique. Par exemple pour un verbe, ce sera son infinitif. Pour un nom, son masculin singulier. L'idÃ©e Ã©tant encore une fois de ne **conserver que le sens des mots** utilisÃ©s dansÂ le corpus.\n",
    "\n",
    "Si l'on reprend notre exemple prÃ©cÃ©dent, \"Bonjour, je suis un texte d'exemple pour le cours d'Openclassrooms. Soyez attentifs Ã  ce cours !\"\n",
    "\n",
    "L'idÃ©al serait d'extraire les lemmes suivants : Â« bonjour, Ãªtre, texte, exemple, cours, openclassrooms, Ãªtre, attentif, cours Â». Dans le processus de lemmatisation, on transforme donc Â« suis Â» en Â« ÃªtreÂ»  et Â« attentifs Â» en Â« attentif Â».\n",
    "\n",
    "Dans notre cas, je voulais Ã©tudier la richesse du vocabulaire des artistes. C'est donc mieux de compter le nombre d'occurrences du verbe Ãªtre plutÃ´t que de compter sÃ©parÃ©ment chaque usage de conjugaison de ce mÃªme verbe. De mÃªme pour les pluriels etc. On estime que c'est plus reprÃ©sentatif, j'espÃ¨re que vous Ãªtes d'accord ! ğŸ˜\n",
    "\n",
    "Il existe un autre processus qui exerce une fonction similaire qui s'appelle la **racinisation** (ou *stemming* en anglais). Cela consiste Ã  ne conserver que la racine des mots Ã©tudiÃ©s. L'idÃ©e Ã©tant de supprimer les suffixes, prÃ©fixes et autres des mots afin de ne conserver que leur origine. C'est un procÃ©dÃ© plus simple que la lemmatisationÂ et plus rapide Ã  effectuer puisqu'on tronque les mots essentiellement contrairement Ã  la lemmatisation qui nÃ©cessite d'utiliser un dictionnaire.\n",
    "\n",
    "Dans notre cas, on va effectuer une racinisation parce qu'il n'existe pas de fonction de lemmatisation de corpus franÃ§ais dans NLTK ğŸ˜¶ Je suis d'accord que ce serait encore mieux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import FrenchStemmer\n",
    "\n",
    "stemmer = FrenchStemmer()\n",
    "\n",
    "def freq_stats_corpora3(lookup_table=[]):\n",
    "    corpora = defaultdict(list)\n",
    "    for artist, block_ids in lt_artists.iteritems():\n",
    "        for block_id in block_ids:\n",
    "            tokens = tokenizer.tokenize(db_flat[block_id]['text'].decode('utf-8').lower())\n",
    "            corpora[artist] += [stemmer.stem(w) for w in tokens if not w in list(sw)]\n",
    "\n",
    "    stats, freq = dict(), dict()\n",
    "\n",
    "    for k, v in corpora.iteritems():\n",
    "        freq[k] = fq = nltk.FreqDist(v)\n",
    "        stats[k] = {'total': len(v), 'unique': len(fq.keys())}\n",
    "    return (freq, stats, corpora)\n",
    "\n",
    "freq3, stats3, corpora3 = freq_stats_corpora3()\n",
    "df3 = pd.DataFrame.from_dict(stats3, orient='index').sort(columns='unique', ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img id=\"r-4867961\" data-claire-element-id=\"8982852\" src=\"https://lh3.googleusercontent.com/an1lAMmPNmo9QcJk5AHs5bztDOGW6d4j2cUbfKu9u-k1LQoe7tmlefdiuUax_rxyDtSaUtGLsNcEgzFfWQRA-ZVPr0i-wjHQtZTfDxEbqlX0QX6JS5zM8PTHebQ8fyv6FICD4ng0\" alt=\"\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a ici utilisÃ© des Ã©tapes de nettoyage classique de texte mais en rÃ©alitÃ©, il est parfois utile de conserver le texte brut quand on est pas dans une recherche du sens d'un document mais de phrases dans leur ensemble puisqu'on cherche le lien entre les diffÃ©rents mots. On aura un aperÃ§u de cet aspect dansÂ les prochains chapitres."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Vous avez effectuÃ© quelques Ã©tapes essentielles du prÃ©traitement du texteÂ : tokenisation, suppression des stop-words, lemmatisation et stemming.Â Nous pouvons maintenant passer Ã  la crÃ©ation de notreÂ ensemble de features reprÃ©sentatives de notre corpus de texte. C'est le sujet de la prochaine partie ! Suivez-moi."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3. TP - Faites vos premiers pas dans l'analyse de donnÃ©es textuelles\n",
    "\n",
    "VidÃ©o d'introduction (2:49) : https://vimeo.com/746902007"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contexte\n",
    "\n",
    "Pour ce premier TP, nous allons travailler sur un jeu de donnÃ©es simple mais trÃ¨s intÃ©ressant. Il sâ€™agit dâ€™un corpus de tweets, pour lesquels il s'agit de prÃ©dire sâ€™ils font rÃ©fÃ©rence Ã  une â€œcatastropheâ€ ou non.\n",
    "\n",
    "Il sâ€™agit dâ€™une compÃ©tition Kaggle. Pour ceux qui ne connaissent pas encore Kaggle, câ€™est LA plateforme web orientÃ©e Data Science. Elle hÃ©berge des datasets, des notebooks et des compÃ©titions. Câ€™est aussi un rÃ©seau social, sur lequel vous pouvez notamment publier vos notebooks.\n",
    "\n",
    "Le jeu de donnÃ©es peut Ãªtre trouvÃ© [Ã  cette adresse](https://www.kaggle.com/competitions/nlp-getting-started/data)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ›ˆ Nous travaillerons uniquement sur le jeu de donnÃ©es de **train**.Â \n",
    "\n",
    "La colonne â€œ**target**â€ fait rÃ©fÃ©rence Ã  la nature du tweet : â€œcatastropheâ€ = 1, â€œpas catastropheâ€ = 0."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consignes\n",
    "\n",
    "Le TP se dÃ©compose en 2 parties :\n",
    "\n",
    "1 - **Exploratory Data Analysis**Â : il vous est demandÃ© de faire un premier notebook afin de **comprendre, dâ€™explorer et dâ€™effectuer un premier nettoyage** des donnÃ©es. Vous devez notamment Ãªtre capable de rÃ©pondre aux questions suivantes :Â \n",
    "* Quelle est la forme du Dataframe ?Â \n",
    "* Y a t-il des valeurs manquantes ou des valeurs dupliquÃ©es ?Â \n",
    "* Quelles sont les colonnes qui vont nous intÃ©resser ?Â \n",
    "* Y a-t-il des donnÃ©es aberrantes ou des incohÃ©rences majeures dans les donnÃ©es ?Â \n",
    "* Y a t-il des tweets anormalement longs / courts ? Peut-on les considÃ©rer comme des outliers ?Â \n",
    "* Quel est le ratio tweet qui parlent de â€œcatastrophesâ€ / tweet normaux ?\n",
    "* En regardant quelques tweets au hasard, peut-on deviner facilement la â€œtargetâ€ ?Â \n",
    "* Peut-on dÃ©jÃ  dÃ©tecter des â€œpatternsâ€ ou des mots clÃ©s dans les tweets?\n",
    "* A votre avis quel serait lâ€™accuracy score quâ€™un humain pourrait obtenir sâ€™il prÃ©disait  les donnÃ©es â€œÃ  la mainâ€ ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ›ˆ Le tempsÂ indicatif proposÃ©Â pour ce travail est de 30 min Ã  1 heure.Â "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - **Text Processing** : Il vous est demandÃ© dâ€™effectuer un premier traitement des donnÃ©es textuelles (colonne â€˜textâ€™). Il sâ€™agira de transformer les donnÃ©es textuelles en **tokens** et de **rÃ©duire la dimensionnalitÃ© du corpus** en rÃ©duisant le vocabulaire (le nombre de tokens diffÃ©rents). Lâ€™enjeu est complexe, il en faut ni trop, ni trop peuâ€¦ Pour vous aider dans ce travail,Â essayez de rÃ©pondreÂ aux questions suivantes :Â \n",
    "\n",
    "* Pouvez-vous Ã©crire une fonction qui : tokenize un document, supprime les stopwords, supprime les tokens de moins de 3 lettres ?\n",
    "* Comment peut-on reconstituer le corpus (c'est-Ã  dire un texte avec lâ€™ensemble des documents) ?Â \n",
    "* Une fois ce corpus constituÃ©, combien de tokens uniques le constitue? Ce nombre vous apparaÃ®t-il faible, important, gigantesque ?\n",
    "* Comment rÃ©duire ce nombre de tokens uniques, ou autrement dit â€œcomment rÃ©duire la taille du vocabulaireâ€ de ce corpus ?Â \n",
    "* Combien de tokens sont prÃ©sents une seule fois ? Ces tokens nous seront-ils utiles ?Â \n",
    "* Appliquer une mÃ©thode de stemmatisation ou de lemmatisation peut-elle nous aider Ã  rÃ©duire la dimensionnalitÃ© du corpus ?Â \n",
    "* Comment visualiser graphiquement, par un WordCloud par exemple, les tokens les plus prÃ©sents ?Â \n",
    "* Pouvez vous appliquer tous les traitements Ã©voquÃ©s afin de crÃ©er une nouvelle colonne â€œtextâ€ qui serait plus pertinente ?Â "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ›ˆ Le temps indicatif proposÃ© pour ce travail est trÃ¨s variable. Il dÃ©pend notamment de votre connaissance du sujet mais aussi et surtout de votre degrÃ© d'exigence. Certains pourront y passer 2 ou 4h, d'autres une journÃ©e entiÃ¨re. Si vous vous sentez bloquÃ©s, perdus, ou vous ne savez pas conclure le travail, pas de soucis! Les vidÃ©os ci dessous sont lÃ  pour Ã§a."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âš  Avant de visionner les vidÃ©os ci dessous, assurez vous d'avoir essayÃ© de faire le travail par vous mÃªme ! En effet, c'est durant cette phase que vous apprendrez le plus."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour l'EDA : https://vimeo.com/746901784 (32:02)\n",
    "\n",
    "penser Ã  utiliser davantage seaborn + Ã  faire des sauvegardes de mes rÃ©sultats intermÃ©diaires.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le Text Processing : https://vimeo.com/746902006 (43:06)\n",
    "\n",
    "libs Ã  connaÃ®tre :\n",
    "* wordcloud, pillow : graphismes\n",
    "* pandarallel : exÃ©cution multi-cpu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aller plus loin :\n",
    "\n",
    "Nous nâ€™avons volontairement pas Ã©voquÃ© dans le cadre de ce TP plusieurs sujets. Nous vous laissons le soin de poursuivre librement les pistes Ã©voquÃ©es ci dessous :\n",
    "\n",
    "* Nous avons choisi de ne garder que les â€œmotsâ€ au sens grammatical du terme. Mais est-ce bien judicieux ? En effet, lâ€™utilisation dâ€™un **emoji** ou de certains **caractÃ¨res de ponctuation** peut Ãªtre trÃ¨s impactant. Par exemple:  â€œ terrorist attack downtown !!! ğŸ˜±ğŸ˜±ğŸ˜±â€\n",
    "* Nous avons choisi de transformer les documents avec la mÃ©thode .lower(). Mais est-ce bien judicieux ? En effet, les **lettres capitales** sont peut-Ãªtre plus utilisÃ©es dans des textes ayant un impact fort. Par exemple : OMG, THE BUILDING IS BURNING !!!Â \n",
    "* Nous nâ€™avons pas Ã©voquÃ© la notion de **bi-gramme** ou de **tri-gramme** (groupe de 2 ou 3 mots se faisant suite). Existe-t-il des bi ou tri-grammes qui seraient intÃ©ressants ?\n",
    "* Le **stemmer et le lemmentizer*** de NLTK ne sont pas trÃ¨s â€œpuissantsâ€. Est-ce que la librairie spacy nous propose des outils plus intÃ©ressants ?Â \n",
    "* Qu'est-ce que le **POS** (part of speech) ? Spacy peut-il nous aider Ã  ne garder que les tokens faisant rÃ©fÃ©rence aux adjectifs ou aux verbes? Cela peut-il avoir un impact sur la taille de notre vocabulaire ?Â "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ›ˆ N'hÃ©sitez pas Ã  rendre votre travail public ! Ce travail est le votre, vous pouvez mettre votre notebook sur Kaggle ou sur github. Cela vous permettra de le rendre disponible pour un futur recruteur, de pouvoir le retrouver facilement ou encore d'inspirer de futurs apprenants en Machine Learning !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ressources complÃ©mentaires :\n",
    "\n",
    "Voici une liste non exhaustive de ressources complÃ©mentaires pour poursuivre votre travail :\n",
    "\n",
    "* Une vidÃ©o de [freecodecamp sur NLTK](https://www.youtube.com/watch?v=X2vAabgKiuM) (38:09 en anglais)\n",
    "* Une vidÃ©o de [freecodecamp sur Spacy](https://www.youtube.com/watch?v=dIUTsFT2MeQ) : (3:02:32 en anglais)\n",
    "* âœ” Une vidÃ©o de [David Louapre (Science Ã©tonnante) sur le SOA (State Of Art) du NLP](https://www.youtube.com/watch?v=CsQNF9s78Nc) (26:15). ATTENTION : la vidÃ©o couvre des notions beaucoup plus complexes que celles prÃ©sentÃ©es dans la premiÃ¨re partie de cours. Ces notions seront couvertes dans les chapitres suivants.\n",
    "    * https://nlp.stanford.edu/projects/glove/\n",
    "* Quelques notebooks Kaggle Ã  lire :Â Â [un premier notebook](https://www.kaggle.com/code/longtng/nlp-preprocessing-feature-extraction-methods-a-z),Â [un deuxiÃ¨me notebook](https://www.kaggle.com/code/dikshabhati2002/nlp-for-beginners),Â [**un troisiÃ¨me notebook**](https://www.kaggle.com/code/ashagutlapalli/nlp-101-with-nltk-and-spacy-text-analysis)Â (en anglais. ATTENTION : certaines notions prÃ©sentes dans ces notebooks sont beaucoup plus complexes que celles prÃ©sentÃ©es dans la premiÃ¨re partie de cours. Ces notions seront couvertes dans les chapitres suivants.Â \n",
    "\n",
    "## Commentaires, suggestions ou questions :\n",
    "\n",
    "Nâ€™hÃ©sitez pas Ã  nous faire un retour : nous contacter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  2.1. ReprÃ©sentez votre corpus en \"bag of words\" "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VidÃ©o introductive : https://vimeo.com/284320353\n",
    "\n",
    "AprÃ¨s avoir vu les diffÃ©rents types de nettoyage du texte possible dans les chapitres prÃ©cÃ©dent,Â nous allonsÂ maintenant Ã©tudierÂ comment extraireÂ l'information du texteÂ pour leÂ traitement ultÃ©rieur par des modÃ¨les de machine learning.Â En d'autres termes,Â nous cherchons une reprÃ©sentation du langage pour un modÃ¨le statistique qui vise Ã  exploiter des donnÃ©es textes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qu'est-ce qu'un Â« bag of words Â»\n",
    "\n",
    "La maniÃ¨re la plus simple de reprÃ©senter un document, c'est ce qu'on a effectuÃ© dans le chapitre prÃ©cÃ©dent oÃ¹ l'on a considÃ©rÃ© tous les mots utilisÃ©s pour chaque artiste, sans distinction ni dÃ©pendance par vers, chanson, etc. L'analogie est donc qu'on a considÃ©rÃ© chaque artiste parÂ la reprÃ©sentation brute d'un \"sac\" de tous les mots qu'il a utilisÃ©, sans soucis de contexte (ordre, utilisation, etc).\n",
    "\n",
    "On peut faire la mÃªme chose Ã  l'Ã©chelle d'un document qu'on reprÃ©sente par un ensemble des mots qu'il contient. En pratique, Ã§a peut Ãªtre par exemple un vecteur de frÃ©quence d'apparition des diffÃ©rents mots utilisÃ©s (ou stem ğŸ˜‰).\n",
    "\n",
    "Une reprÃ©sentation bag-of-words classique sera donc celle dans laquelle on reprÃ©sente chaque document par un vecteur de la taille du vocabulaire $|V|$Â et on utilisera la matrice composÃ©e de lâ€™ensemble de ces $n$ documents qui forment le corpus comme entrÃ©e de nos algorithmes.\n",
    "\n",
    "VidÃ©o : https://vimeo.com/284320371\n",
    "\n",
    "* Analyse des co-occurrences : bigrammes, etc (n-grammes).\n",
    "* probabilitÃ© conditionnelle : elle n'est pas expliquÃ©e clairement\n",
    "* TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "* TN-SNE\n",
    "* Named Entity Recognition\n",
    "* Extraction des relations, des Ã©vÃ©nements\n",
    "* POS tagging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prendre en compte les co-occurences\n",
    "\n",
    "La premiÃ¨re chose Ã  considÃ©rer, au delÃ  d'une tokenisation, c'est qu'il est possible de sÃ©parer le texte en groupes de plusieurs mots. On appelle les groupes de mots les n-grammes (n-gram) : bigrammes pour les couples de mots, trigrammes pour les groupes de 3, etc. SÃ©parer en mot unique est en fait un cas particulier appelÃ© unigrammes.\n",
    "\n",
    "Par exempleÂ dans la phrase : Â« Je mange une pomme Â», on peut extraire les bigrammes {(je, mange), (mange, une) et (une, pomme)}\n",
    "\n",
    "**Pourquoi utiliser des n-grammes avec n>1 ?**\n",
    "\n",
    "Lorsqu'on fait face Ã  une problÃ©matique de modÃ©lisation du langage, on voit bien queÂ pour Ã©tudier idÃ©alement le sens d'un mot il faudrait l'observer dans son contexte. Il existe donc dans un texte (et par extension dans le langage) une forme de dÃ©pendance plus ou moins grande entre les mots.\n",
    "\n",
    "A titre d'exemple, le pronom \"je\" aura grandement plus de chance d'Ãªtre suivi d'un verbe. On peut donc traiter chaque mot comme ayant une probabilitÃ© d'apparition en fonction du texte qui le prÃ©cÃ¨de, c'est Ã  dire comme une sÃ©quence. Dans l'idÃ©al, on veut traiter tout le texte de cette faÃ§on, mais ce n'est pas possible en terme de capacitÃ© de calculs.\n",
    "\n",
    "En pratique, on peut prendre les quelques mots prÃ©cÃ©dents qui reprÃ©sentent assez d'information pour avoir un modÃ¨le sÃ©quentiel (markovien) intÃ©ressant, d'oÃ¹ l'apparition des n-grammes.\n",
    "\n",
    "Par exemple on peut assigner une probabilitÃ© au bigramme (\"je\", \"mange\") :\n",
    "\n",
    "$$p(mange \\vert\\ je) = \\frac{p(mange, je)}{p(mange) p(je)}$$\n",
    "\n",
    "En pratique, on peut aussi utiliser la fonction `bigrams` de NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"Bonjour, je suis un texte d'exemple pour le cours d'Openclassrooms. Soyez attentifs Ã  ce cours !\"\n",
    "tokens = tokenizer.tokenize(test.lower())\n",
    "list(nltk.bigrams(tokens))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ›ˆ Le modÃ¨le de bag-of-words est en fait un cas particulier du modÃ¨le n-gram avec n=1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Une autre maniÃ¨re de pondÃ©rer : le tf-idf\n",
    "\n",
    "Depuis le dÃ©part, on a seulement utilisÃ© les frÃ©quences d'apparition des diffÃ©rents mots/n-grammes prÃ©sents dans notre corpus. Le problÃ¨me est que si l'on veut vraiment reprÃ©senter un document par les n-grammes qu'il contient, il faudrait le faire relativement Ã  leur apparition dans les autres documents.\n",
    "\n",
    "En effet, si un mot apparait dans d'autres documents, il est donc moins reprÃ©sentatif du document qu'un mot qui n'apparait que uniquement dans ce document.\n",
    "\n",
    "Nous avons d'abord supprimÃ© les mots les plus frÃ©quents de maniÃ¨re gÃ©nÃ©rale dans le langage (les fameux stopwords). Ã€ prÃ©sent, il ne faut pas considÃ©rer le poids d'un mot dans un document comme sa frÃ©quence d'apparition uniquement, mais pondÃ©rer cette frÃ©quence par un indicateur **si ce mot est commun ou rare** dans tous les documents.\n",
    "\n",
    "Pour rÃ©sumer, le poids du n-gramme est le suivant :\n",
    "\n",
    "$\\text{poids}=\\text{frÃ©quence du terme}Ã—\\text{indicateur similaritÃ©}$\n",
    "\n",
    "En lâ€™occurence, la mÃ©trique tf-idf (Term-Frequency - Inverse Document Frequency) utilise comme indicateur de similaritÃ© l'inverse document frequency qui est l'inverse de la proportion de document qui contient le terme, Ã  l'Ã©chelle logarithmique. Il est appelÃ© logiquement Â« inverse document frequency Â» (idf). \n",
    "\n",
    "Nous calculons donc le poids tf-idf final attribuÃ© au n-gramme :\n",
    "\n",
    "$\\text{poids}=\\text{frÃ©quence du n-gram}Ã—\\text{idf(n-gramme)}$\n",
    "\n",
    "Dans notre exemple, un document Ã©gale un artiste. Pour connaÃ®tre les termes qui reprÃ©sentent le plus un artiste, nous allons utiliser la fonction tf-idf de scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        file_path = subdir + os.path.sep + file\n",
    "        shakes = open(file_path, 'r')\n",
    "        text = shakes.read()\n",
    "        lowers = text.lower()\n",
    "        no_punctuation = lowers.translate(None, string.punctuation)\n",
    "        token_dict[file] = no_punctuation\n",
    " \n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words=sw)\n",
    "values = tfidf.fit_transform(token_dict.values())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant qu'on a cet indicateur, on peut comparer les diffÃ©rents champs lexicaux qui reprÃ©sentent le plus un artiste. Qui dit comparaison dit similaritÃ©. On peut donc utiliser ... t-SNE !\n",
    "\n",
    "<figure id=\"r-5174005\" data-claire-element-id=\"30532046\"><img id=\"r-4868002\" data-claire-element-id=\"8982984\" src=\"https://user.oc-static.com/upload/2017/12/11/15129745591631_t-sne.png\" alt=\"PremiÃ¨re visualisation t-sne de notre corpus\" /><figcaption>PremiÃ¨re visualisation t-sne de notre corpus</figcaption></figure>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraire les informations\n",
    "\n",
    "La rÃ©cupÃ©ration de caractÃ©ristiques va assez loin puisqu'on essaie de dÃ©gager de nos documents texte non structurÃ©s des informations structurÃ©es informatives trÃ¨s restreintes :\n",
    "\n",
    "* **NER (Named Entity Recognition)** : reconnaÃ®tre des personnes, endroits, entreprises, etc.\n",
    "* **Extraction de relations** : essayer d'extraire des relations sÃ©mantiques entre diffÃ©rents termes du texte. Par exemple, des relations familiales (\"Marie est l'enfant de Patrick\") spatiales (\"Le piano est devant la fenÃªtre\"), etc. Ces informations peuvent ensuite Ãªtre stockÃ©es dans une base de donnÃ©es relationnelles ou un graphe.\n",
    "* **Extraction d'Ã©vÃ©nements** : extraire des actions qui arrivent Ã  nos entitÃ©s. Par exemple \"le cours de l'action X a augmentÃ© de 5%\" ou bien \"le prÃ©sident Ã  dÃ©clarÃ© X dans son discours\"\n",
    "* **POS Tagging (Part-of-Speech Tagging)** : reprÃ©sente les mÃ©thodes qui rÃ©cupÃ¨rent la nature grammatical des mots dâ€™une phrase - nom, verbe, adjectif, etc. Ce sont des propriÃ©tÃ© qui peuvent servir de caractÃ©ristiques utile lors de la crÃ©ation de certains modÃ¨les"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ›ˆ Ce ne sont pas des mÃ©thodes que lâ€™on va traiter dans ce cours. Nous vous donnerons cependant les bases pour pouvoir vous documenter et crÃ©er votre propre tagger. Notez qu'en franÃ§ais, il existe peu de ressources toute faites, contrairement Ã  l'anglais oÃ¹ il existe des NER gÃ©nÃ©ralistes.\n",
    "\n",
    "<img id=\"r-4868038\" data-claire-element-id=\"8983048\" src=\"https://lh6.googleusercontent.com/fOK5o_kpnRxaW_F0YiEeK85GNHrzSWDXdVn0hvwgJf7Wqau-5l-vh0JnM_ibe34ErAd3efpYCfaOdPVV-Qk2HR55vpitHM4B8jGhsq7j94e2lH2Xv2MglkTHh-HRwV-oGs93he0U\" alt=\"\" />\n",
    "\n",
    "*Vous pouvez essayer l'API Google Natural Langage pour avoir une idÃ©e des capacitÃ© d'extraction d'information possible par les algorithmes industriels.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention aux matrices creuses\n",
    "\n",
    "Avec les mÃ©thodes de comptage Ã©voquÃ©es, nous crÃ©ons en rÃ©alitÃ© des Â« matrices creuses Â». En effet, les mots ne sont pas prÃ©sents dans chaque document (le ratio vocabulaire / taille de document est trop Ã©levÃ©). De plus, on utilise plus souvent certains mots (â€œetâ€, â€œleâ€, etc.) et dâ€™autres plus rarement (dans des contextes prÃ©cis).\n",
    "\n",
    "Cette grosse diffÃ©rence crÃ©Ã© des matrices larges (de la taille Â« nombre de documents * taille du vocabulaire Â») qui sont essentiellement vides. On verra quâ€™on peut utiliser ces matrices avec un certain nombre dâ€™algorithmes, mais câ€™est tout de mÃªme un gaspillage non nÃ©gligeable de ressources que de travailler avec des matrices de cette taille alors que la plupart des entrÃ©es ne sont pas informatives.\n",
    "\n",
    "De plus, les matrices creuses peuvent biaiser les algorithmes qui considÃ¨rent ainsi que les observations Ã  zÃ©ro (qui sont prÃ©sentes en majoritÃ©) reprÃ©sente une information Ã  prendre en considÃ©ration. Si on pense en terme de moyenne par exemple, elle sera Ã©crasÃ©e par la prÃ©sence de toute ces entrÃ©es vides sans pour autant apporter plus de sens Ã  notre calcul. Nous allons voir dans les prochaines parties des alternatives de reprÃ©sentation de texte afin de contrecarrer ce problÃ¨me quand c'est nÃ©cessaire."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Ces extractions de caractÃ©ristiques seront assez intuitives en fonction du problÃ¨me rencontrÃ©. L'idÃ©e principale Ã©tant de transformer cette masse de texte non structurÃ©e en donnÃ©es digestes pour vos algorithmes et vos capacitÃ©s de calculs. Cependant, ce type de reprÃ©sentation crÃ©Ã© des matrices creuses quâ€™il est parfois difficile Ã  gÃ©rer, par exemple dans le cadre de lâ€™utilisation de rÃ©seaux de neurones."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Effectuez des plongements de mots (word embeddings)\n",
    "\n",
    "VidÃ©o introductive : https://vimeo.com/284320383\n",
    "\n",
    "Les algorithmes et modÃ¨les de machine learning utilisent des entrÃ©es numÃ©riques, puisque nous travaillons avec des espaces topologiques voire mÃ©triques la plupart du temps.\n",
    "\n",
    "? Alors comment faire pour reprÃ©senter du texte.. ?\n",
    "\n",
    "Lorsquâ€™on traite des donnÃ©es brutes Ã  grande dimensions comme des images ou du spectre audio, on utilise directement des vecteurs de coefficients associÃ©es aux intensitÃ© de pixel dans le premier cas ou les coefficients de densitÃ© spectrales dans le second cas.\n",
    "\n",
    "Le problÃ¨me avec un texte, câ€™est que lâ€™on va traiter les mots et groupes de mots comme des entitÃ©s symboliques non porteuses de sens, câ€™est Ã  dire indÃ©pendantes les unes des autres.\n",
    "\n",
    "<img id=\"r-4868041\" data-claire-element-id=\"8983061\" src=\"https://lh5.googleusercontent.com/ucKgKMHzkUwGqm9wWn-1nTpLIEyhywOQF1q42TwwL49QiKCYp4wrcQ67qOJqmZu-5oxNxDHf6txQ6OZDsKvT_FHuCEIK4kgzYxhvTTv6N7XPp8y5LaJhKTmSEvQ3G04XlVAC9JEu\" alt=\"\" />\n",
    "\n",
    "ğŸ›ˆ On appelle la technique de reprÃ©sentation dâ€™un mot, ou un ensemble de mots en vecteurs de dimension infÃ©rieure, word embeddings, soit littÃ©ralement Â« plongement de mot Â».\n",
    "\n",
    "Dans les chapitre prÃ©cÃ©dents,  nous avons vu une premiÃ¨re maniÃ¨re de reprÃ©senter nos documents comme des bag-of-words ou bag-of-ngrams, essentiellement des mÃ©thodes de comptage direct (frÃ©quence ou tf-idf). On a donc techniquement reprÃ©sentÃ© chaque document par un vecteur de frÃ©quences du dictionnaire de mots que lâ€™on avait Ã  disposition. Nous sommes amenÃ© Ã  traiter des donnÃ©es lacunaires ou creuses (en anglais sparse). MÃªme avec TF-IDF, on considÃ¨re des comptages dÃ©terministes comme reprÃ©sentatifs de nos donnÃ©es, en lâ€™occurrence la frÃ©quence du mot et la frÃ©quence inverse du document.\n",
    "\n",
    "Une rÃ©cente famille de techniques (circa 2013) a permi de repenser ce modÃ¨le avec une reprÃ©sentation des mots dans un espace avec une forme de similaritÃ© entre eux (c'est-Ã -dire probabiliste), dans lesquels le sens des mots les rapproche dans cet espace, en terme de distances statistiques. Câ€™est un plongement dans un espace de dimension infÃ©rieur autour de 20-100 dimensions gÃ©nÃ©ralement. Son petit nom : word2vec.\n",
    "\n",
    "<img id=\"r-4868046\" data-claire-element-id=\"8983072\" src=\"https://lh4.googleusercontent.com/u3CG-wlVB5QyyRrg0_26HgCELgQLTAS6SYzygiPH7SeRQy-2B7dpZyLjX4l8Izy_XIoTPH6gvAPBNDqmx9vw8x5hYlYT9KNjFSSq5C1jfgpHyIUBEPr_mS7A9sDBHgZYGufB-PaU\" alt=\"\" />\n",
    "\n",
    "Mais comment trouver le sens des mots ? Câ€™est un peu vague comme concept, non ?\n",
    "\n",
    "Effectivement. Lâ€™hypothÃ¨se principale de ces mÃ©thodes Ã©tant de prendre en compte le â€œcontexteâ€ dans lequel le mot a Ã©tÃ© trouvÃ©, câ€™est Ã  dire les mots avec lesquels il est souvent utilisÃ©. On appelle cette hypothÃ¨se distributional hypothesis.\n",
    "\n",
    "Et ce qui est intÃ©ressant, câ€™est que ce contexte permet de crÃ©er un espace qui rapproche des mots qui ne se sont pas forcÃ©ment trouvÃ©s Ã  cÃ´tÃ© les uns des autres dans un corpus ! Ces mÃ©thodes de reprÃ©sentation vectorielles ont aussi permis dâ€™entraÃ®ner des modÃ¨les de reprÃ©sentation des mots sur des corpus beaucoup plus grands (des centaines de milliards de mots par exemple...)\n",
    "\n",
    "Ces reprÃ©sentations possÃ¨dent des capacitÃ©s surprenantes. Par exemple, on peut retrouver beaucoup de rÃ©gularitÃ©s linguistiques simplement en effectuant des translation linÃ©aires dans cet espace de reprÃ©sentation. Par exemple le rÃ©sultat de vec(â€œMadridâ€) - vec(â€œSpainâ€) + vec(â€œFranceâ€) donne une position dont le vecteur le plus proche est vec(â€œParisâ€) !\n",
    "\n",
    "<img id=\"r-4868051\" data-claire-element-id=\"8983091\" src=\"https://lh6.googleusercontent.com/xvk-_fxvJp2atVExddjkGBLgUNKJKjbJexMTn7vQRdDNJBZo-eDM9dNN4ZtYzcphYdRRSGhm7NFp-kOD4UvxdzJTeSLye6a3_J_U4p5Rkyis82DwQvg40qtG-IwwSZwFM0RdDPYb\" alt=\"\" />\n",
    "\n",
    "Afin de calculer les vecteurs qui reprÃ©sentent les mots, les mÃ©thodes word2vec utilisent des perceptrons linÃ©aires simples avec une seule couche cachÃ©e. Lâ€™idÃ©e est de compresser notre corpus vers un dictionnaire de vecteurs denses de dimension bien infÃ©rieure choisie.\n",
    "\n",
    "Je nâ€™ai pas lâ€™impression que le contexte a Ã©tÃ© pris en compte, si ?\n",
    "\n",
    "On ne va pas dÃ©tailler les mÃ©thodes dâ€™entraÃ®nement en dÃ©tails, mais il faut savoir quâ€™il en existe deux principales. La premiÃ¨re appelÃ©e Â« Continuous Bag of Words Â» (CBOW), qui entraÃ®ne le rÃ©seau de neurones pour prÃ©dire un mot en fonction de son contexte, câ€™est Ã  dire les mots avant/aprÃ¨s dans une phrase. Dans la seconde mÃ©thode, on essaie de prÃ©dire le contexte en fonction du mot. Câ€™est la technique du Â« skip-gram Â».\n",
    "\n",
    "En dâ€™autres termes, lâ€™entrÃ©e du rÃ©seau de neurones dans le cadre du CBOW prend une fenÃªtre autour du mot et essaie de prÃ©dire le mot en sortie. Dans le cadre du skip-gram on essaie de faire lâ€™inverse, câ€™est-Ã -dire prÃ©dire les mots autour sur une fenÃªtre dÃ©terminÃ©e Ã  lâ€™avance Ã  lâ€™aide du mot Ã©tudiÃ© en entrÃ©e.\n",
    "\n",
    "<img id=\"r-4880386\" data-claire-element-id=\"9012501\" src=\"https://lh4.googleusercontent.com/kNbiyzjUV37jfNv65bkh4uF9zFIvso1I3DI5ccx56JP_bgjp1p92ev9bfotRSkNI7ljk-OPQGGX6Ci8oT3fVNBgaW2ZVRe9qvAUUuK7oiZ35spqtjTv1raw-F6MZf0T9Pxnuag4G\" alt=\"ReprÃ©sentation du modÃ¨le CBOW\" />\n",
    "\n",
    "Et câ€™est cette hypothÃ¨se - le fait que les mots soient caractÃ©risÃ©s par les mots les entourants - qui permet de crÃ©er cette compression. Des mots davantage associÃ©s aux mÃªmes mots seront proches dans lâ€™espace dâ€™arrivÃ©e."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utiliser un modÃ¨le de langage\n",
    "\n",
    "Avoir cette reprÃ©sentation vectorielle des mots permet dâ€™utiliser ces vecteurs comme des features dans un grand nombre de tÃ¢ches de base de traitement du langage. On peut ainsi alimenter des algorithmes classiques tels quâ€™un SVM ou un rÃ©seau de neurones avec nos vecteurs caractÃ©ristiques des mots.\n",
    "\n",
    "Pour rÃ©capituler, on peut transformer un texte en ses features soit :\n",
    "* En utilisant une reprÃ©sentation de comptage creuse - frÃ©quence dâ€™apparition du mot dans un document, ou vecteur tf-idf dâ€™un document, etc.\n",
    "* En utilisant une reprÃ©sentation type word2vec dense - dans laquelle le mot possÃ¨de une reprÃ©sentation dans un espace qui le positionne en fonction des mots adjacents\n",
    "\n",
    "Nous verrons dans les prochains chapitres comment utiliser ces reprÃ©sentations de documents dans nos algorithmes afin de pouvoir effectuer diffÃ©rentes tÃ¢ches de classification, modÃ©lisations non supervisÃ©es et autres manipulations propres au traitement du langage."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatives Ã  word2vec\n",
    "\n",
    "En pratique, on peut utiliser dâ€™autres types de reprÃ©sentations denses des mots, au-delÃ  du choix de lâ€™algorithme (CBOW, skipgram) et de la dimension. Il existe notamment dâ€™autres mÃ©thodes de plongement (word embeddings) tels que [gloVe](https://nlp.stanford.edu/projects/glove/) et [FastText](https://fasttext.cc/).\n",
    "\n",
    "[Certains favorisent](https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/) mÃªme lâ€™utilisation dâ€™une simple dÃ©composition SVD sur une matrice PMI (pointwise mutual information) qui donneraient des performances largement suffisante pour la plupart des applications industrielles.\n",
    "\n",
    "Lâ€™idÃ©e fondamentale reste la mÃªme : compresser de maniÃ¨re non supervisÃ©e la reprÃ©sentation dâ€™un mot Ã  partir dâ€™un gros corpus de texte reprÃ©sentatif de votre langage, afin dâ€™obtenir un vecteur dense qui permet de visualiser et de fournir Ã  nos algorithmes des features plus intÃ©ressantes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EntraÃ®ner son propre embedding\n",
    "\n",
    "Il existe plusieurs plongements disponibles directement en ligne (surtout en anglais) qui ont Ã©tÃ© entrainÃ©s avec les mÃ©thodes Ã©voquÃ©es plus haut. Câ€™est utile pour avoir un modÃ¨le de reprÃ©sentation trÃ¨s gÃ©nÃ©ral de votre vocabulaire sur une langue en particulier. Cependant vous Ãªtes limitÃ© au registre du corpus utilisÃ©. Par exemple si lâ€™embedding a Ã©tÃ© rÃ©alisÃ© sur WikipÃ©dia, vous allez avoir un registre relativement soutenu. Cela biaise un peu la modÃ©lisation et surtout rend moins prÃ©cis votre plongement vis Ã  vis du problÃ¨me rencontrÃ©. Cela a aussi pour effet de dissiper des diffÃ©rences entre vecteurs qui pourraient Ãªtre utiles pour votre problÃ¨me en faveur des grandes disparitÃ©s (comme lâ€™exemple masculin / fÃ©minin ou capitale / pays).\n",
    "\n",
    "Lâ€™avantage dâ€™entraÃ®ner son propre embedding est donc dâ€™avoir un plongement spÃ©cifique Ã  notre corpus et donc plus performant concernant la problÃ©matique que lâ€™on veut traiter. Vous pouvez ensuite comparer cet embedding Ã  une baseline utilisant un embedding gÃ©nÃ©ral.\n",
    "\n",
    "Vous pouvez par exemple entraÃ®ner votre propre embedding word2vec en suivant ce tutorial : https://radimrehurek.com/gensim/auto_examples/\n",
    "\n",
    "Le problÃ¨me qui en dÃ©coule est bien sÃ»r un manque de gÃ©nÃ©ralisation si le jeu de donnÃ©es utilisÃ© nâ€™est pas reprÃ©sentatif de la population totale."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Les plongements de mots sont les reprÃ©sentations les plus utilisÃ©es actuellement dans les derniÃ¨res mÃ©thodes de traitement du langage. C'est devenu un outil incontournable Ã  tester lors de vos manipulations de texte. Si vous avez le temps et les ressources nÃ©cessaires, n'hÃ©sitez pas Ã  entraÃ®ner votre propre embedding spÃ©cialisÃ© sur votre problÃ©matique."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 ModÃ©lisez des sujets avec des mÃ©thodes non supervisÃ©es\n",
    "\n",
    "VidÃ©o d'introduction : https://vimeo.com/284320403\n",
    "\n",
    "Dans le monde actuel, oÃ¹ la quantitÃ© de texte non structurÃ©, augmente drastiquement (commentaires, articles de blog, etc.), il serait vraiment utile dâ€™avoir des outils qui permettent de structurer automatiquement lâ€™information, de maniÃ¨re Ã  pouvoir rapidement accÃ©der Ã  ce qui nous intÃ©resse, filtrer le bruit mais aussi dÃ©tecter lâ€™apparition de nouveau sujet dâ€™intÃ©rÃªts.\n",
    "\n",
    "On peut retrouver cette nÃ©cessitÃ© Ã  plusieurs niveau dans les applications :\n",
    "* DÃ©tecter les fameux â€œtrending topicsâ€ de Twitter\n",
    "* Trouver les nouveaux sujets dâ€™information abordÃ©s par les mÃ©dias\n",
    "* DÃ©tecter un nouvel investissement intÃ©ressant dâ€™aprÃ¨s un groupement de textes dâ€™experts\n",
    "* Organiser un corpus de textes scientifiques autour des thÃ©matiques abordÃ©es\n",
    "* Trouver les diffÃ©rentes aspects dâ€™un produit abordÃ©s par des commentaires afin de pouvoir plus facilement lâ€™amÃ©liorer Ã  partir de feedback utilisateur\n",
    "\n",
    "<img id=\"r-4883082\" data-claire-element-id=\"9020716\" src=\"https://user.oc-static.com/upload/2017/12/13/15131779850317_Screen%20Shot%202017-11-28%20at%2015.44.16.png\" alt=\"Les Â« trending topics Â» de Twitter\" />\n",
    "Les Â« trending topics Â» de Twitter\n",
    "\n",
    "Câ€™est dans ce cadre quâ€™intervient la modÃ©lisation de sujets (*topic modeling* en anglais) qui reprÃ©sente le spectre des diffÃ©rentes approches permettant cette dÃ©tection.\n",
    "\n",
    "Dans ce chapitre, on va Ã©tudier les plus populaires, afin dâ€™avoir une intuition de cette famille dâ€™algorithmes. Il faut savoir, en revanche, quâ€™il reste difficile dâ€™appliquer directement ces algorithmes Ã  toute les situations et quâ€™il existe un grand nombre de variantes spÃ©cifiques Ã  des problÃ©matiques plus prÃ©cises qui correspondront Ã  ce que vous recherchez.\n",
    "\n",
    "Il faut donc se documenter et comprendre les diffÃ©rents critÃ¨res diffÃ©renciants (e.g. modÃ©lisation dynamique des sujets dans le temps, longueur du document, nombre de sujets abordÃ©s, etc.) qui vous permettront dâ€™effectuer un choix informÃ©.\n",
    "\n",
    "Câ€™est aussi une famille de mÃ©thode utilisÃ©e essentiellement en exploration voire semi-supervisÃ©e, câ€™est Ã  dire qui permet de dÃ©tecter si effectivement il y a de grandes catÃ©gories abordÃ©es, et ensuite les affiner lors du passage en production, et supervision des nouveaux documents entrants.\n",
    "\n",
    "Pour rÃ©sumer, la modÃ©lisation automatique de sujets permet de dÃ©tecter les sujets latents abordÃ©s dans un corpus de documents, assigner les sujets dÃ©tectÃ©s Ã  ces diffÃ©rents documents. On peut ensuite utiliser ces sujets pour effectuer des recherches plus rapide, organiser les documents ou les rÃ©sumer automatiquement."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PremiÃ¨re intuition : Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "La premiÃ¨re mÃ©thode vraiment efficace est nommÃ© **LDA** (*Latent Dirichlet Allocation*). Câ€™est une mÃ©thode non-supervisÃ©e gÃ©nÃ©rative qui se base sur les hypothÃ¨ses suivantes :\n",
    "* Chaque document du corpus est un ensemble de mots sans ordre (*bag-of-words*) ;\n",
    "* Chaque document $m$ aborde un certain nombre de thÃ¨mes dans diffÃ©rentes proportions qui lui sont propres $p(\\theta_m)$ ;\n",
    "* Chaque mot possÃ¨de une distribution associÃ©e Ã  chaque thÃ¨me $(p(\\phi_k)$. On peut ainsi reprÃ©senter chaque thÃ¨me par une probabilitÃ© sur chaque mot.\n",
    "* $z_n$ reprÃ©sente le thÃ¨me du mot $w_n$\n",
    "\n",
    "Puisque l'on a accÃ¨s uniquement aux documents, on doit dÃ©terminer quels sont les thÃ¨mes, les distributions de chaque mot sur les thÃ¨mes, la frÃ©quence dâ€™apparition de chaque thÃ¨me sur le corpus.\n",
    "\n",
    "Une reprÃ©sentation formelle sous forme de modÃ¨le probabiliste graphique est la suivante :\n",
    "\n",
    "<img id=\"r-4883117\" data-claire-element-id=\"9020814\" src=\"https://lh3.googleusercontent.com/BGQYdx1LDOr4fwpBzeI5IJbspqkb2xLRhGewRKtChQkK15KGnW9I9GLpEHpCYKp13jL2ZHZTMe9PUqNI7MZYPBKa-oOIQUlEQnZuGF3u7hH3kOnOlm-c8b-L9T2k3CXhOUt3GJBp\" alt=\"ModÃ¨le probabiliste\" />\n",
    "\n",
    "Pour rappel, un modÃ¨le gÃ©nÃ©ratif dÃ©finit une probabilitÃ© de distribution jointe sur les diffÃ©rentes variables identifiÃ©es, Ã  la fois observÃ©es et latentes. Une fois ces variables identifiÃ©es, ainsi que les diffÃ©rentes probabilitÃ©s de distribution associÃ©es, lâ€™objectif est de retrouver par exemple les distributions latentes par rapport aux variables observÃ©es.\n",
    "\n",
    "Dans notre cas, nous souhaitons retrouver les distributions de mots sur les diffÃ©rents thÃ¨mes, les diffÃ©rentes proportions de thÃ¨mes pour chaque document, les proportions dâ€™apparition dâ€™un thÃ¨me sur le corpus. Tout cela Ã  partir des diffÃ©rents documents. Ce qui nous permet par la suite de dÃ©terminer le thÃ¨me dâ€™un document, les mots les plus associÃ©s Ã  certains thÃ¨mes, etc.\n",
    "\n",
    "<img id=\"r-4868062\" data-claire-element-id=\"8983123\" src=\"https://lh5.googleusercontent.com/oLha3ucca8lMSS1mvpOPpJuYsRYgJqYncdcOPz5HGn-iZxt1qENFUBlVDCGPjaSQ5qf-rPAM1AUk4zlUa9is6Rk2QTkc6jN4ZG9iSqJQjJl9fDsfXbZCzaHjv9Agif2MMBzvenxW\" alt=\"\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ›ˆ La **loi Dirichlet** est la conjuguÃ©e de la loi multinomiale, ce qui signifie quâ€™elle sâ€™accorde bien avec cette loi en tant que distribution a posteriori en termes de factorisation. On utilise ainsi la distribution de Dirichlet sur la proportion globale des thÃ¨mes ainsi que sur chaque distribution de thÃ¨mes sur les mots."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous l'avons dit en introduction, ce modÃ¨le reprÃ©sente une version de base assez gÃ©nÃ©rale de la modÃ©lisation dâ€™un corpus de documents. Il est possible dâ€™ajouter des hypothÃ¨ses supplÃ©mentaires sur la structure des donnÃ©es afin de capturer une plus grande partie de lâ€™organisation de lâ€™information. A titre dâ€™exemple, voici une liste des hypothÃ¨ses supplÃ©mentaires qui mÃ¨nent Ã  une modÃ©lisation plus riche :\n",
    "* **La distribution des mots sur les thÃ¨mes Ã©voluent avec le temps**. Ce qui signifie quâ€™il faut crÃ©er une sÃ©quence de distribution pour chaque thÃ¨me qui permet de modÃ©liser lâ€™Ã©volution du thÃ¨me dans le temps.\n",
    "* **Certains thÃ¨mes sont plus proches que dâ€™autres**. Lâ€™hypothÃ¨se dâ€™utiliser la distribution de Dirichlet considÃ¨re que les diffÃ©rents thÃ¨mes sont complÃ¨tement indÃ©pendants alors quâ€™en rÃ©alitÃ© certains thÃ¨mes ont en gÃ©nÃ©ral plus de chances dâ€™apparaÃ®tre ensemble."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laissez la partie infÃ©rence aux librairies !\n",
    "\n",
    "Effectuer lâ€™infÃ©rence de ce modÃ¨le est relativement complexe techniquement. Il faut notamment passer par des approximations et des algorithmes qui simplifient le modÃ¨le afin de pouvoir le calculer (par exemple mean field ou gibbs sampling). Dans tous les cas on va utiliser des packages tout fait afin de travailler sur nos donnÃ©es.\n",
    "\n",
    "Les plus connus sont dÃ©jÃ  intÃ©grÃ©s directement dans les librairies (scikit implÃ©mente une version de LDA) mais il faudra par la suite effectuer vos propres recherche afin de trouver des implÃ©mentations que vous pourrez utiliser dans des cas plus prÃ©cis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utiliser LDA sur un cas pratique : Le newsgroup dataset\n",
    "\n",
    "Afin dâ€™illustrer le type de retours que lâ€™on peut avoir avec ce genre de mÃ©thodes, on va appliquer lâ€™algorithme du LDA sur un dataset classique dÃ©jÃ  prÃ©sent dans la librairie scikit : le newsgroup dataset qui regroupe un ensemble de 20,000 document articles d'actualitÃ©.\n",
    "\n",
    "VidÃ©o : https://vimeo.com/284320427"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PrÃ©traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CrÃ©er le modÃ¨le LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(learning_method=&#x27;online&#x27;, learning_offset=50.0,\n",
       "                          max_iter=5, n_components=20, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(learning_method=&#x27;online&#x27;, learning_offset=50.0,\n",
       "                          max_iter=5, n_components=20, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LatentDirichletAllocation(learning_method='online', learning_offset=50.0,\n",
       "                          max_iter=5, n_components=20, random_state=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "n_topics = 20\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tf_vectorizer = CountVectorizer(\n",
    "    max_df=0.95,\n",
    "    min_df=2,\n",
    "    max_features=1_000,\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# CrÃ©er le modÃ¨le LDA\n",
    "lda = LatentDirichletAllocation(\n",
    "        n_components=n_topics, \n",
    "        max_iter=5, \n",
    "        learning_method='online', \n",
    "        learning_offset=50.,\n",
    "        random_state=0)\n",
    "\n",
    "# Fitter sur les donnÃ©es\n",
    "lda.fit(tf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "On affiche les mots les plus reprÃ©sentatifs des sujets modÃ©lisÃ©s, afin de nous donner une idÃ©e de leur signification et voir si effectivement on trouve des catÃ©gories claires pour les humains et reprÃ©sentatifs de notre corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "people gun state control right guns crime states law police\n",
      "Topic 1:\n",
      "time question book years did like don space answer just\n",
      "Topic 2:\n",
      "mr line rules science stephanopoulos title current define int yes\n",
      "Topic 3:\n",
      "key chip keys clipper encryption number des algorithm use bit\n",
      "Topic 4:\n",
      "edu com cs vs w7 cx mail uk 17 send\n",
      "Topic 5:\n",
      "use does window problem way used point different case value\n",
      "Topic 6:\n",
      "windows thanks know help db does dos problem like using\n",
      "Topic 7:\n",
      "bike water effect road design media dod paper like turn\n",
      "Topic 8:\n",
      "don just like think know people good ve going say\n",
      "Topic 9:\n",
      "car new price good power used air sale offer ground\n",
      "Topic 10:\n",
      "file available program edu ftp information files use image version\n",
      "Topic 11:\n",
      "ax max b8f g9v a86 145 pl 1d9 0t 34u\n",
      "Topic 12:\n",
      "government law privacy security legal encryption court fbi technology information\n",
      "Topic 13:\n",
      "card bit memory output video color data mode monitor 16\n",
      "Topic 14:\n",
      "drive scsi disk mac hard apple drives controller software port\n",
      "Topic 15:\n",
      "god jesus people believe christian bible say does life church\n",
      "Topic 16:\n",
      "year game team games season play hockey players league player\n",
      "Topic 17:\n",
      "10 00 15 25 20 11 12 14 16 13\n",
      "Topic 18:\n",
      "armenian israel armenians war people jews turkish israeli said women\n",
      "Topic 19:\n",
      "president people new said health year university school day work\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic {}:\".format(topic_idx))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(lda, tf_vectorizer.get_feature_names(), 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on peut le voir, quelques sujets qui ont Ã©tÃ© modÃ©lisÃ©s sont effectivement interprÃ©tables : le sujet 4 reprÃ©sente simplement les nombres. Le sujet 5 reprÃ©sente globalement l'informatique. Le sujet 8 semble reprÃ©senter la religion, etc etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Une alternative, NMF\n",
    "\n",
    "Une autre type de modÃ©lisation de sujet automatique non supervisÃ©e est NMF (Negative Matrix Factorisation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "c:\\Users\\franc\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1477: FutureWarning: `alpha` was deprecated in version 1.0 and will be removed in 1.2. Use `alpha_W` and `alpha_H` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "people time right did good said say make way government\n",
      "Topic 1:\n",
      "window problem using server application screen display motif manager running\n",
      "Topic 2:\n",
      "god jesus bible christ faith believe christian christians sin church\n",
      "Topic 3:\n",
      "game team year games season players play hockey win league\n",
      "Topic 4:\n",
      "new 00 sale 10 price offer shipping condition 20 15\n",
      "Topic 5:\n",
      "thanks mail advance hi looking info help information address appreciated\n",
      "Topic 6:\n",
      "windows file files dos program version ftp ms directory running\n",
      "Topic 7:\n",
      "edu soon cs university ftp internet article email pub david\n",
      "Topic 8:\n",
      "key chip clipper encryption keys escrow government public algorithm nsa\n",
      "Topic 9:\n",
      "drive scsi drives hard disk ide floppy controller cd mac\n",
      "Topic 10:\n",
      "just ll thought tell oh little fine work wanted mean\n",
      "Topic 11:\n",
      "does know anybody mean work say doesn help exist program\n",
      "Topic 12:\n",
      "card video monitor cards drivers bus vga driver color memory\n",
      "Topic 13:\n",
      "like sounds looks look bike sound lot things really thing\n",
      "Topic 14:\n",
      "don know want let need doesn little sure sorry things\n",
      "Topic 15:\n",
      "car cars engine speed good bike driver road insurance fast\n",
      "Topic 16:\n",
      "ve got seen heard tried good recently times try couple\n",
      "Topic 17:\n",
      "use used using work available want software need image data\n",
      "Topic 18:\n",
      "think don lot try makes really pretty wasn bit david\n",
      "Topic 19:\n",
      "com list dave internet article sun hp email ibm phone\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, \n",
    "    min_df=2, \n",
    "    max_features=1_000, \n",
    "    stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 20\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd')\n",
    "nmf.fit(tfidf)\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "L'objectif de ce type de modÃ©lisation de sujets est de rÃ©cupÃ©rer de potentielles catÃ©gories pour des traitements ultÃ©rieurs. Cette modÃ©lisation offre surtout une meilleure comprÃ©hension de la structuration du texte en vue de crÃ©ation de features manuelles (mettre l'accent sur certains mots, comprendre ce qui dÃ©finit une catÃ©gorie, etc.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 OpÃ©rez une premiÃ¨re classification naÃ¯ve de sentiments\n",
    "\n",
    "VidÃ©o d'introduction : https://vimeo.com/284320448\n",
    "\n",
    "Dans cette partie, nous allons traiter le problÃ¨me de la classification supervisÃ©e, et voir comment utiliser les features que nous avons crÃ©Ã©es pour alimenter nos algorithmes dâ€™apprentissage, essentiellement dans le cadre de la catÃ©gorisation de texte.\n",
    "\n",
    "! Cette partie ne traitera pas le problÃ¨me de classification plus gÃ©nÃ©ral qui sâ€™applique Ã  plusieurs niveaux dans un texte : prÃ©dire quel sera le prochain mot si on considÃ¨re chaque mot comme une catÃ©gorie. Ou encore le POS tagging qui consiste Ã  considÃ©rer la catÃ©gorie grammaticale dâ€™un mot (verbe nom etc).\n",
    "\n",
    "? Mais attends, on a vraiment besoin dâ€™algorithme dâ€™apprentissage supervisÃ© pour catÃ©goriser du texte ? On ne peut pas juste assigner quelques mots-clÃ©s Ã  des catÃ©gories, et hop ?\n",
    "\n",
    "Vous faites rÃ©fÃ©rence aux Â« systÃ¨mes experts Â» (des rÃ¨gles de fonctionnement codÃ©es Ã  la main) qui ne sont malheureusement pas assez flexibles dans la plupart des cas qui nous intÃ©ressent et demandent beaucoup de maintenance, qui va en grandissant avec lâ€™Ã©chelle de nos donnÃ©es et le nombre de classes Ã  traiter. Cela reste cependant une alternative valable et robuste dans certains cas, Ã  ne pas nÃ©gliger automatiquement."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Petit rappel de la mÃ©thodologie\n",
    "\n",
    "La premiÃ¨re question Ã  se poser est : quâ€™est-ce que l'on veut accomplir ?\n",
    "\n",
    "Dans cette partie, nous allons nous concentrer sur le problÃ¨me de classification : **pouvoir assigner une catÃ©gorie Ã  un document texte fourni en entrÃ©e**. Par exemple, un type dâ€™actualitÃ© associÃ© Ã  un article, les catÃ©gories dâ€™une page WikipÃ©dia, le tag associÃ© Ã  un tweet, etc.\n",
    "\n",
    "ğŸ›ˆ Une application spÃ©cifique au langage est la dÃ©tection de sentiment/Ã©motion dâ€™un document. Cela peut Ãªtre binaire (positif ou nÃ©gatif), ou avec plus de nuances (triste, joyeux, en colÃ¨re, etc). Câ€™est une forme de classification qui est souvent demandÃ©e.\n",
    "\n",
    "Pour cela on va avoir besoin dâ€™un jeu de donnÃ©es dâ€™entraÃ®nement. Lâ€™idÃ©e, comme dâ€™habitude est quâ€™il soit Ã©quilibrÃ© par catÃ©gorie et relativement consistant. Cette premiÃ¨re Ã©tape est essentielle afin de sâ€™assurer du bon fonctionnement du reste des traitements.\n",
    "\n",
    "La seconde Ã©tape sera de faire passer nos donnÃ©es non-structurÃ©es de texte Ã  la moulinette que lâ€™on a vue dans la premiÃ¨re partie pour en sortir des features utilisables et structurÃ©es, par exemple sous forme vectorielle.\n",
    "\n",
    "Une fois notre jeu de donnÃ©e dâ€™entraÃ®nement et de test formatÃ©, on va pouvoir appliquer les mÃ©thodes classiques de classification (SVM, rÃ©seau de neurones, rÃ©gression logistique) sur ces donnÃ©es afin de rÃ©soudre la problÃ©matique Ã©noncÃ©e.\n",
    "\n",
    "Sâ€™en suit un rÃ©glage des hyperparamÃ¨tres par recherche de grille (ou autre mÃ©thodologie) pour amÃ©liorer le modÃ¨le en fonction de la mesure de performance choisie."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rappel sur les algorithmes de classification\n",
    "\n",
    "Dans ce chapitre, on va se concentrer sur la famille dâ€™algorithme de classification NaÃ¯ve Bayes et sur la  rÃ©gression logistique. Ces algorithmes permettent de catÃ©goriser le sentiment dâ€™un texte (positif ou nÃ©gatif), ou encore si un e-mail est un spam ou non.\n",
    "\n",
    "Pour rappel, lâ€™algorithme Naive Bayes (multinomial ou non) permet dâ€™effectuer des classifications probabilistes, qui assignent la probabilitÃ© dâ€™appartenance Ã  une classe.\n",
    "\n",
    "Câ€™est un type dâ€™algorithme gÃ©nÃ©ratif, oÃ¹ lâ€™on va modÃ©liser chaque classe et essayer de dÃ©terminer la probabilitÃ© lâ€™appartenance dâ€™une observation Ã  cette classe. A contrario, les algorithmes discriminatifs essaient de comprendre les caractÃ©ristiques diffÃ©renciantes entre les diffÃ©rentes classes possibles."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation du sentiment dâ€™un corpus de commentaires Amazon\n",
    "\n",
    "Comme terrain dâ€™Ã©tudes, nous allons utiliser le jeu de donnÃ©es de commentaire de produits Amazon. Lâ€™objectif sera de dÃ©terminer si un commentaire est positif ou pas.\n",
    "\n",
    "### Petit rappel de classification Naive Bayes\n",
    "\n",
    "Un peu plus formellement, le problÃ¨me de classification se traduit ainsi : \"trouver la classe 'c' qui a la probabilitÃ© la plus grande Ã©tant donnÃ© le document 'd' fourni\" :\n",
    "\n",
    "$$\\hat{c} = argmax_c p(c \\vert d)$$\n",
    "\n",
    "On ne sait pas calculer p(c|d) directement, on a donc besoin de simplifier un peu le problÃ¨me. Ainsi, dâ€™aprÃ¨s le thÃ©orÃ¨me de Bayes on a :\n",
    "\n",
    "$$p(c|d) = \\frac{p(d \\vert c)p(c)}{p(d)}$$\n",
    "\n",
    "Lâ€™objectif Ã©tant une maximisation sur la classe c, p(d) nâ€™influence pas le rÃ©sultat. Le problÃ¨me peut Ãªtre simplifiÃ© :\n",
    "\n",
    "$$\\hat{c} = argmax_c p(d \\vert c) p(c)$$\n",
    "\n",
    "Comme on lâ€™a vu dans les chapitres prÃ©cÃ©dents, le document 'd' est reprÃ©sentÃ© par un certain nombres de features (les mots quâ€™il contient), sans conserver lâ€™ordre de ces mots (bag-of-words) et en considÃ©rant quâ€™ils sont indÃ©pendant. On a ainsi pour un document de N mots $w_i$\n",
    "\n",
    "$$p(d \\vert c) = \\prod_{i=1}^N p(w_i \\vert c)$$\n",
    "\n",
    "Dans le cadre dâ€™Ã©tude de texte, nous travaillons sur des probabilitÃ©s faibles. Nous allons donc plutÃ´t travailler Ã  lâ€™Ã©chelle logarithmique, ce qui ne change rien au problÃ¨me de maximisation (la fonction log est monotone strictement croissante). Cela nous permet, en bonus, de travailler avec des sommes.\n",
    "\n",
    "$$\\hat{c} = argmax_c log ( p( d \\vert c) p(c) ) = argmax_c log p(c) + \\sum log( p(w_i \\vert c ) )$$\n",
    "\n",
    "Dans le cadre dâ€™une classification binaire de texte avec unigramme.\n",
    "\n",
    "La question restante est donc : comment estimer p(c) et $p(w_i \\vert c)$ Ã  partir de notre jeu de donnÃ©es dâ€™entraÃ®nement. Vous lâ€™aurez devinÃ©, on va utiliser des frÃ©quences :)\n",
    "\n",
    "La probabilitÃ© dâ€™une classe est simplement la frÃ©quence dâ€™apparition de la classe dans le jeu de donnÃ©es dâ€™entraÃ®nement :\n",
    "\n",
    "$$\n",
    "p(c) = \\frac{N_c}{N_{total doc}}\n",
    "$$\n",
    "\n",
    "Et la probabilitÃ© dâ€™un mot dans une classe est simplement : la frÃ©quence dâ€™apparition de ce mot dans un type de document par rapport au nombre de mot total dans c.\n",
    "\n",
    "$$\n",
    "p(w_i \\vert c) = \\frac{N_{w_i \\text{dans c}}}{ \\sum_V N_{w_t \\text{dans c}}}\n",
    "$$\n",
    "\n",
    "On lisse cette probabilitÃ© pour les mots qui n'apparaitraientt pas dans une classe, ce qui Ã©vite de rendre nulle notre fonction de vraisemblance si un mot est Ã  zÃ©ro prob (lissage Laplacien) :\n",
    "\n",
    "$$\n",
    "p(w_i \\vert c) = \\frac{N_{w_i \\in c} + 1}{ \\sum_V N_{w_t \\in c} + \\vert V \\vert }\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testons le modÃ¨le\n",
    "\n",
    "Nous allons utiliser [ce jeu de donnÃ©es](https://ai.stanford.edu/~amaas/data/sentiment/) uniquement pour le test de la dÃ©tection de sentiment. Avec NLTK, il suffit de charger les donnÃ©es dans un tableau labellisÃ© pour crÃ©er notre classifieur, avec chaque mot associÃ© Ã  un boolÃ©en confirmant son existence dans le document (en lâ€™occurence ici dans le commentaire).\n",
    "\n",
    "C'est parti ! Chargeons les donnÃ©es et formatons-les en bag-of-words associÃ© Ã  des boolÃ©ens :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m ap\n\u001b[0;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mformat_sentence\u001b[39m(sent):\n\u001b[0;32m      6\u001b[0m     \u001b[39mreturn\u001b[39;00m ({ word: \u001b[39mTrue\u001b[39;00m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m nltk\u001b[39m.\u001b[39mword_tokenize(sent\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)) })\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tools'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "from tools import ap  # THIS IS DEPRECATED MODULE. USE http://pypi.python.org/pypi/weblib MODULE INSTEAD.\n",
    "\n",
    "def format_sentence(sent):\n",
    "    return ({ word: True for word in nltk.word_tokenize(sent.decode('utf-8')) })\n",
    "\n",
    "def load_training_set():\n",
    "    training = []\n",
    "\n",
    "    for fp in os.listdir(ap('aclImdb/train/pos')):\n",
    "        example = '{}/{}'.format(ap('aclImdb/train/pos'), fp)\n",
    "        with open(example) as fp:\n",
    "            for i in fp:\n",
    "                training.append([format_sentence(i), 'pos'])\n",
    "\n",
    "    for fp in os.listdir(ap('aclImdb/train/neg')):\n",
    "        example = '{}/{}'.format(ap('aclImdb/train/neg'), fp)\n",
    "        with open(example) as fp:\n",
    "            for i in fp:\n",
    "                training.append([format_sentence(i), 'neg'])\n",
    "\n",
    "    return training\n",
    "\n",
    "training = load_training_set()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant entraÃ®ner notre classifieur, qui va utiliser les comptages expliquÃ©s plus haut pour crÃ©er le modÃ¨le probabiliste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je vous invite trÃ¨s fortement Ã  regarder le code d'implÃ©mentation du classifieur, voire dâ€™implÃ©menter le vÃ´tre pour comparer les performances et comprendre comment il fonctionne : http://www.nltk.org/_modules/nltk/classify/naivebayes.html \n",
    "\n",
    "Maintenant quâ€™on a entraÃ®nÃ© notre modÃ¨le, on peut par exemple observer les features les plus reprÃ©sentatives des classes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.show_most_informative_features(n=25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ca parait relativement rÃ©aliste sur les ratio - l'utilisation de *avoid* est trÃ¨s significative d'une review negative pour un ratio d'utilisation de 1 / 93.4 etc\n",
    "\n",
    "Et les performances de classification sur les donnÃ©es test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy(classifier, test))\n",
    "# 0.88754"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit qu'on arrive Ã  avoir une prÃ©cision de prÃ©diction de sentiments relativement intÃ©ressante (88.75% de prÃ©cision) pour un premier essai sur le jeu de donnÃ©es test. Pour aller plus loin dans l'amÃ©lioration des performances, il peut Ãªtre judicieux dâ€™effectuer une validation croisÃ©e ğŸ˜‰\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "L'application d'algorithmes de classification classiques fonctionne dÃ¨s lors que l'on sait quelles features utiliser Ã  partir de notre corpus de dÃ©part.\n",
    "\n",
    "D'autres Ã©tudiants confirment mon sentiment gÃ©nÃ©ral : ce cours est une catastrophe qui en dit long sur le contrÃ´le qualitÃ© chez OC : https://openclassrooms.com/forum/sujet/cours-analysez-vos-donnees-textuelles\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2  Allez plus loin dans la classification de mots\n",
    "\n",
    "L'utilisation de maniÃ¨re brute d'algorithmes de classification n'est souvent pas suffisante pour obtenir des performances optimales en vue des problÃ¨mes auxquels vous serez confrontÃ©s. A travers l'exemple de la classification de sentiments, on va voir qu'il est important d'effectuer des modifications prÃ©alables de son corpus et de ses features spÃ©cifiques Ã  nos problÃ¨mes.\n",
    "\n",
    "## ParticularitÃ©s de la classification de sentiments\n",
    "\n",
    "Comme la classification des sentiments dâ€™un texte est un problÃ¨me relativement classique de traitement du langage, il existe un certain nombre de techniques spÃ©cifiques qui permettent dâ€™obtenir des modÃ¨les plus performants sur cette tÃ¢che. Câ€™est une bonne illustration du type de manipulation qui permet dâ€™orienter son travail de modÃ©lisation en fonction de la problÃ©matique abordÃ©e.\n",
    "\n",
    "Par exemple, on ne va pas utiliser un *bag-of-words* avec comptage, mais simplement la **prÃ©sence ou pas dâ€™un mot** dans un document. En effet, que le mot â€œdÃ©sastreâ€ apparaisse une ou deux fois influence peu la probabilitÃ© que le document soit nÃ©gatif. C'est un critÃ¨re flag binaire.\n",
    "\n",
    "Voici une autre technique utilisÃ©e pour rÃ©gler **le problÃ¨me de la nÃ©gation**. Si je dis Â« je nâ€™ai pas aimÃ© ce film Â», la nÃ©gation â€œnâ€™â€ **inverse** le sentiment de ma phrase comparÃ© Ã  â€œaimerâ€ qui se retrouve davantage dans des documents positifs habituellement. Pour contrer ce problÃ¨me de maniÃ¨re simple, il suffit dâ€™ajouter un indicateur au mot suivant la nÃ©gation comme signifiant que son sens a Ã©tÃ© modifiÃ©.\n",
    "\n",
    "Ainsi, une phrase telle que â€œje nâ€™aime pas cette personneâ€ sera transformÃ©e en bag-of-words : { â€œjeâ€, â€œneâ€, â€œNON_aimeâ€, â€œpasâ€, â€œcetteâ€, â€œpersonneâ€ }. en ajoutant des prÃ©fixes du type \"NON_\", une bonne partie des problÃ¨mes sur ce genre dâ€™inversions de sens est ainsi rÃ©glÃ©e.\n",
    "\n",
    "Enfin, une technique de base est dâ€™utiliser un lexique de mots reprÃ©sentatifs de vos classes, en lâ€™occurrence ici positif et nÃ©gatif. Si on utilise simplement le nombre dâ€™apparitions de mots du lexique par classe bien construit pour entraÃ®ner un modÃ¨le de classification, on a dÃ©jÃ  une bonne baseline.\n",
    "\n",
    "Ainsi, toute ces petites modifications et ajouts permettent de rendre notre modÃ¨le plus spÃ©cialisÃ© pour notre problÃ©matique afin de donner de meilleures performances. Je vous encourage Ã  dÃ©velopper ce genre de tweaks lors de la crÃ©ation dâ€™un modÃ¨le pour un problÃ¨me donnÃ©.\n",
    "\n",
    "## Utiliser dâ€™autres types d'algorithmes de classification supervisÃ©e\n",
    "\n",
    "â“ On a utilisÃ© un modÃ¨le simple et robuste, le classifieur bayÃ©sien naÃ¯f. Pourquoi vouloir utiliser un autre type de classifieur ?\n",
    "\n",
    "Une hypothÃ¨se trÃ¨s forte qui est faite lors de lâ€™utilisation du classifieur Bayes est **lâ€™indÃ©pendance des features**. Ce qui signifie que si deux features sont en rÃ©alitÃ© corrÃ©lÃ©es, elles auront un effet plus fort que ce quâ€™elles apportent en rÃ©alitÃ© comme information pour la classification. Dâ€™autres types de modÃ¨les ne sont pas aussi sensibles Ã  cette corrÃ©lation entre les features, voire permettent de modÃ©liser cette interaction entre les features pour rendre le modÃ¨le plus performant. C'est une des diffÃ©rences possibles qui influenceront votre choix de classifieur. En gÃ©nÃ©ral, l'idÃ©e est de prÃ©-tester un certain nombre de classifieurs qui intuitivement correspondent Ã  votre problÃ¨me pour savoir sur lequel vous concentrer.\n",
    "\n",
    "### RÃ©gression Logistique\n",
    "\n",
    "La rÃ©gression logistique, Ã  lâ€™opposÃ© de la classification bayes, est un modÃ¨le de classification discriminant. Il est expliquÃ© [plus en dÃ©tail ici](https://openclassrooms.com/fr/courses/4444646-entrainez-un-modele-predictif-lineaire/4507831-predisez-lineairement-la-probabilite-de-l-appartenance-d-un-point-a-une-classe).\n",
    "\n",
    "Son objectif est de maximiser la probabilitÃ© dâ€™avoir une classe $y = c$ Ã©tant donnÃ© certaines features $f(x, c)$ calculÃ©es Ã  partir des observations $x$. Cette maximisation sâ€™effectue en gÃ©nÃ©ral avec les mÃ©thodes classiques de descente de gradient, avec un terme de rÃ©gularisation supplÃ©mentaire.\n",
    "\n",
    "On peut directement appliquer la rÃ©gression logistique sur des matrices creuses de la taille du vocabulaire ou sur les vecteurs plus denses que lâ€™on a crÃ©Ã© Ã  lâ€™aide des techniques utilisÃ©es dans les chapitres prÃ©cÃ©dents.\n",
    "\n",
    "### SVM, forÃªts alÃ©atoires\n",
    "\n",
    "Le choix dâ€™un classifieur est en fait un retour Ã  lâ€™Ã©ternel dilemme biais-variance, indÃ©pendamment du fait quâ€™on est amenÃ© Ã  traiter du texte. Le classifieur Naive Bayes possÃ¨de une variance faible et va pouvoir mieux gÃ©nÃ©raliser plus rapidement, ce qui peut Ãªtre utile lorsquâ€™on a un petit jeu de donnÃ©es ou des documents avec peu de texte. La contrepartie, bien sÃ»r, câ€™est que ce genre de classifieur va avoir une plus faible prÃ©cision (un biais plus grand) comparÃ© Ã  des classifieurs type SVM + RBF Kernel ou une rÃ©gression logistique.\n",
    "\n",
    "ğŸ“Œ Pour en savoir plus sur les avantages et inconvÃ©nients de diffÃ©rents classifieurs, notamment appliquÃ©es Ã  la classification de sentiments, vous pouvez consulter [cet article simple](https://aclanthology.org/P12-2018/) et efficace. Il s'agit d'un exemple de comparaison quâ€™il vous sera utile de consulter lors de votre travail prÃ©liminaire dâ€™exploration.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Dans un  premier temps, nous favorisions un travail expert sur les features avant de se concentrer sur l'utilisation d'un classifieur appropriÃ© Ã  la problÃ©matique en cours et aux hypothÃ¨ses de dÃ©part."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Traitez le corpus de textes Ã  l'aide de rÃ©seaux de neurones\n",
    "\n",
    "Dans ce chapitre, on va sâ€™intÃ©resser Ã  des architectures de rÃ©seaux de neurones assez utilisÃ©es dans le cadre de tÃ¢ches de traitement de langage. Ils vont permettre notamment de traiter nos problÃ¨mes sous forme de sÃ©rie temporelle (de sÃ©quences), ce qui semble adaptÃ© Ã  nos applications puisque le texte est une sÃ©quence de mots qui sont eux mÃªmes une sÃ©quence de lettres. ğŸš€\n",
    "\n",
    "ğŸ“Œ On traitait dÃ©jÃ  dans certains modÃ¨les lâ€™aspect sÃ©quentiel (par exemple en utilisant des n-grammes > 1) mais lâ€™hypothÃ¨se markovienne est trÃ¨s contraignante pour la rÃ©prÃ©sentativitÃ© du modÃ¨le puisquâ€™on â€œoublieâ€ de prendre en compte les Ã©lÃ©ments prÃ©cÃ©dents les Ã©lÃ©ments conditionnÃ©s.\n",
    "\n",
    "âš¡ Attention Ã  ne pas plonger dans ces architectures sans tester au prÃ©alable des solutions qui sont en apparence plus simple mais beaucoup plus efficace (en terme de temps de calcul notamment) pour la majoritÃ© des problÃ©matiques que vous rencontrerez. En effet, les dÃ©butants ont tendance Ã  tout de suite vouloir se diriger vers ces mÃ©thodes qui sont certes extrÃªmement performantes mais sont difficiles Ã  structurer, entraÃ®ner et relativement lente par rapport Ã  des mÃ©thodes plus simple qui offrent des performances similaires. L'idÃ©e est au moins d'avoir une baseline sur des classifieurs classiques avant de tester des architectures plus complexes !\n",
    "\n",
    "â—Ce chapitre a vocation Ã  Ãªtre une introduction aux (relativement) nouvelles architectures de rÃ©seaux de neurones et les mÃ©thodes associÃ©es. Cependant, je vous invite Ã  vous documenter de maniÃ¨re spÃ©cifique sur votre problÃ©matique afin de pouvoir crÃ©er votre modÃ¨le. Je prÃ©sente ici les bases afin que vous puissiez vous dÃ©brouiller dans le champ des possibles et pouvoir aborder les ressources avec le bagage nÃ©cessaire.\n",
    "\n",
    "## ReprÃ©sentation des features\n",
    "\n",
    "On retrouve notamment les plongements (*embeddings*) en dimension infÃ©rieure word2vec (ou autre) comme entrÃ©e pour les rÃ©seaux de neurones. Ces manipulations prÃ©alables ne sont pas inutiles ! Il faut notamment prendre en compte les capacitÃ©s de calcul et le niveau de reprÃ©sentativitÃ© nÃ©cessaire dans votre application afin de dÃ©terminer la dimension de lâ€™espace dâ€™embedding, par exemple (qui peut varier grosso modo entre 20 et 300 dimensions).\n",
    "\n",
    "## DiffÃ©rentes architectures possibles\n",
    "\n",
    "### Lâ€™architecture de base et ses problÃ¨mes : le RNN\n",
    "\n",
    "Les RNN reprÃ©sentent la famille de rÃ©seau de neurones qui traite les donnÃ©es de maniÃ¨re sÃ©quentielles. Lâ€™idÃ©e principale Ã©tant que chaque nouveau mot qui est prÃ©dit Ã  partir de notre modÃ¨le, prend en compte lâ€™Ã©tat prÃ©cÃ©dent afin de sâ€™actualiser. Lâ€™Ã©tat reprÃ©sente fondamentalement lâ€™historique - la mÃ©moire utilisÃ©e dans le rÃ©seau de neurones pour prendre en compte le passÃ© (TOUT le passÃ©) afin de lâ€™utiliser dans la prÃ©diction Ã  lâ€™instant $t$.\n",
    "\n",
    "<figure id=\"r-4881228\" data-claire-element-id=\"30015822\"><img id=\"r-4881226\" data-claire-element-id=\"9015461\" src=\"https://user.oc-static.com/upload/2017/12/13/15131655946094_RNN-rolled.png\" alt=\"L'architecture du RNN\"   style=\"background-color:White;\"/><figcaption>L'architecture du RNN</figcaption></figure>\n",
    "\n",
    "Dans le cas de tÃ¢ches associÃ©es au texte, on essaie par exemple Ã  chaque Ã©tape de prÃ©dire le mot en fonction des mots prÃ©cÃ©dents ainsi quâ€™une observation. Par exemple, le mot dans une langue Ã©trangÃ¨re dans le cadre dâ€™une traduction.\n",
    "\n",
    "On a donc une sÃ©rie temporelle qui Ã  chaque instant t, va prendre en entrÃ©e une observation $o_t$ et lâ€™Ã©tat prÃ©cÃ©dent $x_{tâˆ’1}$ pour Ãªtre calculÃ© :\n",
    "\n",
    "$$\n",
    "x_t = W_{rec} \\sigma(x_{t-1}) + W_{in} o_t + b\n",
    "$$\n",
    "\n",
    "<img id=\"r-4881247\" data-claire-element-id=\"9015305\" src=\"https://lh5.googleusercontent.com/7U_SQzt7w9KJ3HbWEarKSZybYjOPjhnQkRKT2KPX8Umeb_pkYzkDhoWsNVDghBDCZuHImE4HNDjoqKiZggPfGpOayoyo-L1MzJada678lYdH2eGGrrBktTsjOm5e1LMbTcF1-OR9\" alt=\"\"   style=\"background-color:White;\"/>\n",
    "\n",
    "Deux problÃ¨mes principaux font suite Ã  cette architecture fondamentale : la **disparition** du gradient, et son **explosion**. En effet lors de la backpropagation, on doit calculer un gradient (en particulier $x_t/x$) qui est sujet Ã  disparition/explosion. Cette disparition notamment a pour effet de faire â€œoublierâ€ au RNN des informations qui pourraient pourtant Ãªtre utiles au traitement actuel. Si jâ€™ai un texte dans lequel une information primordiale est prÃ©sente au dÃ©but, il est trÃ¨s difficile de crÃ©er un RNN qui prenne en compte cette information sur la fin du texte. \n",
    "\n",
    "En pratique, on voit que le RNN ne peut prendre en compte que trÃ¨s peu de contexte passÃ© Ã  cause de cette disparition de gradient.\n",
    "\n",
    "Il existe une grande variÃ©tÃ© de techniques qui sont utilisÃ©s pour mitiger ce problÃ¨me. Notamment en premier lieu, une architecture plus robuste qui a Ã©tÃ© imaginÃ©e quelques annÃ©es aprÃ¨s lâ€™apparition des RNN appelÃ©e **LSTM (Long Short Term Memory)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Networks (Long Short Term Memory Networks)\n",
    "\n",
    "Lâ€™idÃ©e principale des LSTM est de permettre au rÃ©seau â€œdâ€™oublierâ€ ou de ne pas prendre en compte certaines observations passÃ©es afin de pouvoir donner du poids aux informations importantes dans la prÃ©diction actuelle.\n",
    "\n",
    "Cette idÃ©e se traduit par des â€œgatesâ€ qui sont chargÃ©es de dÃ©terminer lâ€™importance dâ€™une entrÃ©e, afin de savoir si on enregistre lâ€™information qui en sort ou pas. Pour une explication dÃ©taillÃ©e intuitive, rendez-vous sur [cet article de blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) qui explique les diffÃ©rentes Ã©tapes. En pratique la seule partie qui change vraiment est lors de la mise Ã  jour des rÃ©seaux de neurones qui est un peu plus technique.\n",
    "\n",
    "**Note** L'article de blog est la source des images du cours.\n",
    "\n",
    "<figure id=\"r-4881324\" data-claire-element-id=\"30015825\"><img id=\"r-4881322\" data-claire-element-id=\"9015494\" src=\"https://lh6.googleusercontent.com/iyYgSSiBUHfIPkjSmEzw70VRzy7ru77mXhOuXRhwqAe6xKLLu45P5F4M_PaEVuKaXiCX739uEvz_qTozgYZB73GlTsdRiDns-6NxqioTRrIY3DziFyMMA3rCutWMZpHwi6JQdBV6\" alt=\"Une cellule LSTM\"  style=\"background-color:White;\"/><figcaption>Une cellule LSTM </figcaption></figure>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etat de lâ€™art des rÃ©seaux de neurones en traitement du langage\n",
    "\n",
    "Lâ€™utilisation des rÃ©seaux de neurones dans les applications de traitement de langage reprÃ©sente encore un problÃ¨me ouvert dans la recherche, qui demande Ã  Ãªtre traitÃ© plus en profondeur, notamment sur les problÃ©matiques spÃ©cifique Ã  des tÃ¢ches (quelle architecture correspondent Ã  quelles tÃ¢ches), des optimisations plus globales et les technique de transfert dâ€™apprentissage afin de pouvoir utiliser des rÃ©seaux dÃ©jÃ  entraÃ®nÃ©s sur de nouveaux problÃ¨mes plus facilement.\n",
    "\n",
    "A titre illustratif, je voudrais citer deux mÃ©canismes qui attirent lâ€™attention depuis quelques temps.\n",
    "\n",
    "### Les rÃ©seaux de neurones rÃ©siduels et autre augmentation de densitÃ©\n",
    "\n",
    "Les rÃ©seaux de neurones rÃ©siduels sont trÃ¨s utilisÃ©s, Ã  la base pour le traitement dâ€™images mais aussi maintenant dans le traitement du texte. Ils permettent aussi de mitiger lâ€™effet de diminution trop brutal du gradient. Pour cela simplement, on ajoute directement lâ€™identitÃ© de lâ€™Ã©tat prÃ©cÃ©dent dans la fonction de calcul. Cela a pour effet de diminuer ce problÃ¨me de disparition du gradient.\n",
    "\n",
    "<img id=\"r-4881343\" data-claire-element-id=\"9015568\" src=\"https://lh5.googleusercontent.com/1QSJNWWNxC8acis3ynDXs3cLJpe-81MMOlVR3dvtM6_sjo_OK1BwoFJXCejx63ThjcjEQyM34yQxTWhjCubNf5y7Jkxe79Zh_IFLLNhkgDw3JZfVHHBdOVZxbXer0bvyM5poko6h\" alt=\"\"   style=\"background-color:White;\"/>\n",
    "\n",
    "Dâ€™autres procÃ©dÃ©s approfondissent cette idÃ©e comme par exemple les [highway networks](https://arxiv.org/pdf/1505.00387.pdf) qui augmentent les fonctions de maniÃ¨re diffÃ©rente mais servent ce mÃªme objectif. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les techniques dâ€™attention\n",
    "\n",
    "Lâ€™idÃ©e principale des techniques dâ€™attention vise Ã  se concentrer sur une partie de lâ€™information en particulier qui est fournie au rÃ©seau de neurones. Ca peut Ãªtre un mot Ã  traduire au milieu dâ€™une phrase, une partie dâ€™une image lors de la description de celle-ci, etc. Il s'agit donc dâ€™entraÃ®ner un rÃ©seau de neurones aussi sur un score dâ€™importance Ã  donner Ã  lâ€™ensemble de lâ€™information qui lui est prÃ©sentÃ©e. Ces techniques sont trÃ¨s prometteuses en terme de performance de prÃ©diction.\n",
    "\n",
    "<img id=\"r-4881363\" data-claire-element-id=\"9015660\" src=\"https://lh6.googleusercontent.com/HYNAC-98gAJs0FysDiUtmAOG-LGsphynk5Lc3vmGGAjStf_hOp9vPcwfLKnYz3Dw4nuZ1nySf9PUOkNzDy25cTkvoCMipteo7vEHRRELG10DXLoYl5JVd6BnsBXxvK-0Ex1fPL_k\" alt=\"\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications\n",
    "\n",
    "Avec ces architectures, on peut avoir une meilleure reprÃ©sentation dâ€™un corpus de texte puisquâ€™on prend en compte son caractÃ¨re sÃ©quentiel. Voici quelques domaines dâ€™application dans lesquels ce genre dâ€™architecture a Ã©tÃ© particuliÃ¨rement performant :\n",
    "* Classification de texte\n",
    "* POS Tagging et NER\n",
    "* Speech to text\n",
    "* Traduction automatique\n",
    "* GÃ©nÃ©ration automatique de texte et notamment de lÃ©gendes dâ€™images\n",
    "\n",
    "Ce quâ€™il faut garder en tÃªte, câ€™est que les rÃ©seaux de neurones rÃ©current bien entraÃ®nÃ©s permettent dâ€™intÃ©grer la maniÃ¨re dont est structurÃ© un langage (ou un sous-ensemble du langage) ou toute sÃ©quence de texte. Le fait dâ€™avoir ce modÃ¨le permet une infinitÃ© dâ€™applications liÃ©e aux texte, quâ€™il faut paramÃ©trer et personnaliser pour votre problÃ¨me spÃ©cifique.\n",
    "\n",
    "â—Encore une fois, attention Ã  ne pas  utiliser un canon pour tuer une mouche, et Ãªtre bien sÃ»r que toutes les circonstances (taille des donnÃ©es dâ€™entraÃ®nement etc) sont prÃ©sentes pour lâ€™utilisation dâ€™un rÃ©seau de neurones efficace."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Une petite application illustrative : retour aux chanteurs de rap\n",
    "\n",
    "Afin de dÃ©montrer lâ€™efficacitÃ© de ces modÃ¨les rÃ©currents, essayons de l'appliquer Ã  l'Ã©chelle des caractÃ¨res dâ€™une phrase afin de gÃ©nÃ©rer des paroles de rap, dans le style dâ€™un rappeur. Pour cela, on va rÃ©cupÃ©rer [lâ€™implÃ©mentation du LSTM de Andrej Karpathy](https://github.com/karpathy/char-rnn).\n",
    "\n",
    "Si on entraÃ®ne sur un corpus de texte particulier (ici le rappeur Nekfeu), voici le texte gÃ©nÃ©rÃ© au bout de quelques milliers d'itÃ©rations :\n",
    "\n",
    "    Paclois ce b'est un boutes qu'on heÃªtembarais j'on sa c'est, sout je julam\n",
    "    DÃ©vans\n",
    "    J'etfu l'Ã©-travy pei fais se Ã  jÃ  aura\n",
    "    Qu'le garl\n",
    "    La quincer gÃ© veull'\n",
    "    Atrait\n",
    "    tu vow mon avet peu san enfiiment aure maisas\n",
    "    Et quand j'ta rag, s'lerr\n",
    "    Dâ€™est pomps\n",
    "    Fourde bes des quand tout samiectieu ryais\n",
    "    L'heux\n",
    "    Lans l'ai Ã  le gropas, caquis\n",
    "    Je Dacie, Ã  nan poustier juk quâ€™retape ys qurises suis des conpes Ã©moy ne da tiendir\n",
    "    Plassine grissoble\n",
    "    J'es it-di\n",
    "    J'suis d'acinon puant fet-pes c'tui t'baq\n",
    "\n",
    "On peut observer notamment la propension Ã  comprendre la longueur dâ€™un vers ou mÃªme la taille d'un mot, ce qui montre que le RNN retient que le vers a commencÃ© quelque part avec une majuscule et doit finir par un retour Ã  la ligne. De mÃªme, il a compris environ la taille d'un mot et ajoute des espaces dans les bonnes proportions. Il y a donc une forme de mÃ©morisation du contexte passÃ©. On voit aussi des virgules, apostrophes ou des tirets en bonne proportions. L'alternance consoles-voyelles aussi est respectÃ©e ainsi que certaines combinaisons caractÃ©ristiques ('tt', 'eu', 'es', etc). Quelques mots commencent Ã  Ãªtre rÃ©alistes, mais il n'y a pas assez de donnÃ©es sur le corpus (<250k caractÃ¨re)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "On a vu ici le type de modÃ©lisation possible Ã  l'aide des rÃ©seaux de neurones. L'idÃ©al est pour vous d'entraÃ®ner votre propre rÃ©seau de neurones en vu d'une des tÃ¢ches applicatives citÃ©e plus haut - classification, POS tagging, traduction automatique etc afin de vous rendre compte de la mÃ©thodologie d'entraÃ®nement sur ce type de problÃ©matique. L'entraÃ®nement de rÃ©seaux de neurones reste pour le moment un processus relativement intuitif avec beaucoup d'heuristiques et de tests nÃ©cessaires avant d'obtenir des rÃ©sultats probants et stables, notamment pour dÃ©terminer la valeur des hyperparamÃ¨tres (nombre & taille des calques, learning rate, algorithme de gradient utilisÃ©, etc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 EntraÃ®nez-vous Ã  classifier du texte\n",
    "\n",
    "## Ã€ vous de jouer\n",
    "\n",
    "â—Pour vous entraÃ®ner, rÃ©alisez cet exercice Ã©tape par Ã©tape. Une fois terminÃ©, vous pouvez comparer votre travail avec les pistes que je vous propose.\n",
    "\n",
    "Vous utiliserez les donnÃ©es Â« Reuters Corpus Volume Â» accessible directement dans scikit learn Ã  lâ€™aide de la fonction `fetch_rcv1` qui contient 800,000 annonces de presses Reuters Ã©tiquetÃ©es manuellement. Votre objectif est de rÃ©aliser un benchmark de diffÃ©rents types de classifieurs afin de comparer les diffÃ©rentes performances sur ce type de problÃ¨me.\n",
    "\n",
    "### Consigne\n",
    "\n",
    "Vous devez rÃ©aliser les tÃ¢ches suivantes :\n",
    "* Charger les donnÃ©es\n",
    "* CrÃ©er diffÃ©rents classifieurs (au moins 3)\n",
    "* Effectuer une validation croisÃ©e sur les diffÃ©rents classifieurs\n",
    "* Afficher les diffÃ©rentes performances\n",
    "\n",
    "Le jeu de donnÃ©es est relativement lourd pour un travail en local, avec 650MB compressÃ© de donnÃ©es. Il est conseillÃ© de travailler sur un Ã©chantillon dans un premier temps pour sâ€™assurer que tout fonctionne comme prÃ©vu pour ensuite traiter tout le jeu de donnÃ©es et obtenir les rÃ©sultats finaux.\n",
    "\n",
    "VÃ©rifiez-bien que vous avez les Ã©lÃ©ments suivants :\n",
    "* Au moins 3 classifieurs diffÃ©rents ont Ã©tÃ© appliquÃ©s par validations croisÃ©es sur les donnÃ©es correctement, puis les performances ont Ã©tÃ© Ã©valuÃ©es sur chacun."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9aff9e50adfaa9e30c910fb3872ffdc72747acb5f50803ca0504f00e980f7c25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

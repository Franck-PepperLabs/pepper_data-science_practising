{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5. [**Décomposer des signaux en composantes (problèmes de factorisation matricielle)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_5_decomposition.ipynb)</br>([*Decomposing signals in components (matrix factorization problems)*](https://scikit-learn.org/stable/modules/decomposition.html))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='principal-component-analysis-pca'></a> 2.5.1 Analyse en composantes principales (ACP) (*Principal component analysis (PCA)*)\n",
    "\n",
    "## <a id='exact-pca-and-probabilistic-interpretation'></a> 2.5.1.1. ACP exacte et interprétation probabilistique\n",
    "\n",
    "[PCA]: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "L'analyse en composantes principales (ACP) est utilisée pour décomposer un jeu de données multivarié en un ensemble de composantes orthogonales successives qui expliquent un maximum de variance. Dans scikit-learn, la [**`PCA`**][PCA] est implémentée sous forme d'objet *transformeur* qui apprend $n$ composantes dans sa méthode `fit` d'ajustement, et peut être utilisée sur de nouvelles données pour les projeter sur ces composantes.\n",
    "\n",
    "L'ACP centre les données d'entrée, mais ne les met pas à l'échelle pour chaque caractéristique avant d'appliquer la SVD. Le paramètre optionnel `whiten=True` permet de projeter les données sur l'espace singulier tout en mettant à l'échelle chaque composante à la variance unitaire. Cela est souvent utile si les modèles en aval font des hypothèses fortes sur l'isotropie du signal : c'est par exemple le cas pour les machines à vecteurs de support avec le noyau RBF et l'algorithme de clustering K-Means.\n",
    "\n",
    "Ci-dessous, un exemple de l'ensemble de données iris, qui comprend 4 caractéristiques, projeté sur les 2 dimensions qui expliquent la plus grande variance :\n",
    "\n",
    "<figure class=\"align-center\">\n",
    "<img alt=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_pca_vs_lda_001.png\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_pca_vs_lda_001.png\" style=\"width: 480.0px; height: 360.0px;\" />\n",
    "</figure>\n",
    "\n",
    "L'objet [**`PCA`**][PCA] fournit également une interprétation probabiliste de l'ACP qui peut donner une vraissemblance des données en fonction de la quantité de variance qu'elle explique. En tant que tel, il implémente une méthode de [score](https://scikit-learn.org/stable/glossary.html#term-score) qui peut être utilisée dans la validation croisée :\n",
    "\n",
    "<figure class=\"align-center\">\n",
    "<img alt=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_pca_vs_fa_model_selection_001.png\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_pca_vs_fa_model_selection_001.png\" style=\"width: 480.0px; height: 360.0px;\" />\n",
    "</figure>\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Comparaison des projections 2D LDA et PCA de l'ensemble de données Iris**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_5_decomposition/plot_pca_vs_lda.ipynb)<br/>([*Comparison of LDA and PCA 2D projection of Iris dataset*](https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_lda.html))\n",
    "\n",
    "#### [**Sélection du modèle avec PCA probabiliste et analyse factorielle (FA)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_5_decomposition/plot_pca_vs_fa_model_selection.ipynb)<br/>([*Model selection with Probabilistic PCA and Factor Analysis (FA)*](https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_fa_model_selection.html))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='incremental-pca'></a> 2.5.1.2 Analyse en Composantes Principales Incrémentale\n",
    "\n",
    "[PCA]: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "[IncrementalPCA]: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html\n",
    "\n",
    "L'objet [**`PCA`**][PCA] est très utile, mais présente certaines limitations pour les grandes ensembles de données. La plus grande limitation est que [**`PCA`**][PCA] ne prend en charge que le traitement par lots, ce qui signifie que toutes les données à traiter doivent tenir dans la mémoire principale. L'objet [**`IncrementalPCA`**][IncrementalPCA] utilise une forme de traitement différente et permet des calculs partiels qui correspondent presque exactement aux résultats de [**`PCA`**][PCA] tout en traitant les données en mini-lots. [**`IncrementalPCA`**][IncrementalPCA] rend possible la mise en œuvre de l'analyse en composantes principales en dehors de la mémoire centrale soit en:\n",
    "\n",
    "* utilisant sa méthode `partial_fit` sur des blocs de données récupérés séquentiellement depuis le disque dur local ou une base de données réseau.\n",
    "* en appelant sa méthode `fit` sur une matrice creuse ou un fichier mémoire mappé en utilisant `numpy.memmap`.\n",
    "\n",
    "[**`IncrementalPCA`**][IncrementalPCA] ne stocke que les estimations des variances des composantes et du bruit, pour mettre à jour de manière incrémentale `explained_variance_ratio_`. C'est pourquoi l'utilisation de la mémoire dépend du nombre d'échantillons par lot, plutôt que du nombre d'échantillons à traiter dans l'ensemble de données.\n",
    "\n",
    "Comme dans [**`PCA`**][PCA], [**`IncrementalPCA`**][IncrementalPCA] centre mais n'échelle pas les données d'entrée pour chaque caractéristique avant d'appliquer la SVD.\n",
    "\n",
    "<figure class=\"align-center\">\n",
    "<img alt=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_incremental_pca_001.png\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_incremental_pca_001.png\" style=\"width: 600.0px; height: 600.0px;\" />\n",
    "</figure>\n",
    "<figure class=\"align-center\">\n",
    "<img alt=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_incremental_pca_002.png\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_incremental_pca_002.png\" style=\"width: 600.0px; height: 600.0px;\" />\n",
    "</figure>\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**ACP incrémentale**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_5_decomposition/plot_incremental_pca.ipynb)<br/>([*Incremental PCA*](https://scikit-learn.org/stable/auto_examples/decomposition/plot_incremental_pca.html))\n",
    "\n",
    "\n",
    "#### [**Décompositions de jeux de données de visages**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_5_decomposition/plot_faces_decomposition.ipynb)<br/>([*Faces dataset decompositions*](https://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='pca-using-randomized-svd'></a>  2.5.1.3 Analyse en Composantes Principales utilisant SVD aléatoire\n",
    "\n",
    "[PCA]: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "Il est souvent intéressant de projeter les données sur un espace de dimension inférieure qui préserve l'essentiel de la variance, en éliminant les vecteurs singuliers des composantes associées aux valeurs singulières inférieures.\n",
    "\n",
    "Par exemple, si nous travaillons avec des images en niveaux de gris de 64x64 pixels pour la reconnaissance faciale, la dimensionnalité des données est de 4096 et il est lent d'entraîner une machine à vecteurs de support RBF sur des données aussi larges. De plus, nous savons que la dimensionnalité intrinsèque des données est beaucoup plus faible que 4096, car toutes les images de visages humains se ressemblent d'une certaine manière. Les échantillons se trouvent sur une variété de dimension beaucoup plus faible (disons environ 200 par exemple). L'algorithme PCA peut être utilisé pour transformer linéairement les données tout en réduisant la dimensionnalité et en préservant la plupart de la variance expliquée en même temps.\n",
    "\n",
    "La classe [**`PCA`**][PCA] utilisée avec le paramètre optionnel `svd_solver='randomized'` est très utile dans ce cas: étant donné que nous allons éliminer la plupart des vecteurs singuliers, il est beaucoup plus efficace de limiter le calcul à une estimation approchée des vecteurs singuliers que nous allons conserver pour effectuer réellement la transformation.\n",
    "\n",
    "Par exemple, l'image suivante montre 16 portraits d'échantillons (centrés autour de 0,0) provenant de la base de données Olivetti. A droite se trouvent les 16 premiers vecteurs singuliers réorganisés en portraits. Comme nous ne nécessitons que les 16 premiers vecteurs singuliers d'un ensemble de données de taille $n_{\\text{samples}}=400$ et $n_{\\text{features}}=64 \\times 64 = 4096$, le temps de calcul est inférieur à 1s\n",
    "\n",
    "<img alt=\"orig_img\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_001.png\" style=\"width: 360.0px; height: 275.4px;\" />\n",
    "\n",
    "<img alt=\"pca_img\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_002.png\" style=\"width: 360.0px; height: 275.4px;\" />\n",
    "\n",
    "Si nous notons $n_{\\max} = \\max(n_{\\mathrm{samples}}, n_{\\mathrm{features}})$ et $n_{\\min} = \\min(n_{\\mathrm{samples}}, n_{\\mathrm{features}})$, la complexité temporelle de la [**`PCA`**][PCA] aléatoire est $O(n_{\\max}^2 \\cdot n_{\\mathrm{components}})$ au lieu de $O(n_{\\max}^2 \\cdot n_{\\min})$ pour la méthode exacte implémentée dans [**`PCA`**][PCA].\n",
    "\n",
    "La empreinte mémoire de la [**`PCA`**][PCA] aléatoire est également proportionnelle à $2 \\cdot n_{\\max} \\cdot n_{\\mathrm{components}}$ au lieu de $n_{\\max} \\cdot n_{\\min}$ pour la méthode exacte.\n",
    "\n",
    "Note: l'implémentation de `inverse_transform` dans [**`PCA`**][PCA] avec `svd_solver='randomized'` n'est pas la transformation inverse exacte de `transform` même lorsque `whiten=False` (par défaut).\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Exemple de reconnaissance faciale utilisant des eigenfaces et des SVMs**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/applications/plot_face_recognition.ipynb)<br/>([*Faces recognition example using eigenfaces and SVMs*](https://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html))\n",
    "    \n",
    "#### [**Décompositions de jeux de données de visages**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_5_decomposition/plot_faces_decomposition.ipynb)<br/>([*Faces dataset decompositions*](https://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html))\n",
    "\n",
    "### Références\n",
    "\n",
    "Algorithm 4.3 in [“**Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions**](https://arxiv.org/abs/0909.4061)[”](https://drive.google.com/file/d/1O9-E0VIScKwDGpK-TMI9C--xscgOfJMY/view?usp=share_link) Halko, et al., 2009\n",
    "\n",
    "[“**An implementation of a randomized algorithm for principal component analysis**](https://arxiv.org/abs/1412.3510)[”](https://drive.google.com/file/d/133PgG41En86q1mYcFUfvMj8DK1wCT-0k/view?usp=share_link) A. Szlam et al. 2014"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <a id='pca-using-randomized-svd'></a> 2.5.1.4. Sparse principal components analysis (SparsePCA et MiniBatchSparsePCA)\n",
    "\n",
    "[PCA]: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "[SparsePCA]: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.SparsePCA.html\n",
    "[MiniBatchSparsePCA]: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.MiniBatchSparsePCA.html\n",
    "\n",
    "[**`SparcePCA`**][SparcePCA] est une variante de l'analyse en composantes principales (PCA), qui a pour objectif d'extraire l'ensemble des composantes creuses qui reconstituent le mieux les données.\n",
    "\n",
    "Mini-batch sparse PCA ([**`MiniBatchSparsePCA`**][MiniBatchSparsePCA]) est une variante de [**`SparcePCA`**][SparcePCA] qui est plus rapide mais moins précise. La vitesse accrue est obtenue en parcourant des petits morceaux de l'ensemble des caractéristiques, pour un certain nombre d'itérations.\n",
    "\n",
    "L'analyse en composantes principales ([**`PCA`**][PCA]) a l'inconvénient que les composants extraits par cette méthode ont des expressions uniquement denses, c'est-à-dire qu'ils ont des coefficients non nuls lorsqu'ils sont exprimés sous forme de combinaisons linéaires des variables originales. Cela peut rendre l'interprétation difficile. Dans de nombreux cas, les véritables composants sous-jacents peuvent être plus facilement imaginés comme des vecteurs creux; par exemple, dans la reconnaissance de visages, les composants peuvent naturellement être liés à des parties du visage.\n",
    "\n",
    "Les composantes principales creuses donnent une représentation plus économe et plus interprétable, mettant clairement en évidence les caractéristiques originales qui contribuent aux différences entre les échantillons.\n",
    "\n",
    "L'exemple suivant illustre 16 composants extraits à l'aide de l'analyse en composantes principales creuses (sparse PCA) à partir de l'ensemble de données de visages Olivetti. On peut voir comment le terme de régularisation introduit de nombreux zéros. De plus, la structure naturelle des données fait que les coefficients non nuls sont adjacents verticalement. Le modèle n'impose pas cela mathématiquement : chaque composant est un vecteur $h \\in \\mathbf{R}^{4096}$, et il n'y a pas de notion d'adjacence verticale sauf pendant la visualisation conviviale pour l'homme en tant qu'images de pixels 64x64. Le fait que les composants ci-dessous apparaissent localement est l'effet de la structure inhérente des données, ce qui fait que de tels motifs locaux minimisent l'erreur de reconstruction. Il existe des normes induisant la sparsité qui tiennent compte de l'adjacence et de différents types de structures; voir [Jen09] pour un examen de ces méthodes. Pour plus de détails sur l'utilisation de l'analyse en composantes principales creuses, voir la section Exemples ci-dessous.\n",
    "\n",
    "<img alt=\"pca_img\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_002.png\" style=\"width: 360.0px; height: 275.4px;\" />\n",
    "\n",
    "<img alt=\"spca_img\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_005.png\" style=\"width: 360.0px; height: 275.4px;\" />\n",
    "\n",
    "Notez qu'il existe de nombreuses formulations différentes pour le problème de l'analyse en composantes principales creuses. Celle qui est implémentée ici est basée sur [Mrl09]. Le problème d'optimisation résolu est un problème d'analyse en composantes principales (apprentissage de dictionnaire) avec une pénalisation $\\ell_1$ sur les composantes :\n",
    "\n",
    "$$\n",
    "\\begin{split}(U^*, V^*) =\n",
    "    \\underset{U, V}{\\operatorname{arg\\,min\\,}} & \\frac{1}{2}\n",
    "    ||X-UV||_{\\text{Fro}}^2+\\alpha||V||_{1,1} \\\\\n",
    "    \\text{subject to } & ||U_k||_2 \\le 1 \\text{ for all }\n",
    "    0 \\leq k < n_{components}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "où $||.||_{\\text{Fro}}$ représente la norme de Frobenius et $||.||_{1,1}$ représente la norme matricielle par entrée qui est la somme des valeurs absolues de toutes les entrées dans la matrice. La norme matricielle $||.||_{1,1}$  inductrice d'éparsité empêche également l'apprentissage de composantes à partir de bruit lorsque peu d'échantillons d'entraînement sont disponibles. Le degré de pénalisation (et donc de sparsité) peut être ajusté à travers l'hyperparamètre `alpha`. De petites valeurs conduisent à une factorisation doucement régularisée, tandis que des valeurs plus grandes réduisent beaucoup de coefficients à zéro.\n",
    "\n",
    "**Note** : Bien que dans l'esprit d'un algorithme en ligne, la classe [**`MiniBatchSparsePCA`**][MiniBatchSparsePCA] ne met pas en oeuvre `partial_fit` car l'algorithme est en ligne le long de la direction des caractéristiques, pas le long de la direction des échantillons."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e03b612d84ba21ce95ed447e81b3062e1eb99b56c6d885cdab4aaa12f1b8e240"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

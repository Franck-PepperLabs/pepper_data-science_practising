{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5. [**Décomposer des signaux en composantes (problèmes de factorisation matricielle)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_5_decomposition.ipynb)</br>([*Decomposing signals in components (matrix factorization problems)*](https://scikit-learn.org/stable/modules/decomposition.html))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='principal-component-analysis-pca'></a> 2.5.1 Analyse en composantes principales (ACP)\n",
    "\n",
    "## <a id='exact-pca-and-probabilistic-interpretation'></a> 2.5.1.1. ACP exacte et interprétation probabilistique\n",
    "\n",
    "[PCA]: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "L'analyse en composantes principales (ACP) est utilisée pour décomposer un jeu de données multivarié en un ensemble de composantes orthogonales successives qui expliquent un maximum de variance. Dans scikit-learn, la [**`PCA`**][PCA] est implémentée sous forme d'objet *transformeur* qui apprend $n$ composantes dans sa méthode `fit` d'ajustement, et peut être utilisée sur de nouvelles données pour les projeter sur ces composantes.\n",
    "\n",
    "L'ACP centre les données d'entrée, mais ne les met pas à l'échelle pour chaque caractéristique avant d'appliquer la SVD. Le paramètre optionnel `whiten=True` permet de projeter les données sur l'espace singulier tout en mettant à l'échelle chaque composante à la variance unitaire. Cela est souvent utile si les modèles en aval font des hypothèses fortes sur l'isotropie du signal : c'est par exemple le cas pour les machines à vecteurs de support avec le noyau RBF et l'algorithme de clustering K-Means.\n",
    "\n",
    "Ci-dessous, un exemple de l'ensemble de données iris, qui comprend 4 caractéristiques, projeté sur les 2 dimensions qui expliquent la plus grande variance :\n",
    "\n",
    "<figure class=\"align-center\">\n",
    "<img alt=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_pca_vs_lda_001.png\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_pca_vs_lda_001.png\" style=\"width: 480.0px; height: 360.0px;\" />\n",
    "</figure>\n",
    "\n",
    "L'objet [**`PCA`**][PCA] fournit également une interprétation probabiliste de l'ACP qui peut donner une probabilité des données en fonction de la quantité de variance qu'elle explique. Ainsi, il met en œuvre une méthode de [score](https://scikit-learn.org/stable/glossary.html#term-score) qui peut être utilisée dans la validation croisée :\n",
    "\n",
    "<figure class=\"align-center\">\n",
    "<img alt=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_pca_vs_fa_model_selection_001.png\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_pca_vs_fa_model_selection_001.png\" style=\"width: 480.0px; height: 360.0px;\" />\n",
    "</figure>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "># 2.5.1. Analyse en composantes principales (ACP) (*Principal component analysis (PCA)*)\n",
    "\n",
    "## 2.5.1.1. ACP exacte et interprétation probabiliste\n",
    "\n",
    "L'ACP est utilisée pour décomposer un ensemble de données multivariées en un ensemble de composantes orthogonales successives qui expliquent une quantité maximale de la variance. Dans scikit-learn, [`PCA`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) est implémenté comme un objet *transformateur* qui apprend $n$ composants dans sa méthode `fit`, et peut être utilisé sur de nouvelles données pour le projeter sur ces composants.\n",
    "\n",
    "L'ACP centre mais ne met pas à l'échelle les données d'entrée pour chaque entité avant d'appliquer la SVD. Le paramètre optionnel `whiten=True` permet de projeter les données sur l'espace singulier tout en mettant à l'échelle chaque composante à la variance unitaire. Ceci est souvent utile si les modèles en aval font des hypothèses fortes sur l'isotropie du signal : c'est par exemple le cas pour les Support Vector Machines avec le noyau RBF et l'algorithme de clustering K-Means.\n",
    "\n",
    "Vous trouverez ci-dessous un exemple de l'ensemble de données de l'iris, composé de 4 caractéristiques, projetées sur les 2 dimensions qui expliquent le plus de variance :\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_pca_vs_lda_001.png)\n",
    "\n",
    "L'objet [`PCA`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) fournit également une interprétation probabiliste de l'ACP qui peut donner une vraisemblance des données en fonction de la quantité de variance qu'elle explique. En tant que tel, il implémente une méthode de [score](https://scikit-learn.org/stable/glossary.html#term-score) qui peut être utilisée dans la validation croisée :\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_pca_vs_fa_model_selection_001.png)\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### [**Comparaison des projections 2D LDA et PCA de l'ensemble de données Iris**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_5_decomposition/plot_pca_vs_lda.ipynb)<br/>([*Comparison of LDA and PCA 2D projection of Iris dataset*](https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_lda.html))\n",
    "\n",
    "#### [**Sélection du modèle avec PCA probabiliste et analyse factorielle (FA)**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_5_decomposition/plot_pca_vs_fa_model_selection.ipynb)<br/>([*Model selection with Probabilistic PCA and Factor Analysis (FA)*](https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_fa_model_selection.html))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5.1.2. ACP incrémentale\n",
    "\n",
    "L'obje>t [**`PCA`**](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) est très utile, mais présente certaines limitations pour les grands ensembles de données. La plus grande limitation est que [**`PCA`**](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) ne prend en charge que le traitement par lots, ce qui signifie que toutes les données à traiter doivent tenir dans la mémoire principale. L'objet [**`IncrementalPCA`**](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html) utilise une forme de traitement différente et permet des calculs partiels qui correspondent presque exactement aux résultats de [**`PCA`**](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) tout en traitant les données en mini-lot. [**`IncrementalPCA`**](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html) permet de mettre en œuvre une Analyse en Composantes Principales out-of-core soit par :\n",
    "\n",
    "* Utilisation de sa méthode `partial_fit` sur des blocs de données récupérés séquentiellement à partir du disque dur local ou d'une base de données réseau.\n",
    "* Appel de sa méthode d'ajustement sur une matrice clairsemée ou un fichier mappé en mémoire à l'aide de `numpy.memmap`.\n",
    "\n",
    "[**`IncrementalPCA`**](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html) ne stocke que les estimations des variances des composants et du bruit, afin de mettre à jour le `explained_variance_ratio_` de manière incrémentielle. C'est pourquoi l'utilisation de la mémoire dépend du nombre d'échantillons par lot, plutôt que du nombre d'échantillons à traiter dans l'ensemble de données.\n",
    "\n",
    "Comme dans [**`PCA`**](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html), [**`IncrementalPCA`**](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html) centre mais ne met pas à l'échelle les données d'entrée pour chaque entité avant d'appliquer le SVD.\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_incremental_pca_001.png)\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_incremental_pca_002.png)\n",
    "\n",
    "### Exemple\n",
    "\n",
    "#### [**ACP incrémentale**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/examples/2_5_decomposition/plot_incremental_pca.ipynb)<br/>([*Incremental PCA*](https://scikit-learn.org/stable/auto_examples/decomposition/plot_incremental_pca.html))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5.1.3. ACP avec SVD aléatoire\n",
    "\n",
    "Il est souvent intéressant de projeter les données sur un espace de dimension inférieure qui préserve la plupart de la variance en enlevant le vecteur singulier des composantes associées aux valeurs singulières inférieures.\n",
    "\n",
    "Par exemple, si nous travaillons avec des images en niveaux de gris de 64x64 pixels pour la reconnaissance de visages, la dimensionnalité des données est de 4096 et il est lent de former une machine à vecteurs de support à noyau RBF sur des données aussi larges. De plus, nous savons que la dimensionnalité intrinsèque des données est beaucoup plus faible que 4096 car toutes les images de visages humains se ressemblent d'une certaine manière. Les échantillons se trouvent sur une variété de dimension beaucoup plus faible (disons environ 200 par exemple). L'algorithme ACP peut être utilisé pour transformer linéairement les données tout en réduisant la dimensionnalité et en préservant en même temps la plupart de la variance expliquée.\n",
    "\n",
    "La classe ACP utilisée avec le paramètre facultatif `svd_solver='randomized'` est très utile dans ce cas : étant donné que nous allons abandonner la plupart des vecteurs singuliers, il est beaucoup plus efficace de limiter les calculs à une estimation approchée des vecteurs singuliers que nous allons garder pour réellement effectuer la transformation.\n",
    "\n",
    "Par exemple, voici 16 portraits d'échantillons (centrés autour de 0,0) du jeu de données Olivetti. A droite se trouvent les 16 premiers vecteurs singuliers réorganisés sous forme de portraits. Comme nous ne nécessitons que les 16 premiers vecteurs singuliers d'un jeu de données de taille et , le temps de calcul est inférieur à 1s :"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e03b612d84ba21ce95ed447e81b3062e1eb99b56c6d885cdab4aaa12f1b8e240"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

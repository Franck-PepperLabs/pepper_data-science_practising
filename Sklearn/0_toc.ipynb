{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectif d'accès rapide.\n",
    "\n",
    "Objectif d'apprentissage des vocabulaires FR | EN mis en correspondance.\n",
    "\n",
    "Et pour le temps du reste à étudier et traduire, pour point d'avancement / reste à faire.\n",
    "\n",
    "# TODO LIST\n",
    "\n",
    "* Compléter la TOC v1 (FR | EN)\n",
    "* ✔ Ajouter des ancres\n",
    "    * soluce : https://stackoverflow.com/questions/38132862/html-anchors-in-a-jupyter-notebook-on-github\n",
    "* Déporter les exemples dans le dossier examples et leur ajouter un index préfixe\n",
    "* Scinder le chapitre 1, dans un premier temps en un nb par niveau 2 (niveau 3 plus tard si pertinent)\n",
    "* Recommit\n",
    "* Liens vers Github (c'est très lisible)\n",
    "* Pointer le fait et le reste à faire\n",
    "\n",
    "# Chapitres et sections\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. **Apprentissage supervisé** | [Supervised learning](https://scikit-learn.org/stable/supervised_learning.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Modèles linéaires | [Linear Models](https://scikit-learn.org/stable/modules/linear_model.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. Moindres carrés ordinaires | [Ordinary Least Squares]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2. Régression et classification de crête | [Ridge regression and classification]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3. Lasso | [Lasso]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4. Lasso multi-tâches | [Multi-task Lasso]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.5. Elastic-Net | [Elastic-Net]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.6. Elastic-Net multi-tâches | [Multi-task Elastic-Net]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.7. Régression au moindre angle (LAR) | [Least Angle Regression]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.8. LARS Lasso | [LARS Lasso]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.9. Poursuite par correspondance orthogonale (OMP) | [Orthogonal Matching Pursuit]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.10. Régression bayésienne | [Bayesian Regression]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.11. Régression logistique | [Logistic regression]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.12. Régression linéaire généralisée (GLR) | [Generalized Linear Regression]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.13. Descente de gradient stochastique (SGD) | [Stochastic Gradient Descent]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.14. Perceptron | [Perceptron]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.15. Algorithmes passifs agressifs | [Passive Aggressive Algorithms]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.16. Régression de robustesse : valeurs aberrantes et erreurs de modélisation | [Robustness regression: outliers and modeling errors]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.17. Régression quantile | [Quantile Regression]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.18. Régression polynomiale : extension des modèles linéaires avec des fonctions de base | [Polynomial regression: extending linear models with basis functions](https://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Analyse discriminante linéaire et quadratique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Régression de crête à noyau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Machines à vecteurs de support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Descente de gradient stochastique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Plus proches voisins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7. Processus gaussiens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8. Décomposition croisée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9. Bayes naïf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.10. Arbres de décision | [Decision Trees](https://scikit-learn.org/stable/modules/tree.html#)\n",
    "\n",
    "* 1.10.1. Classification | [Classification](https://scikit-learn.org/stable/modules/tree.html#classification)\n",
    "* 1.10.2. Régression | [Regression](https://scikit-learn.org/stable/modules/tree.html#regression)\n",
    "* 1.10.3. Problèmes multi-sorties | [Multi-output problems](https://scikit-learn.org/stable/modules/tree.html#multi-output-problems)\n",
    "* 1.10.4. Complexité | [Complexity](https://scikit-learn.org/stable/modules/tree.html#complexity)\n",
    "* 1.10.5. Conseils d'utilisation pratique | [Tips on practical use](https://scikit-learn.org/stable/modules/tree.html#tips-on-practical-use)\n",
    "* 1.10.6. Algorithmes d'arbre : ID3, C4.5, C5.0 et CART | [Tree algorithms: ID3, C4.5, C5.0 and CART](https://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart)\n",
    "* 1.10.7. Formulation mathématique | [Mathematical formulation](https://scikit-learn.org/stable/modules/tree.html#mathematical-formulation)\n",
    "* 1.10.8. Élagage à coût-complexité minimal | [Minimal Cost-Complexity Pruning](https://scikit-learn.org/stable/modules/tree.html#minimal-cost-complexity-pruning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.11. Méthodes ensemblistes\n",
    "\n",
    "* 1.11.1. Méta-estimateur de bagging | Bagging meta-estimator\n",
    "* 1.11.2. Forêts d'arbres aléatoires | Forests of randomized trees\n",
    "* 1.11.3. AdaBoost | AdaBoost\n",
    "* 1.11.4. Amélioration de l'arbre par gradient | Gradient Tree Boosting\n",
    "* 1.11.5. Amplification du gradient basée sur l'histogramme | Histogram-Based Gradient Boosting\n",
    "* 1.11.6. Classifieur de vote | Voting Classifier\n",
    "* 1.11.7. Régresseur de vote | Voting Regressor\n",
    "* 1.11.8. Généralisation empilée | Stacked generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.12. Algorithmes multiclasses et multisorties | Multiclass and multioutput algorithms\n",
    "\n",
    "* 1.12.1. Classification multiclasse | Multiclass classification\n",
    "* 1.12.2. Classement multilabel | Multilabel classification\n",
    "* 1.12.3. Classification multiclasses-multisorties | Multiclass-multioutput classification\n",
    "* 1.12.4. Régression multi-sorties | Multioutput regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.13. Sélection de caractéristiques\n",
    "\n",
    "* 1.13.1. Suppression des caractéristiques à faible variance | Removing features with low variance\n",
    "* 1.13.2. Sélection de caractéristiques univariées | Univariate feature selection\n",
    "* 1.13.3. Élimination récursive des caractéristiques | Recursive feature elimination\n",
    "* 1.13.4. Sélection de caractéristiques à l'aide de SelectFromModel | Feature selection using SelectFromModel\n",
    "* 1.13.5. Sélection séquentielle des caractéristiques | Sequential Feature Selection\n",
    "* 1.13.6. Sélection de caractéristiques dans le cadre d'un pipeline | Feature selection as part of a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.14. Apprentissage semi-supervisé | Semi-supervised learning\n",
    "\n",
    "* 1.14.1. Auto entrainement | Self Training\n",
    "* 1.14.2. Propagation des étiquettes | Label Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.15. Régression isotonique | Isotonic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.16. Étalonnage de probabilité | Probability calibration\n",
    "\n",
    "* 1.16.1. Courbes d'étalonnage | Calibration curves\n",
    "* 1.16.2. Calibrer un classifieur | Calibrating a classifier\n",
    "* 1.16.3. Usage | Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.17. Modèles de réseaux de neurones (supervisés) | Neural network models (supervised)\n",
    "\n",
    "* 1.17.1. Perceptron multicouche | Multi-layer Perceptron\n",
    "* 1.17.2. Classification | Classification\n",
    "* 1.17.3. Régression | Regression\n",
    "* 1.17.4. Régularisation | Regularization\n",
    "* 1.17.5. Algorithmes | Algorithms\n",
    "* 1.17.6. Complexité | Complexity\n",
    "* 1.17.7. Formulation mathématique | Mathematical formulation\n",
    "* 1.17.8. Conseils d'utilisation pratique | Tips on Practical Use\n",
    "* 1.17.9. Plus de contrôle avec warm_start | More control with warm_start\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. **Unsupervised learning**\n",
    "\n",
    "## 2.1. Modèles de mélange gaussien | Gaussian mixture models\n",
    "\n",
    "* 2.1.1. Mélange gaussien | Gaussian Mixture\n",
    "* 2.1.2. Mélange gaussien bayésien variationnel | Variational Bayesian Gaussian Mixture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Apprentissage de variétés\n",
    "\n",
    "* 2.2.1. Introduction | Introduction\n",
    "* 2.2.2. Isocarte | Isomap\n",
    "* 2.2.3. Incorporation localement linéaire | Locally Linear Embedding\n",
    "* 2.2.4. Incorporation localement linéaire modifiée | Modified Locally Linear Embedding\n",
    "* 2.2.5. Cartographie propre de Hesse | Hessian Eigenmapping\n",
    "* 2.2.6. Intégration spectrale |  Spectral Embedding\n",
    "* 2.2.7. Alignement de l'espace tangent local | Local Tangent Space Alignment\n",
    "* 2.2.8. Mise à l'échelle multidimensionnelle (MDS) | Multi-dimensional Scaling (MDS)\n",
    "* 2.2.9. Encastrement du voisin stochastique distribué en t (t-SNE) | t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "* 2.2.10. Conseils d'utilisation pratique | Tips on practical use\n",
    "\n",
    "\n",
    "Compléments :\n",
    "\n",
    "* Wikipédia :\n",
    "    * [Variété (géométrie)](https://fr.wikipedia.org/wiki/Variété_(géométrie)) | [Manifold](https://en.wikipedia.org/wiki/Manifold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. [**Partitionnement**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/2_3_clustering.ipynb)</br>([*Clustering*](https://scikit-learn.org/stable/modules/clustering.html))\n",
    "\n",
    "* 2.3.1. Présentation des méthodes de partitionnement | Overview of clustering methods\n",
    "* 2.3.2. K-moyennes | K-means\n",
    "* 2.3.3. Propagation par affinité | Affinity Propagation\n",
    "* 2.3.4. Décalage moyen | Mean Shift\n",
    "* 2.3.5. Partitionnement spectral | Spectral clustering\n",
    "* 2.3.6. Partitionnement hiérarchique | Hierarchical clustering\n",
    "* 2.3.7. DBSCAN | DBSCAN\n",
    "* 2.3.8. OPTICS | OPTICS\n",
    "* 2.3.9. BIRCH | BIRCH\n",
    "* 2.3.10. Évaluation des performances de partitionnement | Clustering performance evaluation\n",
    "\n",
    "Compléments :\n",
    "\n",
    "* Wikipédia :\n",
    "    * [K-moyennes](https://fr.wikipedia.org/wiki/K-moyennes) | [$k$-means clustering](https://en.wikipedia.org/wiki/K-means_clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Classification double | Biclustering\n",
    "\n",
    "* 2.4.1. Co-classification spectrale | Spectral Co-Clustering\n",
    "* 2.4.2. Classification double spectrale | Spectral Biclustering\n",
    "* 2.4.3. Évaluation de la classification double | Biclustering evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Décomposer les signaux en composantes (problèmes de factorisation matricielle) | Decomposing signals in components (matrix factorization problems)\n",
    "\n",
    "* 2.5.1. Analyse en composantes principales (ACP) | Principal component analysis (PCA)\n",
    "* 2.5.2. Analyse en composantes principales du noyau (kACP) | Kernel Principal Component Analysis (kPCA)\n",
    "* 2.5.3. Décomposition en valeurs singulières tronquées et analyse sémantique latente | Truncated singular value decomposition and latent semantic analysis\n",
    "* 2.5.4. Apprentissage par dictionnaire | Dictionary Learning\n",
    "* 2.5.5. Analyse factorielle | Factor Analysis\n",
    "* 2.5.6. Analyse en composantes indépendantes (ICA) | Independent component analysis (ICA)\n",
    "* 2.5.7. Factorisation matricielle non négative (NMF ou NNMF) | Non-negative matrix factorization (NMF or NNMF)\n",
    "* 2.5.8. Allocation de Dirichlet latente (LDA) | Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "Compléments :\n",
    "\n",
    "* Wikipédia :\n",
    "    * [Allocation de Dirichlet latente](https://fr.wikipedia.org/wiki/Allocation_de_Dirichlet_latente) | [Latent Dirichlet allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Estimation de la covariance| Covariance estimation\n",
    "\n",
    "* 2.6.1. Covariance empirique | Empirical covariance\n",
    "* 2.6.2. Covariance réduite | Shrunk Covariance\n",
    "* 2.6.3. Covariance inverse parcimonieuse | Sparse inverse covariance\n",
    "* 2.6.4. Estimation robuste de la covariance | Robust Covariance Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7. Détection de nouveauté et de valeurs aberrantes | Novelty and Outlier Detection\n",
    "\n",
    "* 2.7.1. Présentation des méthodes de détection des valeurs aberrantes | Overview of outlier detection methods\n",
    "* 2.7.2. Détection de nouveauté | Novelty Detection\n",
    "* 2.7.3. Détection des valeurs aberrantes | Outlier Detection\n",
    "* 2.7.4. Détection de nouveauté avec Local Outlier Factor | Novelty detection with Local Outlier Factor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8. Estimation de la densité | Density Estimation\n",
    "\n",
    "* 2.8.1. Estimation de la densité : histogrammes | Density Estimation: Histograms\n",
    "* 2.8.2. Estimation de la densité du noyau (KDE) | Kernel Density Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9. Modèles de réseaux de neurones (non supervisés) | Neural network models (unsupervised)\n",
    "\n",
    "* 2.9.1. Machines Boltzmann restreintes | Restricted Boltzmann machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _3. Model selection and evaluation\n",
    "\n",
    "3.1. Cross-validation: evaluating estimator performance\n",
    "\n",
    "3.2. Tuning the hyper-parameters of an estimator\n",
    "\n",
    "3.3. Metrics and scoring: quantifying the quality of predictions\n",
    "\n",
    "3.4. Validation curves: plotting scores to evaluate models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✔ 4. [**Inspection**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/4_inspection.ipynb)</br>([*Inspection*](https://scikit-learn.org/stable/inspection.html))\n",
    "\n",
    "La performance prédictive est souvent l'objectif principal du développement de modèles d'apprentissage automatique. Pourtant, résumer les performances avec une métrique d'évaluation est souvent insuffisant : cela suppose que la métrique d'évaluation et l'ensemble de données de test reflètent parfaitement le domaine cible, ce qui est rarement vrai. Dans certains domaines, un modèle a besoin d'un certain niveau d'interprétabilité avant de pouvoir être déployé. Un modèle qui présente des problèmes de performances doit être débogué pour comprendre le problème sous-jacent du modèle. Le module sklearn.inspection fournit des outils pour aider à comprendre les prédictions d'un modèle et ce qui les affecte. Cela peut être utilisé pour évaluer les hypothèses et les biais d'un modèle, concevoir un meilleur modèle ou diagnostiquer les problèmes de performances du modèle.\n",
    "\n",
    "✔ 4.1. [**Graphiques de dépendance partielle et d'espérance conditionnelle individuelle**](https://scikit-learn.org/stable/modules/partial_dependence.html)\n",
    "\n",
    "* ✔ 4.1.1. Graphiques de dépendance partielle\n",
    "* ✔ 4.1.2. Diagramme d'espérance conditionnelle individuelle (ICE)\n",
    "* ✔ 4.1.3. Définition mathématique\n",
    "* ✔ 4.1.4. Méthodes de calcul\n",
    "\n",
    "✔ 4.2. [**Importance des caractéristiques de permutation**](https://scikit-learn.org/stable/modules/permutation_importance.html)\n",
    "\n",
    "* ✔ 4.2.1. Aperçu de l'algorithme d'importance de permutation\n",
    "* ✔ 4.2.2. Relation avec l'importance basée sur les impuretés dans les arbres\n",
    "* ✔ 4.2.3. Valeurs trompeuses sur les caractéristiques fortement corrélées\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✔ 5. [**Visualisations**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/5_visualizations.ipynb)</br>([*Visualizations*](https://scikit-learn.org/stable/visualizations.html))\n",
    "\n",
    "Scikit-learn définit une API simple pour créer des visualisations pour l'apprentissage automatique. La principale caractéristique de cette API est de permettre un traçage rapide et des ajustements visuels sans recalcul. Nous fournissons des classes `Display` qui exposent deux méthodes de création de tracés : `from_estimator` et `from_predictions`.\n",
    "\n",
    "✔ 5.0. [**Exemples**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/5_visualizations.ipynb#examples)\n",
    "\n",
    "✔ 5.1. [**Utilitaires de traçage disponibles**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/5_visualizations.ipynb#available-plotting-utilities)\n",
    "([Available Plotting Utilities](https://scikit-learn.org/stable/visualizations.html#available-plotting-utilities))\n",
    "\n",
    "* ✔ 5.1.1. [**Fonctions**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/5_visualizations.ipynb#functions)\n",
    "([*Functions*](https://scikit-learn.org/stable/visualizations.html#functions))\n",
    "\n",
    "* ✔ 5.1.1. [**Objets d'affichage**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/5_visualizations.ipynb#display-objects)\n",
    "([*Display Objects*](https://scikit-learn.org/stable/visualizations.html#display-objects))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _6. Dataset transformations\n",
    "\n",
    "6.1. partial ✔ Pipelines et estimateurs composites | Pipelines and composite estimators\n",
    "* partial ✔ 6.1.1. Pipeline : estimateurs de chaînage | Pipeline: chaining estimators\n",
    "* ✘ 6.1.2. Transformer la cible en régression | Transforming target in regression\n",
    "* ✘ 6.1.3. FeatureUnion : espaces d'entités composites | FeatureUnion: composite feature spaces\n",
    "* ✘ 6.1.4. ColumnTransformer pour les données hétérogènes | ColumnTransformer for heterogeneous data\n",
    "* ✘ 6.1.5. Visualisation des estimateurs composites | Visualizing Composite Estimators\n",
    "\n",
    "✘ 6.2. Extraction de caractéristiques | Feature extraction\n",
    "* ✘ 6.2.1. Chargement de fonctionnalités à partir de dicts | Loading features from dicts\n",
    "* ✘ 6.2.2. Hachage des fonctionnalités | Feature hashing\n",
    "* ✘ 6.2.3. Extraction de caractéristiques de texte | Text feature extraction\n",
    "* ✘ 6.2.4. Extraction de caractéristiques d'image | Image feature extraction\n",
    "\n",
    "✘ 6.3. Prétraitement des données | Preprocessing data\n",
    "* ✘ 6.3.1. Standardisation, ou suppression de la moyenne et mise à l'échelle de la variance | Standardization, or mean removal and variance scaling\n",
    "* ✘ 6.3.2. Transformation non linéaire | Non-linear transformation\n",
    "* ✘ 6.3.3. Normalisation | Normalization\n",
    "* ✘ 6.3.4. Encodage des caractéristiques catégorielles | Encoding categorical features\n",
    "* ✘ 6.3.5. Discrétisation | Discretization\n",
    "* ✘ 6.3.6. Imputation des valeurs manquantes | Imputation of missing values\n",
    "* ✘ 6.3.7. Génération de caractéristiques polynomiales | Generating polynomial features\n",
    "* ✘ 6.3.8. Transformateurs personnalisés | Custom transformers\n",
    "\n",
    "✘ 6.4. Imputation des valeurs manquantes | Imputation of missing values\n",
    "* ✘ 6.4.1. Imputation univariée vs imputation multivariée | Univariate vs. Multivariate Imputation\n",
    "* ✘ 6.4.2. Imputation de caractéristique univariée | Univariate feature imputation\n",
    "* ✘ 6.4.3. Imputation de caractéristiques multivariées | Multivariate feature imputation\n",
    "* ✘ 6.4.4. Références | References\n",
    "* ✘ 6.4.5. Imputation des plus proches voisins | Nearest neighbors imputation\n",
    "* ✘ 6.4.6. Marquage des valeurs imputées | Marking imputed values\n",
    "* ✘ 6.4.7. Estimateurs qui gèrent les valeurs NaN | Estimators that handle NaN values\n",
    "\n",
    "✘ 6.5. Réduction de dimensionnalité non supervisée | Unsupervised dimensionality reduction\n",
    "* ✘ 6.5.1. ACP : analyse en composantes principales | PCA: principal component analysis\n",
    "* ✘ 6.5.2. Projections aléatoires | Random projections\n",
    "* ✘ 6.5.3. Agglomération de fonctionnalités | Feature agglomeration\n",
    "\n",
    "✘ 6.6. Projection aléatoire | Random Projection\n",
    "* ✘ 6.6.1. Le lemme de Johnson-Lindenstrauss | The Johnson-Lindenstrauss lemma\n",
    "* ✘ 6.6.2. Projection aléatoire gaussienne | Gaussian random projection\n",
    "* ✘ 6.6.3. Projection aléatoire parcimonieuse | Sparse random projection\n",
    "* ✘ 6.6.4. Transformation inverse | Inverse Transform\n",
    "\n",
    "✘ 6.7. Approximation du noyau | Kernel Approximation\n",
    "* ✘ 6.7.1. Méthode Nystroem pour l'approximation du noyau | Nystroem Method for Kernel Approximation\n",
    "* ✘ 6.7.2. Noyau de fonction de base radiale | Radial Basis Function Kernel\n",
    "* ✘ 6.7.3. Additif Chi Squared Kernel | Additive Chi Squared Kernel\n",
    "* ✘ 6.7.4. Noyau au carré de chi asymétrique | Skewed Chi Squared Kernel\n",
    "* ✘ 6.7.5. Approximation du noyau polynomial via Tensor Sketch | Polynomial Kernel Approximation via Tensor Sketch\n",
    "* ✘ 6.7.6. Détails mathématiques | Mathematical Details\n",
    "\n",
    "✘ 6.8. Métriques par paires, affinités et noyaux | Pairwise metrics, Affinities and Kernels\n",
    "* ✘ 6.8.1. Similitude cosinus | Cosine similarity\n",
    "* ✘ 6.8.2. Noyau linéaire | Linear kernel\n",
    "* ✘ 6.8.3. Noyau polynomial | Polynomial kernel\n",
    "* ✘ 6.8.4. Noyau sigmoïde | Sigmoid kernel\n",
    "* ✘ 6.8.5. Noyau RBF | RBF kernel\n",
    "* ✘ 6.8.6. Noyau laplacien | Laplacian kernel\n",
    "* ✘ 6.8.7. Noyau du chi carré | Chi-squared kernel\n",
    "\n",
    "✔ 6.9. Transformer la cible de prédiction (y) | Transforming the prediction target (y)\n",
    "* ✔ 6.9.1. Binarisation des étiquettes | Label binarization\n",
    "* ✔ 6.9.2. Encodage des étiquettes | Label encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _7. Dataset loading utilities\n",
    "\n",
    "* 7.1. Toy datasets\n",
    "    * 7.1.1. Boston house prices dataset\n",
    "    * 7.1.2. Iris plants dataset\n",
    "    * 7.1.3. Diabetes dataset\n",
    "    * 7.1.4. Optical recognition of handwritten digits dataset\n",
    "    * 7.1.5. Linnerrud dataset\n",
    "    * 7.1.6. Wine recognition dataset\n",
    "    * 7.1.7. Breast cancer wisconsin (diagnostic) dataset\n",
    "* 7.2. Real world datasets\n",
    "    * 7.2.1. The Olivetti faces dataset\n",
    "    * 7.2.2. The 20 newsgroups text dataset\n",
    "    * 7.2.3. The Labeled Faces in the Wild face recognition dataset\n",
    "    * 7.2.4. Forest covertypes\n",
    "    * 7.2.5. RCV1 dataset\n",
    "    * 7.2.6. Kddcup 99 dataset\n",
    "    * 7.2.7. California Housing dataset\n",
    "* 7.3. Generated datasets\n",
    "    * 7.3.1. Generators for classification and clustering\n",
    "    * 7.3.2. Generators for regression\n",
    "    * 7.3.3. Generators for manifold learning\n",
    "    * 7.3.4. Generators for decomposition\n",
    "* 7.4. Loading other datasets\n",
    "    * 7.4.1. Sample images\n",
    "    * 7.4.2. Datasets in svmlight / libsvm format\n",
    "    * 7.4.3. Downloading datasets from the openml.org repository\n",
    "    * 7.4.4. Loading from external datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _8. [**Calculer avec scikit-learn**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/8_computing.ipynb)</br>([*Computing with scikit-learn*](https://scikit-learn.org/stable/computing.html))\n",
    "\n",
    "✔ 8.1. [**Stratégies de mise à l'échelle informatique : données plus volumineuses**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/8_computing.ipynb#strategies-to-scale-computationally-bigger-data)\n",
    "([*Strategies to scale computationally: bigger data*](https://scikit-learn.org/stable/computing.html#strategies-to-scale-computationally-bigger-data))\n",
    "\n",
    "* ✔ 8.1.1. [**Mise à l'échelle avec des instances utilisant l'apprentissage hors cœur**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/8_computing.ipynb#scaling-with-instances-using-out-of-core-learning)\n",
    "([*Scaling with instances using out-of-core learning*](https://scikit-learn.org/stable/computing.html#scaling-with-instances-using-out-of-core-learning))\n",
    "\n",
    "8.2. [**Performances informatiques**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/8_computing.ipynb#)\n",
    "([*Computational Performance*](https://scikit-learn.org/stable/computing.html#))\n",
    "\n",
    "* 8.2.1. [**Latence de prédiction**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/8_computing.ipynb#)\n",
    "([*Prediction Latency*](https://scikit-learn.org/stable/computing.html#))\n",
    "* 8.2.2. [**Débit de prédiction**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/8_computing.ipynb#)\n",
    "([*Prediction Throughput*](https://scikit-learn.org/stable/computing.html#))\n",
    "* 8.2.3. [**Trucs et astuces**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/8_computing.ipynb#)\n",
    "([*Tips and Tricks*](https://scikit-learn.org/stable/computing.html#))\n",
    "\n",
    "8.3. [**Parallélisme, gestion des ressources et configuration**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/8_computing.ipynb#)\n",
    "([*Parallelism, resource management, and configuration*]())\n",
    "\n",
    "* 8.3.1. [**Parallélisme**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/8_computing.ipynb#)\n",
    "([*Parallelism*]())\n",
    "* 8.3.2. [**Commutateurs de configuration**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/8_computing.ipynb#)\n",
    "([*Configuration switches*]())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✔ 9. [**Persistance de modèle**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/9_model_persistence.ipynb)<br/>([*Model persistence*](https://scikit-learn.org/stable/model_persistence.html))\n",
    "\n",
    "✔ 9.1. [**Sérialisation spécifique Python**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/9_model_persistence.ipynb#python-specific-serialization)\n",
    "([*Python specific serialization*](https://scikit-learn.org/stable/model_persistence.html#python-specific-serialization))\n",
    "\n",
    "* ✔ 9.1.1. [**Limites de sécurité et de maintenabilité**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/9_model_persistence.ipynb#security-maintainability-limitations)\n",
    "([*Security & maintainability limitations*](https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations))\n",
    "\n",
    "✔ 9.2. [**Formats interopérables**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/9_model_persistence.ipynb#interoperable-formats)\n",
    "([*Interoperable formats*](https://scikit-learn.org/stable/model_persistence.html#interoperable-formats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✔ 10. [**Pièges courants et pratiques recommandées**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/10_pitfalls_good_practises.ipynb)<br/>([*Common pitfalls and recommended practices*](https://scikit-learn.org/stable/common_pitfalls.html))\n",
    "\n",
    "✔ 10.1. [**Prétraitement incohérent**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/10_pitfalls_good_practises.ipynb#inconsistent-preprocessing)\n",
    "([*Inconsistent preprocessing*](https://scikit-learn.org/stable/common_pitfalls.html#inconsistent-preprocessing))\n",
    "\n",
    "✔ 10.2. [**Fuite de données**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/10_pitfalls_good_practises.ipynb#data-leakage)\n",
    "([*Data leakage*](https://scikit-learn.org/stable/common_pitfalls.html#data-leakage))\n",
    "* ✔ 10.2.1. [**Fuite de données lors du prétraitement**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/10_pitfalls_good_practises.ipynb#data-leakage-during-pre-processing)\n",
    "([*Data leakage during pre-processing*](https://scikit-learn.org/stable/common_pitfalls.html#data-leakage-during-pre-processing))\n",
    "* ✔ 10.2.2. [**Comment éviter les fuites de données**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/10_pitfalls_good_practises.ipynb#how-to-avoid-data-leakage)\n",
    "([*How to avoid data leakage*](https://scikit-learn.org/stable/common_pitfalls.html#how-to-avoid-data-leakage))\n",
    "\n",
    "✔ 10.3. [**Contrôle de l'aléatoire**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/10_pitfalls_good_practises.ipynb#controlling-randomness)\n",
    "([*Controlling randomness*](https://scikit-learn.org/stable/common_pitfalls.html#controlling-randomness))\n",
    "* ✔ 10.3.1. [**Utilisation d'instances None ou RandomState, et appels répétés pour ajuster et diviser**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/10_pitfalls_good_practises.ipynb#using-none-or-randomstate-instances-and-repeated-calls-to-fit-and-split)\n",
    "([*Using None or RandomState instances, and repeated calls to fit and split*](https://scikit-learn.org/stable/common_pitfalls.html#using-none-or-randomstate-instances-and-repeated-calls-to-fit-and-split))\n",
    "* ✔ 10.3.2. [**Pièges et subtilités courants**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/10_pitfalls_good_practises.ipynb#common-pitfalls-and-subtleties)\n",
    "([*Common pitfalls and subtleties*](https://scikit-learn.org/stable/common_pitfalls.html#common-pitfalls-and-subtleties))\n",
    "* ✔ 10.3.3. [**Recommandations générales**](https://nbviewer.org/github/Franck-PepperLabs/pepper_data-science_practising/blob/main/Sklearn/10_pitfalls_good_practises.ipynb#general-recommendations)\n",
    "([*General recommendations*](https://scikit-learn.org/stable/common_pitfalls.html#general-recommendations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrêt précoce de la descente de gradient stochastique\n",
    "\n",
    "User Guide | [Early stopping of Stochastic Gradient Descent](https://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_early_stopping.html?highlight=ignore_warnings#)\n",
    "\n",
    "Traduction | ... lien vers Github\n",
    "\n",
    "In :\n",
    "* [SGDClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e03b612d84ba21ce95ed447e81b3062e1eb99b56c6d885cdab4aaa12f1b8e240"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

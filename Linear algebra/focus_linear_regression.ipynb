{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focus\n",
    "\n",
    "Le problème de régression linéaire et la méthode des moindres carrés constituent une base commune essentielle de nombreux algorithmes qui en sont finalement des prolongements.\n",
    "\n",
    "Il n'est donc pas tolérable d'avancer dans l'apprentissage des algorithmes ML sans parfaitement maîtriser cette base, tant du point de vue du principe que de celui des outils formels.\n",
    "\n",
    "Or, à ce sujet, le cours OC / CentraleSupélec m'a laissé sur ma faim.\n",
    "\n",
    "Je vais donc compiler ici une sélection de ressources et les étudier à fond.\n",
    "\n",
    "Je reprends l'intégralité des textes et les traduis en français, ma langue maternelle, reconstruis le $\\LaTeX$, ce qui fait partie de ma méthode d'appropriation et d'assimilation conceptuelle.\n",
    "\n",
    "Ces ressources sont :\n",
    "* [Eli Bendersky's website | Derivation of the Normal Equation for linear regression ](https://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression), donné en référence par le cours OC.\n",
    "* [Wolfram MathWorld | Least Squares Fitting](https://mathworld.wolfram.com/LeastSquaresFitting.html)\n",
    "* [Wikipedia | Linear least squares](https://en.wikipedia.org/wiki/Linear_least_squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Eli Bendersky's website | Dérivation de l'équation normale pour la régression linéaire](https://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression)\n",
    "\n",
    "J'étais en train de suivre le cours Coursera \"Machine Learning\", et dans la section sur la régression linéaire multivariée, quelque chose a attiré mon attention. Andrew Ng a présenté l'[équation normale](https://en.wikipedia.org/wiki/Linear_least_squares) comme une solution analytique au problème de régression linéaire avec une fonction de coût des moindres carrés. Il a mentionné que dans certains cas (comme pour les petits ensembles de caractéristiques), son utilisation était plus efficace que l'application d'une descente de gradient; malheureusement, il a omis sa dérivation.\n",
    "\n",
    "Ici, je veux montrer comment l'équation normale est dérivée.\n",
    "\n",
    "Tout d'abord, un peu de terminologie. Les symboles qui suivent sont compatibles avec le cours de Machine Learning, mais pas avec l'exposition de l'équation normale sur Wikipédia et d'autres sites - sémantiquement, c'est la même chose, seuls les symboles différent.\n",
    "\n",
    "Étant donné la fonction d'hypothèse :\n",
    "\n",
    "$h_\\theta(x) = \\theta_0 x_0 + \\theta_1 x_1 + \\cdots + \\theta_n x_n$\n",
    "\n",
    "Nous souhaitons minimiser le coût des moindres carrés :\n",
    "\n",
    "$J(\\theta_{0 \\cdots n}) = \\frac{1}{2m} \\displaystyle\\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)}\\right)$\n",
    "\n",
    "Où $x^{(i)}$ est le `i`ème échantillon (d'un ensemble de $m$ échantillons) et $y^{(i)}$ le `i`ème résultat attendu.\n",
    "\n",
    "Pour continuer, nous allons représenter le problème en notation matricielle ; c'est naturel, puisque nous avons essentiellement ici un système d'équations linéaires. Les coefficients de régression $\\theta$ que nous recherchons forment le vecteur :\n",
    "\n",
    "$\\begin{pmatrix}\\theta_0\\\\\\theta_1\\\\\\vdots\\\\\\theta_n\\end{pmatrix} \\in \\mathbb{R}^{n + 1}$\n",
    "\n",
    "De même, chacun des $m$ échantillons d'entrée est un vecteur colonne avec $n+1$ lignes, avec $x_0 = 1$ par commodité. Nous pouvons donc maintenant réécrire la fonction d'hypothèse comme suit :\n",
    "\n",
    "$h_\\theta(x) = \\theta^\\top x = x^\\top \\theta$\n",
    "\n",
    "*NDLR - le troisième terme est un ajout DLR*\n",
    "\n",
    "Lorsque cela est additionné sur tous les échantillons, nous pouvons plonger davantage dans la notation matricielle. Nous définirons la \"matrice de conception\" $X$ (X majuscule) comme une matrice de $m$ lignes, dans laquelle chaque ligne est le ième échantillon (le vecteur $x^{(i)}$). Avec cela, nous pouvons réécrire le coût des moindres carrés comme suit, en remplaçant la somme explicite par la multiplication matricielle :\n",
    "\n",
    "$J(\\theta) = \\frac{1}{2m} (X\\theta - y)^\\top(X\\theta - y)$\n",
    "\n",
    "Maintenant, en utilisant certaines identités de transposition matricielle, nous pouvons simplifier un peu cela. Je vais jeter la partie $\\frac{1}{2m}$ puisque nous allons de toute façon comparer une dérivée à zéro :\n",
    "\n",
    "$J'(\\theta) = \\left((X\\theta)^\\top - y^\\top\\right)(X\\theta - y) = (X\\theta)^\\top X\\theta - (X\\theta)^\\top y - y^\\top (X\\theta) + y^\\top y$\n",
    "\n",
    "Rappel des propriétés de la transposée :\n",
    "* $(A + B)^\\top = A^\\top + B^\\top$\n",
    "* $(AB)^\\top = B^\\top A^\\top$\n",
    "\n",
    "Notons que $X\\theta$ est un vecteur, tout comme $y$. Ainsi, lorsque nous multiplions l'un par l'autre, peu importe l'ordre (du moment que les dimensions fonctionnent). On peut donc encore simplifier :\n",
    "\n",
    "$J'(\\theta) = \\theta^\\top X^\\top X\\theta - 2(X\\theta)^\\top y + y^\\top y$\n",
    "\n",
    "Rappelons qu'ici $\\theta$ est notre inconnue. Pour trouver le minimum de la fonction ci-dessus, nous dériverons par $\\theta$ et nous comparerons à 0. Dériver par un vecteur peut sembler inconfortable, mais il n'y a rien à craindre. Rappelons qu'ici nous n'utilisons la notation matricielle que pour représenter commodément un système de formules linéaires. Nous dérivons donc par chaque composante du vecteur, puis combinons à nouveau les dérivées résultantes dans un vecteur. Le résultat est:\n",
    "\n",
    "*NDLR - là, ça ne me suffit pas, il m'en faut davantage* : <mark>trouver un lien qui explique la dérivation par un vecteur</mark>\n",
    "\n",
    "$\\frac{\\partial J'}{\\partial \\theta} = 2X^\\top X\\theta-2X^\\top y = 0$\n",
    "\n",
    "Soit :\n",
    "\n",
    "$X^\\top X\\theta = X^\\top y$\n",
    "\n",
    "Maintenant, en supposant que la matrice $X^\\top X$ est inversible, nous pouvons multiplier les deux côtés par $(X^\\top X)^{-1}$ et obtenir :\n",
    "\n",
    "$\\theta=(X^\\top X)^{-1} X^\\top y$\n",
    "\n",
    "C'est l'**équation normale**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Wolfram MathWorld | Ajustement des moindres carrés](https://mathworld.wolfram.com/LeastSquaresFitting.html)\n",
    "\n",
    "<mark>Insérer l'image : comment mettre le fond en blanc ? div background-color: ne marche pas...</mark>\n",
    "\n",
    "<div background-color:#ABBAEA>\n",
    "\n",
    "![](https://mathworld.wolfram.com/images/eps-svg/LeastSquaresFitting_1000.svg)\n",
    "\n",
    "\n",
    "Une procédure mathématique pour trouver la courbe la mieux adaptée à un ensemble de points donné en minimisant la somme des carrés des décalages (\"les résidus\") des points de la courbe. La somme des carrés des décalages est utilisée à la place des valeurs absolues de décalage car cela permet de traiter les résidus comme une quantité différentiable continue. Cependant, étant donné que les carrés des décalages sont utilisés, les points périphériques peuvent avoir un effet disproportionné sur l'ajustement, une propriété qui peut être souhaitable ou non selon le problème à résoudre.\n",
    "\n",
    "<mark>Insérer l'image</mark>\n",
    "\n",
    "![](https://mathworld.wolfram.com/images/eps-svg/LeastSquaresOffsets_1000.svg)\n",
    "\n",
    "En pratique, les décalages *verticaux* par rapport à une droite (polynôme, surface, hyperplan, etc.) sont presque toujours minimisés au lieu des [décalages perpendiculaires](https://mathworld.wolfram.com/LeastSquaresFittingPerpendicularOffsets.html). Cela fournit une fonction d'ajustement pour la variable indépendante $X$ qui estime $y$ pour un $x$ donné (le plus souvent ce qu'un expérimentateur veut), permet d'incorporer simplement les incertitudes des points de données le long des axes $x^-$ et $y^-$, et fournit également forme analytique beaucoup plus simple pour les paramètres d'ajustement que celle qui serait obtenue en utilisant un ajustement basé sur des [décalages perpendiculaires](https://mathworld.wolfram.com/LeastSquaresFittingPerpendicularOffsets.html). De plus, la technique d'ajustement peut être facilement généralisée à partir d'une droite de meilleur ajustement à un [polynôme de meilleur ajustement](https://mathworld.wolfram.com/LeastSquaresFittingPolynomial.html) lorsque des sommes de distances verticales sont utilisées. Dans tous les cas, pour un nombre raisonnable de points de données bruités, la différence entre les ajustements verticaux et perpendiculaires est assez faible.\n",
    "\n",
    "La technique d'ajustement linéaire des moindres carrés est la forme de [régression linéaire](https://mathworld.wolfram.com/LinearRegression.html) la plus simple et la plus couramment appliquée et fournit une solution au problème de trouver la ligne droite la mieux ajustée à travers un ensemble de points. En fait, si la relation fonctionnelle entre les deux quantités représentées graphiquement est connue à l'intérieur de constantes additives ou multiplicatives, il est courant de transformer les données de telle manière que la ligne résultante soit une ligne droite, par exemple en traçant $T$ vs $\\sqrt{\\ell}$ au lieu de $T$ vs $\\ell$ dans le cas de l'analyse de la période $T$ d'un pendule en fonction de sa longueur $\\ell$. Pour cette raison, les formes standard des lois [exponentielles](https://mathworld.wolfram.com/LeastSquaresFittingExponential.html), [logarithmiques](https://mathworld.wolfram.com/LeastSquaresFittingLogarithmic.html) et de [puissance](https://mathworld.wolfram.com/LeastSquaresFittingPowerLaw.html) sont souvent calculées explicitement. Les formules d'ajustement linéaire des moindres carrés ont été dérivées indépendamment par Gauss et Legendre.\n",
    "\n",
    "Pour l'[ajustement non linéaire des moindres carrés](https://mathworld.wolfram.com/NonlinearLeastSquaresFitting.html) à un certain nombre de paramètres inconnus, l'ajustement linéaire des moindres carrés peut être appliqué de manière itérative à une forme linéarisée de la fonction jusqu'à ce que la convergence soit atteinte. Cependant, il est souvent également possible de linéariser une fonction non linéaire au départ et d'utiliser encore des méthodes linéaires pour déterminer les paramètres d'ajustement sans recourir à des procédures itératives. Cette approche viole généralement l'hypothèse implicite selon laquelle la distribution des erreurs est [normale](https://mathworld.wolfram.com/NormalDistribution.html), mais donne souvent des résultats acceptables en utilisant des équations normales, une [pseudo-inverse](https://mathworld.wolfram.com/Pseudoinverse.html), etc. Selon le type d'ajustement et les paramètres initiaux choisis, l'ajustement non linéaire peut avoir une bonne ou une mauvaise propriétés de convergence. Si des incertitudes (dans le cas le plus général, des ellipses d'erreur) sont données pour les points, les points peuvent être pondérés différemment afin de donner plus de poids aux points de haute qualité.\n",
    "\n",
    "L'ajustement vertical des moindres carrés consiste à trouver la somme des *carrés* des écarts *verticaux* $R^2$ d'un ensemble de $n$ points de données\n",
    "\n",
    "$\\begin{align}\n",
    "    R^2 = \\sum [y_i - f(x_i, a_1, a_2, \\cdots, a_n)]^2\n",
    "    \\tag{1}\n",
    "\\end{align}$\n",
    "\n",
    "à partir d'une fonction $f$. Notez que cette procédure ne minimise pas les écarts réels par rapport à la ligne (qui seraient mesurés perpendiculairement à la fonction donnée). De plus, bien que la somme non carrée des distances puisse sembler une quantité plus appropriée à minimiser, l'utilisation de la valeur absolue entraîne des dérivées discontinues qui ne peuvent pas être traitées analytiquement. Les écarts au carré à partir de chaque point sont donc additionnés, et le résidu résultant est ensuite minimisé pour trouver la meilleure ligne d'ajustement. Cette procédure donne aux points périphériques une pondération disproportionnée.\n",
    "\n",
    "La condition pour que $R^2$ soit un minimum est que\n",
    "\n",
    "$\\begin{align}\n",
    "    \\frac{\\partial \\left(R^2\\right)}{\\partial a_i} = 0\n",
    "    \\tag{2}\n",
    "\\end{align}$\n",
    "\n",
    "pour $i=1, \\dots, n$. Pour un ajustement linéaire,\n",
    "\n",
    "$\\begin{align}\n",
    "    f(a, b) = a + b \\, x\n",
    "    \\tag{3}\n",
    "\\end{align}$\n",
    "\n",
    "donc\n",
    "\n",
    "$\\begin{align}\n",
    "    R^2(a, b) \\equiv \\displaystyle\\sum^n_{i=1} [y_i - (a + b \\, x_i)]^2\n",
    "    \\tag{4}\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "    \\frac{\\partial \\left(R^2\\right)}{\\partial a} =\n",
    "    -2 \\displaystyle\\sum^n_{i=1} [y_i - (a + b \\, x_i)] = 0\n",
    "    \\tag{5}\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "    \\frac{\\partial \\left(R^2\\right)}{\\partial b} =\n",
    "    -2 \\displaystyle\\sum^n_{i=1} [y_i - (a + b \\, x_i)]x_i = 0\n",
    "    \\tag{6}\n",
    "\\end{align}$\n",
    "\n",
    "Celles-ci conduisent aux équations\n",
    "\n",
    "$\\begin{align}\n",
    "    n \\, a + b \\displaystyle\\sum^n_{i=1} x_i = \\displaystyle\\sum^n_{i=1} y_i\n",
    "    \\tag{7}\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "    a \\displaystyle\\sum^n_{i=1} x_i + b \\displaystyle\\sum^n_{i=1} x^2_i = \\displaystyle\\sum^n_{i=1} x_i \\, y_i\n",
    "    \\tag{8}\n",
    "\\end{align}$\n",
    "\n",
    "Sous forme [matricielle](https://mathworld.wolfram.com/Matrix.html),\n",
    "\n",
    "$\\begin{align}\n",
    "    \\begin{pmatrix}\n",
    "        n & \\sum^n_{i=1} x_i\\\\\n",
    "        \\sum^n_{i=1} x_i & \\sum^n_{i=1} x^2_i\\\\\n",
    "    \\end{pmatrix}\n",
    "    \\begin{pmatrix}\n",
    "        a\\\\\n",
    "        b\\\\\n",
    "    \\end{pmatrix} =\n",
    "    \\begin{pmatrix}\n",
    "        \\sum^n_{i=1} y_i\\\\\n",
    "        \\sum^n_{i=1} x_i \\, y_i\\\\\n",
    "    \\end{pmatrix}    \n",
    "    \\tag{9}\n",
    "\\end{align}$\n",
    "\n",
    "d'où\n",
    "\n",
    "$\\begin{align}\n",
    "    \\begin{pmatrix}\n",
    "        a\\\\\n",
    "        b\\\\\n",
    "    \\end{pmatrix} =\n",
    "    \\begin{pmatrix}\n",
    "        n & \\sum^n_{i=1} x_i\\\\\n",
    "        \\sum^n_{i=1} x_i & \\sum^n_{i=1} x^2_i\\\\\n",
    "    \\end{pmatrix}^{-1}\n",
    "    \\begin{pmatrix}\n",
    "        \\sum^n_{i=1} y_i\\\\\n",
    "        \\sum^n_{i=1} x_i \\, y_i\\\\\n",
    "    \\end{pmatrix}    \n",
    "    \\tag{10}\n",
    "\\end{align}$\n",
    "\n",
    "L'[inversion d'une matrice 2 x 2](https://mathworld.wolfram.com/MatrixInverse.html) permet d'obtenir\n",
    "\n",
    "$\\begin{align}\n",
    "    \\begin{pmatrix}\n",
    "        a\\\\\n",
    "        b\\\\\n",
    "    \\end{pmatrix} =\n",
    "    \\frac{1}{n \\sum^n_{i=1} x^2_i - \\left( \\sum^n_{i=1} x_i \\right)^2}\n",
    "    \\begin{pmatrix}\n",
    "        \\sum^n_{i=1} y_i \\sum^n_{i=1} x^2_i - \\sum^n_{i=1} x_i \\sum^n_{i=1} x_i \\, y_i\\\\\n",
    "        n \\sum^n_{i=1} x_i \\, y_i - \\sum^n_{i=1} x_i \\sum^n_{i=1} y_i\\\\\n",
    "    \\end{pmatrix}    \n",
    "    \\tag{11}\n",
    "\\end{align}$\n",
    "\n",
    "donc\n",
    "\n",
    "$\\begin{align}\n",
    "    a = \\frac{\\sum^n_{i=1} y_i \\sum^n_{i=1} x^2_i - \\sum^n_{i=1} x_i \\sum^n_{i=1} x_i \\, y_i}\n",
    "    {n \\sum^n_{i=1} x^2_i - \\left( \\sum^n_{i=1} x_i \\right)^2} =\\\\\n",
    "    \\frac{\\bar{y} \\sum^n_{i=1} x^2_i - \\bar{x} \\sum^n_{i=1} x_i \\, y_i}\n",
    "    {\\sum^n_{i=1} x^2_i - n \\bar{x}^2}\n",
    "    \\tag{12}\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "    b = \\frac{n \\sum^n_{i=1} x_i \\, y_i - \\sum^n_{i=1} x_i \\sum^n_{i=1} y_i}\n",
    "    {n \\sum^n_{i=1} x^2_i - \\left( \\sum^n_{i=1} x_i \\right)^2} =\\\\\n",
    "    \\frac{\\sum^n_{i=1} x_i \\, y_i - n \\bar{x}  \\bar{y}}\n",
    "    {\\sum^n_{i=1} x^2_i - n \\, \\bar{x}^2}\n",
    "    \\tag{13}\n",
    "\\end{align}$\n",
    "\n",
    "(Kenney et Keeping 1962). Ces égalités peuvent être réécrites sous une forme plus simple en définissant les sommes des carrés\n",
    "\n",
    "$\\begin{align}\n",
    "    \\text{ss}_{x \\, x} = \\displaystyle\\sum^n_{i=1} (x_i - \\bar{x})^2 =\n",
    "    \\left( \\displaystyle\\sum^n_{i=1} x_i^2 \\right) - n \\, \\bar{x}^2\n",
    "    \\tag{14}\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "    \\text{ss}_{y \\, y} = \\displaystyle\\sum^n_{i=1} (y_i - \\bar{y})^2 =\n",
    "    \\left( \\displaystyle\\sum^n_{i=1} y_i^2 \\right) - n \\, \\bar{y}^2\n",
    "    \\tag{15}\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "    \\text{ss}_{x \\, y} = \\displaystyle\\sum^n_{i=1} (x_i - \\bar{x})(y_i - \\bar{y}) =\n",
    "    \\left( \\displaystyle\\sum^n_{i=1} x_i \\, y_i \\right) - n \\, \\bar{x}\\bar{y}\n",
    "    \\tag{16}\n",
    "\\end{align}$\n",
    "\n",
    "qui peuvent aussi être écrite comme\n",
    "\n",
    "$\\begin{align}\n",
    "    \\sigma^2_x = \\frac{\\text{ss}_{x \\, x}}{n}\n",
    "    \\tag{17}\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "    \\sigma^2_y = \\frac{\\text{ss}_{y \\, y}}{n}\n",
    "    \\tag{18}\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "    \\text{cov}(x, y) = \\frac{\\text{ss}_{x \\, y}}{n}\n",
    "    \\tag{19}\n",
    "\\end{align}$\n",
    "\n",
    "Ici, $\\text{cov}(x, y)$ est la [covariance](https://mathworld.wolfram.com/Covariance.html) et $\\sigma^2_x$ et  $\\sigma^2_y$ sont les variances. Noter que le quantité $\\sum^n_{i=1} x_i \\, y_i$ et $\\sum^n_{i=1} x_i^2$ peuvent être interprétées comme le [produit scalaire](https://mathworld.wolfram.com/DotProduct.html)\n",
    "\n",
    "$\\begin{align}\n",
    "    \\displaystyle\\sum^n_{i=1} x_i^2 = \\boldsymbol{x} . \\boldsymbol{x}\n",
    "    \\tag{20}\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "    \\displaystyle\\sum^n_{i=1} x_i \\, y_i = \\boldsymbol{x} . \\boldsymbol{y}\n",
    "    \\tag{21}\n",
    "\\end{align}$\n",
    "\n",
    "En termes de sommes de carrés, le [coefficient de régression](https://mathworld.wolfram.com/RegressionCoefficient.html) $b$ est donné par\n",
    "\n",
    "$\\begin{align}\n",
    "    b = \\frac{\\text{cov}(x, y)}{\\sigma^2_x} = \\frac{\\text{ss}_{x \\, y}}{\\text{ss}_{x \\, x}}\n",
    "    \\tag{22}\n",
    "\\end{align}$\n",
    "\n",
    "et $a$ est donné en termes de $b$\n",
    "\n",
    "$\\begin{align}\n",
    "    a = \\bar{y} - b \\, \\bar{x}\n",
    "    \\tag{23}\n",
    "\\end{align}$\n",
    "\n",
    "La qualité globale de l'ajustement est ensuite paramétrée en termes d'une quantité appelée [coefficient de corrélation](https://mathworld.wolfram.com/CorrelationCoefficient.html), définie par\n",
    "\n",
    "$\\begin{align}\n",
    "    r^2 = \\frac{\\text{ss}^2_{x \\, y}}{\\text{ss}_{x \\, x}\\text{ss}_{y \\, y}}\n",
    "    \\tag{24}\n",
    "\\end{align}$\n",
    "\n",
    "ce qui donne la proportion de $\\text{ss}_{y \\, y}$ qui est prise en compte par la régression.\n",
    "\n",
    "Soit $\\hat{y}_i$ la coordonnée verticale de la ligne la mieux ajustée avec la $x$-coordonnée $x_i$, il vient\n",
    "\n",
    "$\\begin{align}\n",
    "    \\hat{y}_i \\equiv a + b \\, x_i\n",
    "    \\tag{25}\n",
    "\\end{align}$\n",
    "\n",
    "alors l'erreur entre le point vertical réel $y_i$ et le point ajusté est donnée par\n",
    "\n",
    "$\\begin{align}\n",
    "    e_i \\equiv y_i - \\hat{y}_i\n",
    "    \\tag{26}\n",
    "\\end{align}$\n",
    "\n",
    "Définissons à présent $s^2$ en tant qu'estimateur de la virance en $e_i$,\n",
    "\n",
    "$\\begin{align}\n",
    "    s^2 = \\displaystyle\\sum^n_{i=1} \\frac{e^2_i}{n - 2}\n",
    "    \\tag{27}\n",
    "\\end{align}$\n",
    "\n",
    "alors $s$ peut être donné par\n",
    "\n",
    "$\\begin{align}\n",
    "    s =\n",
    "    \\sqrt{\\frac{\\text{ss}_{y \\, y} - b \\, \\text{ss}_{x \\, y}}{n - 2}} =\n",
    "    \\sqrt{\\frac{\\text{ss}_{y \\, y} - \\frac{\\text{ss}^2_{x \\, y}}{\\text{ss}_{x \\, x}} }{n - 2}}\n",
    "    \\tag{28}\n",
    "\\end{align}$\n",
    "\n",
    "(Acton 1966, pp. 32-35; Gonick et Smith 1993, pp. 202-204).\n",
    "\n",
    "Les [erreurs standards](https://mathworld.wolfram.com/StandardError.html) pour $a$ et $b$ sont\n",
    "\n",
    "$\\begin{align}\n",
    "    \\text{SE}(a) = s \\sqrt{\\frac{1}{n} + \\frac{\\bar{x}^2}{\\text{ss}_{x \\, x}}}\n",
    "    \\tag{29}\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "    \\text{SE}(b) = \\frac{s}{\\sqrt{\\text{ss}_{x \\, x}}}\n",
    "    \\tag{30}\n",
    "\\end{align}$\n",
    "\n",
    "Voir également :\n",
    "\n",
    "* [ANOVA](https://mathworld.wolfram.com/ANOVA.html)\n",
    "* [Coefficient de corrélation](https://mathworld.wolfram.com/CorrelationCoefficient.html)\n",
    "* [Interpolation](https://mathworld.wolfram.com/Interpolation.html)\n",
    "* [Ajustement des moindres carrés - Exponentiel](https://mathworld.wolfram.com/LeastSquaresFittingExponential.html)\n",
    "* [Ajustement des moindres carrés - Logarithmique](https://mathworld.wolfram.com/LeastSquaresFittingLogarithmic.html)\n",
    "* [Ajustement des moindres carrés - décalages perpendiculaires](https://mathworld.wolfram.com/LeastSquaresFittingPerpendicularOffsets.html)\n",
    "* [Ajustement des moindres carrés - Polynomial](https://mathworld.wolfram.com/LeastSquaresFittingPolynomial.html)\n",
    "* [Ajustement des moindres carrés - loi de puissance](https://mathworld.wolfram.com/LeastSquaresFittingPowerLaw.html)\n",
    "* [MANOVA](https://mathworld.wolfram.com/MANOVA.html)\n",
    "* [1-Inverse d'une matrice](https://mathworld.wolfram.com/Matrix1-Inverse.html)\n",
    "* [Inverse de Moore-Penrose d'une matrice](https://mathworld.wolfram.com/Moore-PenroseMatrixInverse.html)\n",
    "* [Ajustement des moindres carrés non linéaire](https://mathworld.wolfram.com/NonlinearLeastSquaresFitting.html)\n",
    "* [Pseudoinverse](https://mathworld.wolfram.com/Pseudoinverse.html)\n",
    "* [Coéfficient de régression](https://mathworld.wolfram.com/RegressionCoefficient.html)\n",
    "* [Résidu](https://mathworld.wolfram.com/Residual.html)\n",
    "* [Spline](https://mathworld.wolfram.com/Spline.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e03b612d84ba21ce95ed447e81b3062e1eb99b56c6d885cdab4aaa12f1b8e240"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

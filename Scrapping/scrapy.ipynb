{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuto **Scrapy**\n",
    "\n",
    "https://scrapy.org/\n",
    "\n",
    "https://docs.scrapy.org/en/latest/\n",
    "\n",
    "Scrapy is a fast high-level [**web crawling**](https://en.wikipedia.org/wiki/Web_crawler) (*indexation*) and [**web scraping**](https://en.wikipedia.org/wiki/Web_scraping) (*moissonage*) framework, used to crawl websites and extract structured data from their pages. It can be used for a wide range of purposes, from data mining to monitoring and automated testing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Premiers pas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [**Coup d'oeil rapide sur Scrapy**](https://docs.scrapy.org/en/latest/intro/overview.html)\n",
    "\n",
    "Scrapy (/Ààskre…™pa…™/) est un cadre d'application pour explorer des sites Web et extraire des donn√©es structur√©es qui peuvent √™tre utilis√©es pour un large √©ventail d'applications utiles, comme l'exploration de donn√©es, le traitement de l'information ou l'archivage historique.\n",
    "\n",
    "M√™me si Scrapy a √©t√© con√ßu √† l'origine pour le scraping Web, il peut √©galement √™tre utilis√© pour extraire des donn√©es √† l'aide d'API (telles que Amazon Associates Web Services) ou en tant que robot d'exploration Web √† usage g√©n√©ral."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pr√©sentation d'un exemple d'araign√©e\n",
    "\n",
    "Afin de vous montrer ce que Scrapy apporte √† la table, nous vous pr√©senterons un exemple de Scrapy Spider utilisant le moyen le plus simple de faire fonctionner une araign√©e.\n",
    "\n",
    "Voici le code d'une araign√©e qui extrait des citations c√©l√®bres du site Web https://quotes.toscrape.com, en suivant la pagination¬†:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "    start_urls = [\n",
    "        'https://quotes.toscrape.com/tag/humor/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'author': quote.xpath('span/small/text()').get(),\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "            }\n",
    "\n",
    "        next_page = response.css('li.next a::attr(\"href\")').get()\n",
    "        if next_page is not None:\n",
    "            yield response.follow(next_page, self.parse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mettez ceci dans un fichier texte, nommez-le par exemple `quotes_spider.py` et ex√©cutez l'araign√©e en utilisant la commande `runspider`¬†:\n",
    "\n",
    "    scrapy runspider quotes_spider.py -o quotes.jsonl\n",
    "\n",
    "Lorsque cela se termine, vous aurez dans le fichier `quotes.jsonl` une liste des citations au format JSON Lines, contenant le texte et l'auteur, ressemblant √† ceci :\n",
    "\n",
    "    {\"author\": \"Jane Austen\", \"text\": \"\\u201cThe person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.\\u201d\"}\n",
    "    {\"author\": \"Steve Martin\", \"text\": \"\\u201cA day without sunshine is like, you know, night.\\u201d\"}\n",
    "    {\"author\": \"Garrison Keillor\", \"text\": \"\\u201cAnyone who thinks sitting in church can make you a Christian must also think that sitting in a garage can make you a car.\\u201d\"}\n",
    "    ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qu'est-ce qui vient de se passer?\n",
    "\n",
    "Lorsque vous avez ex√©cut√© la commande `scrapy runspider quotes_spider.py`, Scrapy a recherch√© une d√©finition Spider √† l'int√©rieur et l'a ex√©cut√©e via son moteur d'indexation.\n",
    "\n",
    "L'exploration a commenc√© par envoyer des requ√™tes aux URL d√©finies dans l'attribut `start_urls` (dans ce cas, uniquement l'URL pour les citations dans la cat√©gorie humour) et a appel√© la m√©thode de rappel par d√©faut `parse`, en passant l'objet de r√©ponse comme argument. Dans le rappel de `parse`, nous parcourons les √©l√©ments de citation √† l'aide d'un s√©lecteur CSS, produisons un dict Python avec le texte de citation et l'auteur extraits, recherchons un lien vers la page suivante et planifions une autre demande en utilisant la m√™me m√©thode `parse` pour le rappel.\n",
    "\n",
    "Ici, vous remarquez l'un des principaux avantages de Scrapy¬†: les requ√™tes sont [planifi√©es et trait√©es de mani√®re asynchrone](https://docs.scrapy.org/en/latest/topics/architecture.html#topics-architecture). Cela signifie que Scrapy n'a pas besoin d'attendre qu'une demande soit termin√©e et trait√©e, il peut envoyer une autre demande ou faire d'autres choses entre-temps. Cela signifie √©galement que d'autres requ√™tes peuvent continuer m√™me si certaines requ√™tes √©chouent ou si une erreur se produit lors de leur traitement.\n",
    "\n",
    "Bien que cela vous permette d'effectuer des analyses tr√®s rapides (en envoyant plusieurs requ√™tes simultan√©es en m√™me temps, de mani√®re tol√©rante aux pannes), Scrapy vous permet √©galement de contr√¥ler la politesse de l'analyse via [quelques param√®tres](https://docs.scrapy.org/en/latest/topics/settings.html#topics-settings-ref). Vous pouvez faire des choses comme d√©finir un d√©lai de t√©l√©chargement entre chaque requ√™te, limiter le nombre de requ√™tes simultan√©es par domaine ou par adresse IP, et m√™me utiliser une [extension de limitation automatique](https://docs.scrapy.org/en/latest/topics/autothrottle.html#topics-autothrottle) qui essaie de les comprendre automatiquement.\n",
    "\n",
    "üìå Ceci utilise les [exportations de flux](https://docs.scrapy.org/en/latest/topics/feed-exports.html#topics-feed-exports) pour g√©n√©rer le fichier JSON, vous pouvez facilement changer le format d'exportation (XML ou CSV, par exemple) ou le backend de stockage (FTP ou Amazon S3, par exemple). Vous pouvez √©galement √©crire un [pipeline d'√©l√©ments](https://docs.scrapy.org/en/latest/topics/item-pipeline.html#topics-item-pipeline) pour stocker les √©l√©ments dans une base de donn√©es."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quoi d'autre?\n",
    "\n",
    "Vous avez vu comment extraire et stocker des √©l√©ments d'un site Web √† l'aide de Scrapy, mais ce n'est que la surface. Scrapy fournit de nombreuses fonctionnalit√©s puissantes pour rendre le moissonage facile et efficace, telles que :\n",
    "\n",
    "* Prise en charge int√©gr√©e de la [s√©lection et de l'extraction](https://docs.scrapy.org/en/latest/topics/selectors.html#topics-selectors) de donn√©es √† partir de sources HTML/XML √† l'aide de s√©lecteurs CSS √©tendus et d'expressions XPath, avec des m√©thodes d'assistance pour extraire √† l'aide d'expressions r√©guli√®res.\n",
    "* Une [console shell interactive](https://docs.scrapy.org/en/latest/topics/shell.html#topics-shell) (compatible IPython) pour essayer les expressions CSS et XPath pour r√©cup√©rer des donn√©es, tr√®s utile lors de l'√©criture ou du d√©bogage de vos spiders.\n",
    "* Une prise en charge int√©gr√©e pour la [g√©n√©ration des exportations de flux](https://docs.scrapy.org/en/latest/topics/feed-exports.html#topics-feed-exports) dans plusieurs formats (JSON, CSV, XML) et les stocker dans plusieurs backends (FTP, S3, syst√®me de fichiers local)\n",
    "* Prise en charge robuste de l'encodage et d√©tection automatique, pour traiter les d√©clarations d'encodage √©trang√®res, non standard et cass√©es.\n",
    "* [Prise en charge d'une extensibilit√© solide](https://docs.scrapy.org/en/latest/index.html#extending-scrapy), vous permettant de brancher vos propres fonctionnalit√©s √† l'aide de [signaux](https://docs.scrapy.org/en/latest/topics/signals.html#topics-signals) et d'une API bien d√©finie (middlewares, [extensions](https://docs.scrapy.org/en/latest/topics/extensions.html#topics-extensions) et [pipelines](https://docs.scrapy.org/en/latest/topics/item-pipeline.html#topics-item-pipeline)).\n",
    "* Large gamme d'extensions int√©gr√©es et de middlewares pour g√©rer¬†:\n",
    "    * cookies et gestion des sessions\n",
    "    * Fonctionnalit√©s HTTP comme la compression, l'authentification, la mise en cache\n",
    "    * usurpation d'agent utilisateur\n",
    "    * robots.txt\n",
    "    * restriction de profondeur d'exploration\n",
    "    * et plus\n",
    "* Une [console Telnet](https://docs.scrapy.org/en/latest/topics/telnetconsole.html#topics-telnetconsole) pour se connecter √† une console Python ex√©cut√©e dans votre processus Scrapy, pour introspecter et d√©boguer votre crawler\n",
    "* De plus, d'autres avantages tels que des araign√©es r√©utilisables pour explorer des sites √† partir de [plans de site](https://www.sitemaps.org/index.html) et de flux XML/CSV, un pipeline multim√©dia pour [t√©l√©charger automatiquement des images](https://docs.scrapy.org/en/latest/topics/media-pipeline.html#topics-media-pipeline) (ou tout autre m√©dia) associ√©es aux √©l√©ments supprim√©s, un r√©solveur DNS de mise en cache, et bien plus encore¬†!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Et apr√®s?\n",
    "\n",
    "Les prochaines √©tapes pour vous consistent √† installer Scrapy, √† suivre le didacticiel pour apprendre √† cr√©er un projet Scrapy complet et √† rejoindre la communaut√©. Merci de votre int√©r√™t!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [**Installation**](https://docs.scrapy.org/en/latest/intro/install.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [**Didacticiel**](https://docs.scrapy.org/en/latest/intro/tutorial.html)\n",
    "\n",
    "Dans ce didacticiel, nous supposerons que Scrapy est d√©j√† install√© sur votre syst√®me. Si ce n'est pas le cas, consultez le guide d'installation.\n",
    "\n",
    "Nous allons gratter [quotes.toscrape.com](https://quotes.toscrape.com/), un site qui r√©pertorie les citations d'auteurs c√©l√®bres.\n",
    "\n",
    "Ce didacticiel vous guidera √† travers ces t√¢ches¬†:\n",
    "1. Cr√©er un nouveau projet Scrapy\n",
    "2. √âcrire une araign√©e pour explorer un site et extraire des donn√©es\n",
    "3. Exporter les donn√©es r√©cup√©r√©es √† l'aide de la ligne de commande\n",
    "4. Changer l'araign√©e pour suivre r√©cursivement les liens\n",
    "5. Utiliser des arguments d'araign√©e\n",
    "\n",
    "Scrapy est √©crit en Python. Si vous d√©butez dans le langage, vous voudrez peut-√™tre commencer par vous faire une id√©e de ce √† quoi ressemble le langage, pour tirer le meilleur parti de Scrapy.\n",
    "\n",
    "Si vous connaissez d√©j√† d'autres langages et que vous souhaitez apprendre Python rapidement, le didacticiel Python est une bonne ressource.\n",
    "\n",
    "Si vous d√©butez en programmation et que vous souhaitez commencer avec Python, les livres suivants peuvent vous √™tre utiles¬†:\n",
    "* Automatisez les trucs ennuyeux avec Python\n",
    "* Comment penser comme un informaticien\n",
    "* Apprendre Python 3 √† la dure\n",
    "\n",
    "Vous pouvez √©galement consulter cette liste de ressources Python pour les non-programmeurs, ainsi que les ressources sugg√©r√©es dans le learnpython-subreddit."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cr√©er un projet\n",
    "\n",
    "Avant de commencer √† gratter, vous devrez configurer un nouveau projet Scrapy. Entrez un r√©pertoire dans lequel vous souhaitez stocker votre code et ex√©cutez¬†:\n",
    "\n",
    "    scrapy startproject tutorial\n",
    "\n",
    "Cela cr√©era un r√©pertoire `tutorial` avec le contenu suivant¬†:\n",
    "\n",
    "    tutorial/\n",
    "        scrapy.cfg            # deploy configuration file\n",
    "        tutorial/             # project's Python module, you'll import your code from here\n",
    "            __init__.py\n",
    "            items.py          # project items definition file\n",
    "            middlewares.py    # project middlewares file\n",
    "            pipelines.py      # project pipelines file\n",
    "            settings.py       # project settings file\n",
    "            spiders/          # a directory where you'll later put your spiders\n",
    "                __init__.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notre premi√®re araign√©e\n",
    "\n",
    "Les araign√©es sont des classes que vous d√©finissez et que Scrapy utilise pour extraire des informations d'un site Web (ou d'un groupe de sites Web). Ils doivent sous-classer `Spider` et d√©finir les requ√™tes initiales √† effectuer, √©ventuellement comment suivre les liens dans les pages et comment analyser le contenu de la page t√©l√©charg√©e pour extraire des donn√©es.\n",
    "\n",
    "C'est le code de notre premier Spider. Enregistrez-le dans un fichier nomm√© `quotes_spider.py` sous le r√©pertoire `tutorial/spiders` de votre projet¬†:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9aff9e50adfaa9e30c910fb3872ffdc72747acb5f50803ca0504f00e980f7c25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

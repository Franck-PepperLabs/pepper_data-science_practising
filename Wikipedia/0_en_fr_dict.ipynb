{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>Faire mieux qu'un dictionnaire de traductions, faire un index et glossaire !</mark>\n",
    "\n",
    "Récupérer tout ce que j'ai déjà enregisté dans :\n",
    "* mon journal\n",
    "* le journal de formation\n",
    "* les doc de projet\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nouvelles entrées à intégrer\n",
    "\n",
    "* *dépendance partielle*\n",
    "* **classifier** : *classifieur* (et non *classificateur*)\n",
    "* **feature** : *caractéristique* (et non *fonctionnalité*, ni *entité*)\n",
    "\n",
    "Toutes les références enregistrées dans les journaux et non encore classées\n",
    "\n",
    "Lien vers les fiches traduites quand elles l'ont été + les priorités de traduction.\n",
    "\n",
    "Sortir les inserts dans les cours.\n",
    "\n",
    "\n",
    "\n",
    "#### ⦿ [**Support vector machine (SVM)**](https://en.wikipedia.org/wiki/Support_vector_machine) ([*Machine à vecteurs de support (SVM)*](https://fr.wikipedia.org/wiki/Machine_à_vecteurs_de_support))\n",
    "\n",
    "#### ⦿ [**Lasso (statistics)**](https://en.wikipedia.org/wiki/Lasso_(statistics)) ([*Lasso (statistiques)*](https://fr.wikipedia.org/wiki/Lasso_(statistiques)))\n",
    "\n",
    "#### ⦿ [****]() ([**]())\n",
    "\n",
    "#### ⦿ [****]() ([**]())\n",
    "\n",
    "#### ⦿ [****]() ([**]())\n",
    "\n",
    "#### ⦿ [****]() ([**]())\n",
    "\n",
    "#### ⦿ [****]() ([**]())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [**Kaniadakis statistics**](https://en.wikipedia.org/wiki/Kaniadakis_statistics) (*Statistique de Kaniadakis*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data leakage** (*Fuite de données*)\n",
    "\n",
    "[“Data Leakage in Machine Learning”](https://machinelearningmastery.com/data-leakage-machine-learning/), Jason Brownlee, machinelearningmastery.com, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [**Geographic coordinate system**](https://en.wikipedia.org/wiki/Geographic_coordinate_system) ([*Coordonnées géographiques*](https://fr.wikipedia.org/wiki/Coordonnées_géographiques))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  [**Partial correlation**](https://en.wikipedia.org/wiki/Partial_correlation) ([*Corrélation partielle*](https://fr.wikipedia.org/wiki/Corrélation_partielle))\n",
    "\n",
    "**Catégories** : Covariance and correlation · Dimensionless numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  [**Decision boundary**](https://en.wikipedia.org/wiki/Decision_boundary) (*Surface de décision*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [**Kernel trick**](https://en.wikipedia.org/wiki/Kernel_method) ([*Astuce du noyau*](https://fr.wikipedia.org/wiki/Astuce_du_noyau))\n",
    "\n",
    "En apprentissage automatique, l'**astuce du noyau**, ou *kernel trick* en anglais, est une méthode qui permet d'utiliser un classifieur linéaire pour résoudre un problème non linéaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [**Loss function**](https://en.wikipedia.org/wiki/Loss_function) ([*Fonction objectif*](https://fr.wikipedia.org/wiki/Fonction_objectif))\n",
    "\n",
    "\n",
    "**Catégories** : Optimal decisions · Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [**Hinge loss**](https://en.wikipedia.org/wiki/Loss_function) (*Perte charnière ?*)\n",
    "\n",
    "**Catégories** : Loss functions · Support vector machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [**Cluster analysis**](https://en.wikipedia.org/wiki/Cluster_analysis) ([*Partitionnement de données*](https://fr.wikipedia.org/wiki/Partitionnement_de_données))\n",
    "\n",
    "**Categories**: Cluster analysis · Data mining · Geostatistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [⚓](https://en.wikipedia.org/wiki/Category:Machine_learning) [**Machine learning**](https://en.wikipedia.org/wiki/Machine_learning) ([*Apprentissage automatique*](https://fr.wikipedia.org/wiki/Apprentissage_automatique))\n",
    "\n",
    "**Category**: [**Supervised learning**](https://en.wikipedia.org/wiki/Category:Machine_learning) ([*Apprentissage automatique*](https://fr.wikipedia.org/wiki/Catégorie:Apprentissage_automatique))\n",
    "\n",
    "**Problems**:\n",
    "* Classification · <u>Regression</u> · Clustering · dimension reduction · density estimation · Anomaly detection · Data Cleaning · AutoML · Association rules · Structured prediction · Feature engineering · Feature learning · Online learning · Reinforcement learning · Supervised learning · Semi-supervised learning · Unsupervised learning · Learning to rank · Grammar induction\n",
    "\n",
    "**Supervised learning**: (*classification* • <u>*regression*</u>):\n",
    "* Decision trees · Ensembles (Bagging · Boosting · Random forest) · k-NN · Linear regression · Naive Bayes · Artificial neural networks · Logistic regression · Perceptron · Relevance vector machine (RVM) · Support vector machine (SVM)\n",
    "\n",
    "**Clustering**:\n",
    "* BIRCH · CURE · Hierarchical · k-means · Fuzzy · Expectation–maximization (EM) · DBSCAN · OPTICS · Mean shift\n",
    "\n",
    "**Dimensionality reduction**:\n",
    "* Factor analysis · CCA · ICA · LDA · NMF · PCA · PGD · t-SNE · SDL\n",
    "\n",
    "**Structured prediction**:\n",
    "* Graphical models (Bayes net · Conditional random field · Hidden Markov)\n",
    "\n",
    "**Anomaly detection**:\n",
    "*RANSAC · k-NN · Local outlier factor · Isolation forest\n",
    "\n",
    "**Artificial neural network**:\n",
    "* Autoencoder · Cognitive computing · Deep learning · DeepDream · Multilayer perceptron · RNN (LSTM · GRU · ESN · reservoir computing) · Restricted Boltzmann machine · GAN · SOM · Convolutional neural network (U-Net) · Transformer (Vision) · Spiking neural network · Memtransistor · Electrochemical RAM (ECRAM)\n",
    "\n",
    "**Reinforcement learning**:\n",
    "* Q-learning · SARSA · Temporal difference (TD) · Multi-agent · Self-play\n",
    "\n",
    "**Learning with humans**:\n",
    "* Active learning · Crowdsourcing · Human-in-the-loop\n",
    "\n",
    "**Model diagnostics**:\n",
    "* Learning curve\n",
    "\n",
    "**Theory**:\n",
    "* Kernel machines · Bias–variance tradeoff · Computational learning theory · Empirical risk minimization · Occam learning · PAC learning · Statistical learning · VC theory\n",
    "\n",
    "**Machine-learning venues**:\n",
    "* NeurIPS · ICML · ICLR · ML · JMLR\n",
    "\n",
    "**Related articles**:\n",
    "* Glossary of artificial intelligence · List of datasets for machine-learning research · Outline of machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [**Data mining**](https://en.wikipedia.org/wiki/Data_mining) ([*Exploration de données*](https://fr.wikipedia.org/wiki/Exploration_de_données))\n",
    "\n",
    "**Machine learning** and **data mining**:\n",
    "* **Problems**: Classification · Regression · Clustering · Dimension reduction · Density estimation · Anomaly detection · Data Cleaning · AutoML · Association rules · Structured prediction · Feature engineering · Feature learning · Online learning · Reinforcement learning · Supervised learning · Semi-supervised learning · Unsupervised learning · Learning to rank · Grammar induction\n",
    "\n",
    "* **Supervised learning** (*classification* • *regression*): Decision trees · Ensembles (Bagging · Boosting · Random forest) · k-NN · Linear regression · Naive Bayes · Artificial neural networks · Logistic regression · Perceptron · Relevance vector machine (RVM) · Support vector machine (SVM)\n",
    "\n",
    "* **Clustering**: BIRCH · CURE · Hierarchical · k-means · Fuzzy · Expectation–maximization (EM) · DBSCAN · OPTICS · Mean shift\n",
    "\n",
    "* **Dimensionality reduction**: Factor analysis · CCA · ICA · LDA · NMF · PCA · PGD · t-SNE · SDL\n",
    "\n",
    "* **Structured prediction**: Graphical models (Bayes net · Conditional random field · Hidden Markov)\n",
    "\n",
    "* **Anomaly detection**: RANSAC · k-NN · Local outlier factor · Isolation forest\n",
    "\n",
    "* **Artificial neural network**: Autoencoder · Cognitive computing · Deep learning · DeepDream · Multilayer perceptron · RNN (LSTM · GRU · ESN · Reservoir computing) · Restricted Boltzmann machine · GAN · SOM · Convolutional neural network (U-Net) · Transformer (Vision) · Spiking neural network · Memtransistor · Electrochemical RAM (ECRAM)\n",
    "\n",
    "* **Reinforcement learning**: Q-learning · SARSA · Temporal difference (TD) · Multi-agent (Self-play)\n",
    "\n",
    "* **Learning with humans**: Active learning · Crowdsourcing · Human-in-the-loop\n",
    "\n",
    "* **Model diagnostics**: Learning curve\n",
    "\n",
    "* **Theory**: Kernel machines · Bias–variance tradeoff · Computational learning theory · Empirical risk minimization · Occam learning · PAC learning · Statistical learning · VC theory\n",
    "\n",
    "* **Machine-learning venues**: NeurIPS · ICML · ICLR · ML · JMLR\n",
    "\n",
    "* **Related articles**: Glossary of artificial intelligence · List of datasets for machine-learning research · Outline of machine learning\n",
    "\n",
    "* **Software**: Keras · Microsoft Cognitive Toolkit · Scikit-learn · TensorFlow · Theano · Weka · PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subcategories**\n",
    "* Applied machine learning (2 C, 52 P)\n",
    "* Artificial neural networks (3 C, 142 P)\n",
    "* Bayesian networks (13 P)\n",
    "* Blockmodeling (14 P)\n",
    "* Classification algorithms (3 C, 86 P)\n",
    "* Cluster analysis (2 C, 20 P)\n",
    "* Computational learning theory (22 P)\n",
    "* Artificial intelligence conferences (21 P)\n",
    "* Signal processing conferences (5 P)\n",
    "* Data mining and machine learning software (3 C, 89 P)\n",
    "* Datasets in machine learning (1 C, 9 P)\n",
    "* Deep learning (3 C, 26 P)\n",
    "* Dimension reduction (1 C, 46 P)\n",
    "* Ensemble learning (13 P)\n",
    "* Evolutionary algorithms (4 C, 44 P, 2 F)\n",
    "* Genetic programming (14 P)\n",
    "* Inductive logic programming (5 P)\n",
    "*  Kernel methods for machine learning (1 C, 17 P)\n",
    "* Latent variable models (2 C, 26 P)\n",
    "* Learning in computer vision (5 P)\n",
    "* Log-linear models (2 P)\n",
    "* Loss functions (10 P)\n",
    "* Machine learning algorithms (1 C, 75 P)\n",
    "* Machine learning task (1 C, 9 P)\n",
    "* Markov models (2 C, 55 P)\n",
    "* Ontology learning (computer science) (2 P)\n",
    "* Reinforcement learning (10 P)\n",
    "* Machine learning researchers (142 P)\n",
    "* Natural language processing researchers (117 P)\n",
    "* Semisupervised learning (2 P)\n",
    "* Statistical natural language processing (1 C, 36 P)\n",
    "* Structured prediction (1 C, 4 P)\n",
    "* Supervised learning (5 P)\n",
    "* Support vector machines (9 P)\n",
    "* Unsupervised learning (1 C, 24 P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Content**\n",
    "* Bayesian learning mechanisms\n",
    "* Machine learning\n",
    "* List of datasets for machine-learning research\n",
    "* Outline of machine learning\n",
    "* 80 Million Tiny Images\n",
    "* Ablation (artificial intelligence)\n",
    "* Action model learning\n",
    "* Active learning (machine learning)\n",
    "* Adversarial machine learning\n",
    "* AIXI\n",
    "* Algorithm selection\n",
    "* Algorithmic bias\n",
    "* Algorithmic inference\n",
    "* Anomaly detection\n",
    "* Apprenticeship learning\n",
    "* Artificial intelligence in hiring\n",
    "* Associative classifier\n",
    "* Astrostatistics\n",
    "* Attention (machine learning)\n",
    "* Australian Institute for Machine Learning\n",
    "* Automated decision-making\n",
    "* Automated machine learning\n",
    "* Automation in construction\n",
    "* Bag-of-words model\n",
    "* Ball tree\n",
    "* Base rate\n",
    "* Bayesian interpretation of kernel regularization\n",
    "* Bayesian optimization\n",
    "* Bayesian regret\n",
    "* Bayesian structural time series\n",
    "* Bias–variance tradeoff\n",
    "* Binary classification\n",
    "* Bioserenity\n",
    "* Blockmodeling\n",
    "* Bradley–Terry model\n",
    "* Catastrophic interference\n",
    "* Category utility\n",
    "* CIML community portal\n",
    "* Cognitive robotics\n",
    "* Committee machine\n",
    "* Computational learning theory\n",
    "* Concept class\n",
    "* Concept drift\n",
    "* Conditional random field\n",
    "* Confusion matrix\n",
    "* Connectionist temporal classification\n",
    "* Constrained conditional model\n",
    "* Count sketch\n",
    "* Coupled pattern learner\n",
    "* Cross-entropy method\n",
    "* Cross-validation (statistics)\n",
    "* ⦿ [**Curse of dimensionality**](https://en.wikipedia.org/wiki/Curse_of_dimensionality) ([*Fléau de la dimension*](https://fr.wikipedia.org/wiki/Fléau_de_la_dimension))\n",
    "* Data augmentation\n",
    "* Data exploration\n",
    "* Data pre-processing\n",
    "* Data Version Control\n",
    "* Decision list\n",
    "* Decision tree pruning\n",
    "* Developmental robotics\n",
    "* Diffusion model\n",
    "* Dimensionality reduction\n",
    "* Discovery system (AI research)\n",
    "* Document classification\n",
    "* Domain adaptation\n",
    "* Double descent\n",
    "* Drift (data science)\n",
    "* Eager learning\n",
    "* Early stopping\n",
    "* Elastic matching\n",
    "* ELMo\n",
    "* EM algorithm and GMM model\n",
    "* Empirical dynamic modeling\n",
    "* Empirical risk minimization\n",
    "* Energy based model\n",
    "* Equalized odds\n",
    "* Error tolerance (PAC learning)\n",
    "* Evaluation of binary classifiers\n",
    "* Evolutionary programming\n",
    "* Evolvability (computer science)\n",
    "* Expectation propagation\n",
    "* Explanation-based learning\n",
    "* Fairness (machine learning)\n",
    "* Feature (machine learning)\n",
    "* Feature engineering\n",
    "* Feature hashing\n",
    "* Feature learning\n",
    "* Feature scaling\n",
    "* Federated learning\n",
    "* Flow-based generative model\n",
    "* Flux (machine-learning framework)\n",
    "* Formal concept analysis\n",
    "* Foundation models\n",
    "* Generative model\n",
    "* Genetic algorithm\n",
    "* Glossary of artificial intelligence\n",
    "* Google JAX\n",
    "* Grammar induction\n",
    "* Granular computing\n",
    "* Graph neural network\n",
    "* Highway network\n",
    "* Hugging Face\n",
    "* Hyperparameter (machine learning)\n",
    "* Hyperparameter optimization\n",
    "* Inauthentic text\n",
    "* Inception score\n",
    "* Inductive bias\n",
    "* Inductive probability\n",
    "* Inductive programming\n",
    "* Inferential theory of learning\n",
    "* Instance selection\n",
    "* Instance-based learning\n",
    "* Instantaneously trained neural networks\n",
    "* Intelligent automation\n",
    "* Isotropic position\n",
    "* Journal of Machine Learning Research\n",
    "* Kernel density estimation\n",
    "* Kernel embedding of distributions\n",
    "* Knowledge distillation\n",
    "* Knowledge graph embedding\n",
    "* Knowledge integration\n",
    "* Labeled data\n",
    "* Large margin nearest neighbor\n",
    "* Large width limits of neural networks\n",
    "* Latent space\n",
    "* Lazy learning\n",
    "* Leakage (machine learning)\n",
    "* Learnable function class\n",
    "* Learning automaton\n",
    "* Learning curve (machine learning)\n",
    "* Learning rate\n",
    "* Learning to rank\n",
    "* Learning with errors\n",
    "* Leave-one-out error\n",
    "* Life-time of correlation\n",
    "* Linear predictor function\n",
    "* Linear separability\n",
    "* Local case-control sampling\n",
    "* Lyra (codec)\n",
    "* M-theory (learning framework)\n",
    "* Logic learning machine\n",
    "* Machine Learning (journal)\n",
    "* Machine learning control\n",
    "* Machine learning in bioinformatics\n",
    "* Machine learning in earth sciences\n",
    "* Machine learning in physics\n",
    "* Machine learning in video games\n",
    "* Manifold hypothesis\n",
    "* Manifold regularization\n",
    "* The Master Algorithm\n",
    "* Matchbox Educable Noughts and Crosses Engine\n",
    "* Matrix regularization\n",
    "* Maximum inner-product search\n",
    "* Meta-learning (computer science)\n",
    "* Mixture model\n",
    "* MLOps\n",
    "* Mountain car problem\n",
    "* Multi-agent reinforcement learning\n",
    "* Multi-armed bandit\n",
    "* Multi-task learning\n",
    "* Multilinear principal component analysis\n",
    "* Multilinear subspace learning\n",
    "* Multimodal sentiment analysis\n",
    "* Multiple instance learning\n",
    "* Multiple-instance learning\n",
    "* Multiplicative weight update method\n",
    "* Multitask optimization\n",
    "* Multivariate adaptive regression spline\n",
    "* Natarajan dimension\n",
    "* Native-language identification\n",
    "* Nature Machine Intelligence\n",
    "* Nearest neighbor search\n",
    "* Neural modeling fields\n",
    "* Neural network Gaussian process\n",
    "* Neural network quantum states\n",
    "* Node2vec\n",
    "* Novelty detection\n",
    "* Occam learning\n",
    "* Offline learning\n",
    "* Overfitting\n",
    "* Paraphrasing (computational linguistics)\n",
    "* Parity learning\n",
    "* Pattern language (formal languages)\n",
    "* Pattern recognition\n",
    "* Perceiver\n",
    "* Phi coefficient\n",
    "* Physics-informed neural networks\n",
    "* Predictive learning\n",
    "* Predictive state representation\n",
    "* Preference learning\n",
    "* Prior knowledge for pattern recognition\n",
    "* Proactive learning\n",
    "* Proaftn\n",
    "* Probabilistic numerics\n",
    "* Probability matching\n",
    "* Product of experts\n",
    "* Programming by example\n",
    "* Proximal gradient methods for learning\n",
    "* Pythia (machine learning)\n",
    "* Quantification (machine learning)\n",
    "* Quantum machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚓ [**Supervised learning**](https://en.wikipedia.org/wiki/Supervised_learning) ([*Apprentissage supervisé*](https://fr.wikipedia.org/wiki/Apprentissage_supervisé))\n",
    "\n",
    "**Category**: [**Supervised learning**](https://en.wikipedia.org/wiki/Category:Supervised_learning)\n",
    "\n",
    "* Supervised learning\n",
    "* Associative classifier\n",
    "* Graph neural network\n",
    "* Restricted Boltzmann machine\n",
    "* Variational autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Ensemble learning**](https://en.wikipedia.org/wiki/Ensemble_learning) ([*Apprentissage ensembliste*](https://fr.wikipedia.org/wiki/Apprentissage_ensembliste))\n",
    "\n",
    "[**Bootstrap aggregating (~Bagging)**](https://en.wikipedia.org/wiki/Bootstrap_aggregating) ([***B****ootstrap* ***agg****regat****ing*** *(~Ensachage)*](https://fr.wikipedia.org/wiki/Bootstrap_aggregating))\n",
    "\n",
    "[**Boosting (machine learning)**](https://en.wikipedia.org/wiki/Boosting_(machine_learning)) ([*Boosting*](https://fr.wikipedia.org/wiki/Boosting))\n",
    "\n",
    "[**Random forest**](https://en.wikipedia.org/wiki/Random_forest) ([*Forêt aléatoire*](https://fr.wikipedia.org/wiki/For%C3%AAt_d%27arbres_d%C3%A9cisionnels))\n",
    "\n",
    "[**AdaBoost**](https://en.wikipedia.org/wiki/AdaBoost) ([*AdaBoost*](https://fr.wikipedia.org/wiki/AdaBoost))\n",
    "* [**Decision stump**](https://en.wikipedia.org/wiki/Decision_stump) ([*Souche de décision*]())\n",
    "\n",
    "[**Gradient boosting**](https://en.wikipedia.org/wiki/Gradient_boosting) ([*Gradient boosting*]())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [⚓](https://en.wikipedia.org/wiki/Category:Statistical_classification) [**Statistical classification**](https://en.wikipedia.org/wiki/Statistical_classification) ([*Classement automatique*](https://fr.wikipedia.org/wiki/Classement_automatique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ⚓ Classification algorithms (3 C, 86 P)\n",
    "* ⚓ [**Cluster analysis**](https://en.wikipedia.org/wiki/Category:Cluster_analysis) (2 C, 20 P)\n",
    "    * ⚓ [**Cluster analysis algorithms**](https://en.wikipedia.org/wiki/Category:Cluster_analysis_algorithms)\n",
    "        * ⦿ [**Mean shift**](https://en.wikipedia.org/wiki/Mean_shift)\n",
    "* ⚓ Logistic regression (14 P)\n",
    "* Statistical classification\n",
    "* Averaged one-dependence estimators\n",
    "* Automated essay scoring\n",
    "* Bayes classifier\n",
    "* Bayes error rate\n",
    "* Bias–variance tradeoff\n",
    "* Binary classification\n",
    "* Calibration (statistics)\n",
    "* Chi-square automatic interaction detection\n",
    "* Classification rule\n",
    "* Confusion matrix\n",
    "* Cover's theorem\n",
    "* Data classification (business intelligence)\n",
    "* Decision boundary\n",
    "* Double descent\n",
    "* Evaluation of binary classifiers\n",
    "* False positives and false negatives\n",
    "* Firmographics\n",
    "* Geodemographic segmentation\n",
    "* Growth function\n",
    "* Industrial market segmentation\n",
    "* K-nearest neighbors algorithm\n",
    "* Kernel Fisher discriminant analysis\n",
    "* Kernel perceptron\n",
    "* Klecka's tau\n",
    "* Leakage (machine learning)\n",
    "* Least-squares support vector machine\n",
    "* Linear classifier\n",
    "* Linear discriminant analysis\n",
    "* Margin classifier\n",
    "* Mixture (probability)\n",
    "* Multiclass classification\n",
    "* Multiclass LDA\n",
    "* Multinomial probit\n",
    "* Multiple discriminant analysis\n",
    "* Naive Bayes classifier\n",
    "* Natarajan dimension\n",
    "* Neighbourhood components analysis\n",
    "* Net reclassification improvement\n",
    "* One-class classification\n",
    "* Partial Area Under the ROC Curve\n",
    "* Phi coefficient\n",
    "* Platt scaling\n",
    "* Predictive modelling\n",
    "* Prior knowledge for pattern recognition\n",
    "* Proaftn\n",
    "* Probabilistic classification\n",
    "* Probability matching\n",
    "* Quadratic classifier\n",
    "* RCASE\n",
    "* Receiver operating characteristic\n",
    "* Recursive partitioning\n",
    "* Sagacity segmentation\n",
    "* Sensitivity and specificity\n",
    "* Similarity measure\n",
    "* Support vector machine\n",
    "* Total operating characteristic\n",
    "* Typology (social science research method)\n",
    "* Values Modes\n",
    "* Vapnik–Chervonenkis dimension\n",
    "* Variable kernel density estimation\n",
    "* Youden's J statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [**Biclustering**](https://en.wikipedia.org/wiki/Biclustering) : [*Classification double*](https://fr.wikipedia.org/wiki/Classification_double)\n",
    "\n",
    "**Non supervisé**\n",
    "\n",
    "**Catégories** : Cluster analysis · Bioinformatics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚓ [**Regression analysis**](https://en.wikipedia.org/wiki/Regression_analysis) ([*Régression (statistiques)*](https://fr.wikipedia.org/wiki/Régression_(statistiques)))\n",
    "\n",
    "Least squares and regression analysis:\n",
    "* **Computational statistics**: Least squares · Linear least squares · Non-linear least squares · Iteratively reweighted least squares\n",
    "* **Correlation and dependence**: Pearson product-moment correlation · Rank correlation (Spearman's rho Kendall's tau) · Partial correlation · Confounding variable\n",
    "* **Regression analysis**: Ordinary least squares · Partial least squares · Total least squares · Ridge regression\n",
    "* **Regression as a statistical model**:\t\n",
    "* * **Linear regression**: Simple linear regression · Ordinary least squares · Generalized least squares · Weighted least squares · General linear model\n",
    "* * **Predictor structure**: Polynomial regression · Growth curve (statistics) · Segmented regression · Local regression\n",
    "* * **Non-standard**: Nonlinear regression · Nonparametric · Semiparametric · Robust · Quantile · Isotonic\n",
    "* * **Non-normal errors**: Generalized linear model · Binomial · Poisson · Logistic\n",
    "* **Decomposition of variance**: Analysis of variance · Analysis of covariance · Multivariate AOV\n",
    "* **Model exploration**: Stepwise regression · Model selection (Mallows's Cp · AIC · BIC) · Model specification · Regression validation\n",
    "* **Background**: Mean and predicted response · Gauss–Markov theorem · Errors and residuals · Goodness of fit · Studentized residual · Minimum mean-square error · Frisch–Waugh–Lovell theorem\n",
    "* **Design of experiments**: Response surface methodology · Optimal design · Bayesian design\n",
    "* **Numerical approximation**: Numerical analysis · Approximation theory · Numerical integration · Gaussian quadrature · Orthogonal polynomials · Chebyshev polynomials · Chebyshev nodes\n",
    "* **Applications**: Curve fitting · Calibration curve · Numerical smoothing and differentiation · System identificationMoving least squares\n",
    "\n",
    "\n",
    "Corpus **Correlation & Regression analysis** :\n",
    "* [**Correlation and dependence**](https://en.wikipedia.org/wiki/Correlation): Pearson product-moment · Partial correlation · Confounding variable · Coefficient of determination\n",
    "* **Regression analysis**: Errors and residuals · Regression validation · Mixed effects models · Simultaneous equations models · Multivariate adaptive regression splines (MARS)\n",
    "* **Linear regression**: Simple linear regression · Ordinary least squares · General linear model · Bayesian regression\n",
    "* **Non-standard predictors**: Nonlinear regression · Nonparametric · Semiparametric · Isotonic · Robust · Heteroscedasticity · Homoscedasticity\n",
    "* **Generalized linear model**: Exponential families · Logistic (Bernoulli) / Binomial / Poisson regressions\n",
    "* [**Partition of variance**](https://en.wikipedia.org/wiki/Partition_of_sums_of_squares): Analysis of variance (ANOVA, anova) · Analysis of covariance · Multivariate ANOVA · Degrees of freedom\n",
    "\n",
    "**Categories**: Regression analysis · Actuarial science · Curve fitting · Estimation theory\n",
    "\n",
    "**Models**: \n",
    "* Linear regression · Simple regression · Polynomial regression · General linear model · Proportional hazards model\n",
    "* Generalized linear model · Vector generalized linear model · Discrete choice Binomial regression · Binary regression · Logistic regression · Multinomial logistic regression · Mixed logit · Probit · Multinomial probit · Ordered logit · Ordered probit · Poisson\n",
    "* Multilevel model · Fixed effects · Random effects · Linear mixed-effects model · Nonlinear mixed-effects model\n",
    "* Nonlinear regression · Support vector regression · Nonparametric · Semiparametric · Robust · Quantile · Isotonic · Principal components · Least angle · Local Segmented\n",
    "* Errors-in-variables\n",
    "\n",
    "**Estimation**:\n",
    "* [**Computational statistics**](https://en.wikipedia.org/wiki/Computational_statistics): Least squares · Linear · Non-linear\n",
    "* Ordinary · Weighted · Generalized · Generalized estimating equation\n",
    "* Partial · Total · Non-negative · Ridge regression · Regularized\n",
    "* Least absolute deviations · Iteratively reweighted · Bayesian · Bayesian multivariate · Least-squares spectral analysis · Heteroscedasticity Consistent Regression Standard Errors · Heteroscedasticity and Autocorrelation Consistent Regression Standard Errors · Instrumental variables estimation\n",
    "\n",
    "**Background**:\n",
    "* Regression validation · Mean and predicted response · Errors and residuals · Goodness of fit · Studentized residual · Gauss–Markov theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Techniques**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [**Correlation and dependence**](https://en.wikipedia.org/wiki/Correlation)\n",
    "\n",
    "* Pearson product-moment correlation\n",
    "* Rank correlation (Spearman's rho · Kendall's tau)\n",
    "* Partial correlation\n",
    "* Confounding variable\n",
    "* Coefficient of determination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Regression analysis**\n",
    "\n",
    "* Errors and residuals\n",
    "* Regression validation\n",
    "* Mixed effects models\n",
    "* Simultaneous equations models\n",
    "* Multivariate adaptive regression splines (MARS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Linear regression algorithms**\n",
    "\n",
    "* Simple linear regression\n",
    "* Ordinary least squares\n",
    "* General linear model\n",
    "* Bayesian regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Non-standard predictors**\n",
    "\n",
    "* Nonlinear regression\n",
    "* Nonparametric\n",
    "* Semiparametric\n",
    "* Isotonic\n",
    "* Robust\n",
    "* Heteroscedasticity\n",
    "* Homoscedasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Generalized linear model**\n",
    "\n",
    "* Exponential families\n",
    "* Logistic regression (Bernoulli)\n",
    "* Binomial regression\n",
    "* Poisson regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [**Partition of variance**](https://en.wikipedia.org/wiki/Partition_of_sums_of_squares)\n",
    "\n",
    "* Analysis of variance (ANOVA, anova)\n",
    "* Analysis of covariance\n",
    "* Multivariate ANOVA\n",
    "* Degrees of freedom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Models**\n",
    "\n",
    "* Linear regression · Simple regression · Polynomial regression · General linear model · Proportional hazards model\n",
    "* Generalized linear model · Vector generalized linear model · Discrete choice Binomial regression · Binary regression · Logistic regression · Multinomial logistic regression · Mixed logit · Probit · Multinomial probit · Ordered logit · Ordered probit · Poisson\n",
    "* Multilevel model · Fixed effects · Random effects · Linear mixed-effects model · Nonlinear mixed-effects model\n",
    "* Nonlinear regression · Support vector regression · Nonparametric · Semiparametric · Robust · Quantile · Isotonic · Principal components · Least angle · Local Segmented\n",
    "* Errors-in-variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear regression\n",
    "\n",
    "* Linear regression\n",
    "* Simple regression\n",
    "* Polynomial regression\n",
    "* General linear model\n",
    "* Proportional hazards model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [**Generalized linear model  (GLM)**](https://en.wikipedia.org/wiki/Generalized_linear_model) ([*Modèle linéaire généralisé (MLG)*](https://fr.wikipedia.org/wiki/Modèle_linéaire_généralisé))\n",
    "\n",
    "* Generalized linear model\n",
    "* Vector generalized linear model\n",
    "* Discrete choice Binomial regression\n",
    "* Binary regression\n",
    "* Logistic regression\n",
    "* Multinomial logistic regression\n",
    "* Mixed logit\n",
    "* Probit\n",
    "* Multinomial probit\n",
    "* Ordered logit\n",
    "* Ordered probit\n",
    "* Poisson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilevel model\n",
    "\n",
    "* Multilevel model\n",
    "* Fixed effects\n",
    "* Random effects\n",
    "* Linear mixed-effects model\n",
    "* Nonlinear mixed-effects model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nonlinear regression\n",
    "\n",
    "* Nonlinear regression\n",
    "* Support vector regression\n",
    "* Nonparametric\n",
    "* Semiparametric\n",
    "* Robust\n",
    "* Quantile\n",
    "* Isotonic\n",
    "* Principal components\n",
    "* Least angle\n",
    "* Local Segmented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Errors-in-variables\n",
    "\n",
    "* Regression validation\n",
    "* Mean and predicted response\n",
    "* Errors and residuals\n",
    "* Goodness of fit\n",
    "* Studentized residual\n",
    "* Gauss–Markov theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Estimation**\n",
    "\n",
    "* [**Computational statistics**](https://en.wikipedia.org/wiki/Computational_statistics): Least squares · Linear · Non-linear\n",
    "* [**Linear regression**](https://en.wikipedia.org/wiki/Linear_regression): Ordinary · Weighted · Generalized · Generalized estimating equation\n",
    "* Partial · Total · Non-negative · Ridge regression · Regularized\n",
    "* Least absolute deviations · Iteratively reweighted · Bayesian · Bayesian multivariate · Least-squares spectral analysis · Heteroscedasticity Consistent Regression Standard Errors · Heteroscedasticity and Autocorrelation Consistent Regression Standard Errors · Instrumental variables estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [**Computational statistics**](https://en.wikipedia.org/wiki/Computational_statistics)\n",
    "\n",
    "* [**Least squares**](https://en.wikipedia.org/wiki/Least_squares) ([*Moindres carrés*](https://fr.wikipedia.org/wiki/Méthode_des_moindres_carrés))\n",
    "* [**Linear least squares (LLS)**](https://en.wikipedia.org/wiki/Linear_least_squares)\n",
    "* [**Non-linear least squares**](https://en.wikipedia.org/wiki/Non-linear_least_squares)\n",
    "* [**Iteratively reweighted least squares**](https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [**Linear regression**](https://en.wikipedia.org/wiki/Linear_regression)\n",
    "\n",
    "* [**Ordinary least squares (OLS)**](https://en.wikipedia.org/wiki/Ordinary_least_squares)\n",
    "* [**Weighted least squares (WLS)**](https://en.wikipedia.org/wiki/Weighted_least_squares)\n",
    "* [**Generalized least squares (GLS)**](https://en.wikipedia.org/wiki/Generalized_least_squares)\n",
    "* [**Generalized estimating equation (GEE)**](https://en.wikipedia.org/wiki/Generalized_estimating_equation)\n",
    "* [**Simple linear regression**](https://en.wikipedia.org/wiki/Simple_linear_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic 3\n",
    "\n",
    "* Partial\n",
    "* Total\n",
    "* Non-negative\n",
    "* Ridge regression\n",
    "* Regularized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic 4\n",
    "\n",
    "* Least absolute deviations\n",
    "* Iteratively reweighted\n",
    "* Bayesian\n",
    "* Bayesian multivariate\n",
    "* Least-squares spectral analysis\n",
    "* Heteroscedasticity Consistent Regression Standard Errors\n",
    "* Heteroscedasticity and Autocorrelation Consistent Regression Standard Errors\n",
    "* Instrumental variables estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Background**\n",
    "\n",
    "* [**Regression validation**](https://en.wikipedia.org/wiki/Regression_validation)\n",
    "* [**Mean and predicted response**](https://en.wikipedia.org/wiki/Mean_and_predicted_response)\n",
    "* [**Errors and residuals**](https://en.wikipedia.org/wiki/Errors_and_residuals) ([*Résidu (statistiques)*](https://fr.wikipedia.org/wiki/Résidu_(statistiques)))\n",
    "* [**Goodness of fit**](https://en.wikipedia.org/wiki/Goodness_of_fit) ([*Qualité de l'ajustement*](https://fr.wikipedia.org/wiki/Qualité_de_l'ajustement))\n",
    "* [**Studentized residual**](https://en.wikipedia.org/wiki/Studentized_residual)\n",
    "* [**Gauss–Markov theorem**](https://en.wikipedia.org/wiki/Gauss-Markov_theorem) ([*Théorème de Gauss-Markov*](https://fr.wikipedia.org/wiki/Théorème_de_Gauss-Markov))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⦿ [**Regression validation**](https://en.wikipedia.org/wiki/Regression_validation) (*Validation de régression*)\n",
    "\n",
    "In statistics, **regression validation** is the process of deciding whether the numerical results quantifying hypothesized relationships between variables, obtained from **regression analysis**, are acceptable as descriptions of the data. The validation process can involve analyzing the **goodness of fit** of the regression, analyzing whether the **regression residuals** are random, and checking whether the model's predictive performance deteriorates substantially when applied to data that were not used in model estimation.\n",
    "\n",
    "---\n",
    "\n",
    "En statistique, la **validation de régression** est le processus consistant à décider si les résultats numériques quantifiant les relations hypothétiques entre les variables, obtenus à partir de l'**analyse par régression**, sont acceptables en tant que descriptions des données. Le processus de validation peut impliquer d'analyser la **qualité d'ajustement** de la régression, d'analyser si les **résidus de régression** sont aléatoires et de vérifier si les performances prédictives du modèle se détériorent considérablement lorsqu'elles sont appliquées à des données qui n'ont pas été utilisées dans l'estimation du modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ⦿ [**Regression diagnostic**](https://en.wikipedia.org/wiki/Regression_diagnostic)\n",
    "\n",
    "https://en.wikipedia.org/wiki/Category:Regression_diagnostics\n",
    "\n",
    "* Regression diagnostic\n",
    "* Regression validation\n",
    "* Breusch–Godfrey test\n",
    "* Breusch–Pagan test\n",
    "* Chow test\n",
    "* Coefficient of determination\n",
    "* Cook's distance\n",
    "* DFFITS\n",
    "* Goldfeld–Quandt test\n",
    "* Influential observation\n",
    "* Information matrix test\n",
    "* Leverage (statistics)\n",
    "* Mallows's Cp\n",
    "* Park test\n",
    "* Partial leverage\n",
    "* Partial regression plot\n",
    "* Partial residual plot\n",
    "* Portmanteau test\n",
    "* PRESS statistic\n",
    "* Pseudo-R-squared\n",
    "* Ramsey RESET test\n",
    "* Structural break test\n",
    "* Studentized residual\n",
    "* Variance inflation factor\n",
    "* White test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⦿ [**Mean and predicted response**](https://en.wikipedia.org/wiki/Mean_and_predicted_response) (*Réponses moyenne et prévue*)\n",
    "\n",
    "In **linear regression**, **mean response** and **predicted response** are values of the dependent variable calculated from the regression parameters and a given value of the independent variable. The values of these two responses are the same, but their calculated variances are different.\n",
    "\n",
    "---\n",
    "\n",
    "Dans la **régression linéaire**, la **réponse moyenne** et la **réponse prévue** sont les valeurs de la variable dépendante calculées à partir des paramètres de régression et d'une valeur donnée de la variable indépendante. Les valeurs de ces deux réponses sont les mêmes, mais leurs variances calculées sont différentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⦿ [**Errors and residuals**](https://en.wikipedia.org/wiki/Errors_and_residuals) ([*Résidu (statistiques)*](https://fr.wikipedia.org/wiki/Résidu_(statistiques)))\n",
    "\n",
    "In **statistics** and **optimization**, **errors** and **residuals** are two closely related and easily confused measures of the **deviation** of an **observed value** of an **element** of a **statistical sample** from its \"**true value**\" (not necessarily observable). The *error* of an **observation** is the deviation of the observed value from the true value of a quantity of interest (for example, a **population mean**). The **residual** is the difference between the observed value and the **estimated** value of the quantity of interest (for example, a **sample mean**). The distinction is most important in **regression analysis**, where the concepts are sometimes called the **regression errors** and **regression residuals** and where they lead to the concept of **studentized residuals**. In **econometrics**, \"errors\" are also called disturbances.[1][2][3]\n",
    "\n",
    "---\n",
    "\n",
    "En **statistiques** et en **optimisation**, les **erreurs** et les **résidus** sont deux mesures étroitement liées et faciles à confondre de l'**écart** d'une **valeur observée** d'un **élément** d'un **échantillon statistique** à partir de sa \"**valeur vraie**\" (pas nécessairement observable). L'*erreur* d'une **observation** est l'écart entre la valeur observée et la vraie valeur d'une quantité d'intérêt (par exemple, une **moyenne de population**). Le **résidu** est la différence entre la valeur observée et la valeur **estimée** de la quantité d'intérêt (par exemple, une **moyenne d'échantillon**). La distinction est la plus importante dans l'**analyse par régression**, où les concepts sont parfois appelés **erreurs de régression** et **résidus de régression** et où ils conduisent au concept de **résidus studentisés**. En **économétrie**, les \"erreurs\" sont également appelées perturbations.[1][2][3]\n",
    "\n",
    "\n",
    "**Category ⚓** [Errors and residuals](https://en.wikipedia.org/wiki/Category:Errors_and_residuals)\n",
    "\n",
    "**Items**:\n",
    "* [**Errors and residuals**](https://en.wikipedia.org/wiki/Errors_and_residuals) ([*Résidu (statistiques)*](https://fr.wikipedia.org/wiki/Résidu_(statistiques)))\n",
    "* Berkson error model\n",
    "* Forecast error\n",
    "* [**Mean absolute error (MAE)**](https://en.wikipedia.org/wiki/Mean_absolute_error)\n",
    "* Non-sampling error\n",
    "* Observational error\n",
    "* Probability of error\n",
    "* Probable error\n",
    "* [**Residual sum of squares (RSS)**](https://en.wikipedia.org/wiki/Residual_sum_of_squares)\n",
    "* Sampling error\n",
    "* Studentized residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⦿ [**Goodness of fit**](https://en.wikipedia.org/wiki/Goodness_of_fit) ([*Qualité de l'ajustement*](https://fr.wikipedia.org/wiki/Qualité_de_l'ajustement))\n",
    "\n",
    "The **goodness of fit** of a **statistical model** describes how well it fits a set of observations. Measures of goodness of fit typically summarize the discrepancy between observed values and the values expected under the model in question. Such measures can be used in **statistical hypothesis testing**, e.g. to **test for normality** of **residuals**, to test whether two samples are drawn from identical distributions (see **Kolmogorov–Smirnov** test), or whether outcome frequencies follow a specified distribution (see **Pearson's chi-square test**). In the **analysis of variance**, one of the components into which the variance is partitioned may be a **lack-of-fit sum of squares**.\n",
    "\n",
    "---\n",
    "\n",
    "La **qualité d'ajustement** d'un **modèle statistique** décrit dans quelle mesure il s'ajuste à un ensemble d'observations. Les mesures de la qualité de l'ajustement résument généralement l'écart entre les valeurs observées et les valeurs attendues dans le cadre du modèle en question. Ces mesures peuvent être utilisées dans les **tests d'hypothèses statistiques**, par ex. pour **tester la normalité** des **résidus**, pour tester si deux échantillons sont tirés de distributions identiques (voir **test de Kolmogorov-Smirnov**), ou si les fréquences des résultats suivent une distribution spécifiée (voir **Pearson's test du chi carré**). Dans l'**analyse de la variance**, l'une des composantes dans lesquelles la variance est répartie peut être une **somme des carrés sans ajustement**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⦿ [**Studentized residual**](https://en.wikipedia.org/wiki/Studentized_residual) (*Résidu studentisé*)\n",
    "\n",
    "In statistics, a **studentized residual** is the quotient resulting from the division of a **residual** by an **estimate** of its **standard deviation**. It is a form of a **Student's t-statistic**, with the estimate of error varying between points.\n",
    "\n",
    "This is an important technique in the detection of **outliers**. It is among several named in honor of **William Sealey Gosset**, who wrote under the pseudonym *Student*. Dividing a statistic by a **sample standard deviation** is called *studentizing*, in analogy with **standardizing** and **normalizing**.\n",
    "\n",
    "---\n",
    "\n",
    "En statistique, un **résidu studentisé** est le quotient résultant de la division d'un **résidu** par une **estimation** de son **écart type**. Il s'agit d'une forme de **statistique t de Student**, l'estimation de l'erreur variant d'un point à l'autre.\n",
    "\n",
    "Il s'agit d'une technique importante dans la détection des **valeurs aberrantes**. Il fait partie de plusieurs nommés en l'honneur de **William Sealey Gosset**, qui a écrit sous le pseudonyme *Student*. La division d'une statistique par un **écart-type d'échantillon** s'appelle *studentizing*, par analogie avec **standardizing** et **normalizing**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⦿ [**Gauss–Markov theorem**](https://en.wikipedia.org/wiki/Gauss-Markov_theorem) ([*Théorème de Gauss-Markov*](https://fr.wikipedia.org/wiki/Théorème_de_Gauss-Markov))\n",
    "\n",
    "In **statistics**, the **Gauss–Markov theorem** (or simply **Gauss theorem** for some authors)[1] states that the **ordinary least squares** (OLS) estimator has the lowest **sampling variance** within the **class** of **linear unbiased estimators**, if the **errors** in the **linear regression model** are **uncorrelated**, have **equal variances** and expectation value of zero.[2] The errors do not need to be **normal**, nor do they need to be **independent and identically distributed** (only **uncorrelated** with mean zero and **homoscedastic** with finite variance). The requirement that the estimator be unbiased cannot be dropped, since biased estimators exist with lower variance. See, for example, the **James–Stein estimator** (which also drops linearity), **ridge regression**, or simply any **degenerate** estimator.\n",
    "\n",
    "---\n",
    "\n",
    "En **statistiques**, le **théorème de Gauss–Markov** (ou simplement le **théorème de Gauss** pour certains auteurs)[1] stipule que l'estimateur des **moindres carrés ordinaires** (OLS) a le plus basse **variance d'échantillonnage** dans la **classe** des **estimateurs linéaires sans biais**, si les **erreurs** du **modèle de régression linéaire** sont **non corrélées**, ont des **variances égales** et valeur d'espérance de zéro.[2] Les erreurs n'ont pas besoin d'être **normales**, ni d'être **indépendantes et distribuées de manière identique** (seulement **non corrélées** avec une moyenne nulle et **homoscédastiques** avec une variance finie). L'exigence que l'estimateur soit sans biais ne peut pas être abandonnée, car il existe des estimateurs biaisés avec une variance plus faible. Voir, par exemple, l'**estimateur James-Stein** (qui supprime également la linéarité), la **régression ridge** ou simplement tout estimateur **dégénéré**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⦿ [**Homoscedasticity and heteroscedasticity**](https://en.wikipedia.org/wiki/Homoscedasticity_and_heteroscedasticity)\n",
    "\n",
    "In **statistics**, a **sequence** (or a vector) of **random variables** is **homoscedastic** if all its random variables have the same finite **variance**. This is also known as **homogeneity of variance**. The complementary notion is called **heteroscedasticity**. The spellings *homoskedasticity* and *heteroskedasticity* are also frequently used.[1][2][3]\n",
    "\n",
    "Assuming a variable is homoscedastic when in reality it is heteroscedastic results in unbiased but inefficient point estimates and in biased estimates of standard errors, and may result in overestimating the **goodness of fit** as measured by the **Pearson coefficient**.\n",
    "\n",
    "The existence of heteroscedasticity is a major concern in **regression analysis** and the **analysis of variance**, as it invalidates **statistical tests of significance** that assume that the **modelling errors** all have the same variance. While the **ordinary least squares** estimator is still unbiased in the presence of heteroscedasticity, it is inefficient and **generalized least squares** should be used instead.[4][5]\n",
    "\n",
    "Because heteroscedasticity concerns **expectations** of the second **moment** of the errors, its presence is referred to as **misspecification** of the second order.[6]\n",
    "\n",
    "The **econometrician** **Robert Engle** was awarded the 2003 **Nobel Memorial Prize for Economics** for his studies on **regression analysis** in the presence of heteroscedasticity, which led to his formulation of the **autoregressive conditional heteroscedasticity** (ARCH) modeling technique.[7]\n",
    "\n",
    "---\n",
    "\n",
    "En **statistiques**, une **séquence** (ou un vecteur) de **variables aléatoires** est **homoscédastique** si toutes ses variables aléatoires ont la même **variance** finie. Ceci est également connu sous le nom d'**homogénéité de la variance**. La notion complémentaire est appelée **hétéroscédasticité**. Les orthographes *homoskedasticity* et *heteroskedasticity* sont également fréquemment utilisées.[1][2][3]\n",
    "\n",
    "Supposer qu'une variable est homoscédastique alors qu'en réalité elle est hétéroscédastique donne des estimations ponctuelles non biaisées mais inefficaces et des estimations biaisées des erreurs types, et peut entraîner une surestimation de la **qualité de l'ajustement** telle que mesurée par le **coefficient de Pearson**.\n",
    "\n",
    "L'existence d'hétéroscédasticité est une préoccupation majeure dans l'**analyse de régression** et l'**analyse de variance**, car elle invalide les **tests statistiques de signification** qui supposent que les **erreurs de modélisation** ont toutes la même variance. Bien que l'estimateur des **moindres carrés ordinaires** soit toujours sans biais en présence d'hétéroscédasticité, il est inefficace et les **moindres carrés généralisés** doivent être utilisés à la place.\n",
    "\n",
    "Comme l'hétéroscédasticité concerne les **attentes** du deuxième **moment** des erreurs, sa présence est appelée **spécification erronée** du second ordre.[6]\n",
    "\n",
    "L'**économètre** **Robert Engle** a reçu le **prix Nobel d'économie** en 2003 pour ses études sur l'**analyse de régression** en présence d'hétéroscédasticité, ce qui l'a conduit à la formulation de la technique de modélisation **hétéroscédasticité conditionnelle autorégressive** (ARCH).[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚓ [**Unsupervised learning**](https://en.wikipedia.org/wiki/Unsupervised_learning) : [*Apprentissage non supervisé*](https://fr.wikipedia.org/wiki/Apprentissage_non_supervisé)\n",
    "\n",
    "**Category**: [**Unsupervised learning**](https://en.wikipedia.org/wiki/Category:Unsupervised_learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⦿ [**Sparse dictionary learning**](https://en.wikipedia.org/wiki/Sparse_dictionary_learning) : [*???*](???)\n",
    "\n",
    "**Catégories** : Machine learning · Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚓ **Machine learning evaluation metrics**\n",
    "\n",
    "**Corpus**:\n",
    "* **Regression**: MSE · MAE · sMAPE · MAPE · MASE · MSPE · RMS · RMSE/RMSD · R2 · MDA · MAD\n",
    "* **Classification**: F-score · P4 · Accuracy · Precision · Recall · Kappa · MCC · AUC · ROC · Sensitivity and specificity · Logarithmic Loss\n",
    "* **Clustering**: Silhouette · Calinski-Harabasz · Davies-Bouldin · Dunn index · Hopkins statistic · Jaccard index · Rand index · Similarity measure · SMC · SimHash\n",
    "* **Ranking**: MRR · DCG · NDCG · AP\n",
    "* **Computer Vision**: PSNR · SSIM · IoU\n",
    "* **NLP**: Perplexity · BLEU\n",
    "* **Deep Learning Related Metrics**: Inception score · FID\n",
    "* **Recommender system**: Coverage  · Intra-list Similarity\n",
    "* **Similarity**: Cosine similarity  · Euclidean distance  · Pearson correlation coefficient\n",
    "* **Confusion matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⦿ [**Precision and recall**](https://en.wikipedia.org/wiki/Precision_and_recall) : [*Précision et rappel*](https://fr.wikipedia.org/wiki/Précision_et_rappel)\n",
    "\n",
    "**Categories**: Information retrieval evaluation · Information science · Bioinformatics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⚓ **Machine learning algorithms**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⦿ [**Least-squares support-vector machine (LS-SVM)**](https://en.wikipedia.org/wiki/Least-squares_support-vector_machine)\n",
    "\n",
    "**Catégories** : Support vector machines · Classification algorithms · Statistical classification · Least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⦿ [**k-nearest neighbors algorithm (k-NN)**](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) ([*Méthode des k plus proches voisins*](https://fr.wikipedia.org/wiki/Méthode_des_k_plus_proches_voisins))\n",
    "\n",
    "**Catégories** : Classification algorithms · Search algorithms · Machine learning algorithms · Statistical classification · Nonparametric statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Contributeurs clés**\n",
    "\n",
    "#### ⦿ [**Grothendieck**, Alexander](https://en.wikipedia.org/wiki/Alexander_Grothendieck) ([*Alexandre Grothendieck*](https://fr.wikipedia.org/wiki/Alexandre_Grothendieck))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e03b612d84ba21ce95ed447e81b3062e1eb99b56c6d885cdab4aaa12f1b8e240"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
